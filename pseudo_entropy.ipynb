{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97401843",
   "metadata": {},
   "source": [
    "# Quantum Pseudo-Entropy in Post-Selected Entanglement with Noise\n",
    "\n",
    "This notebook implements and analyzes post-selected entanglement in quantum circuits, calculating pseudo-entropy and examining the effects of noise on the measurements. It serves as the computational backend for the theoretical and methodological discussions in the main thesis, offloading detailed derivations, simulation parameters, and extensive data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb24b3",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "This section ensures all necessary Python packages are installed and imports the required libraries for quantum simulation (Qiskit), numerical analysis (NumPy, SciPy), data manipulation (Pandas), and visualization (Matplotlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57376c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install required packages \n",
    "#%pip install -U qiskit qiskit-aer matplotlib pylatexenc scikit-learn tabulate sympy pdflatex pandas openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3872be6",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "This section defines the constants and simulation parameters that govern the quantum experiments. These include the resolution of the simulated parameter space for $\\beta$ (controlled interaction strength) and $\\delta$ (coherent error angle), the number of measurement shots for noisy simulations, and the specific IBM `FakeBackend` used for noise modeling. While these parameters are defined here for computational execution, their detailed justification, selection criteria, and discussion regarding hardware calibration and parameter space design are provided in the main thesis and supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5cd3b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import Aer\n",
    "from qiskit.quantum_info import Statevector, partial_trace\n",
    "from qiskit.circuit import Parameter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from IPython.display import Markdown, display, Latex\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import t, chisquare, beta\n",
    "from scipy import ndimage\n",
    "\n",
    "import sympy as sp\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl.styles import NamedStyle\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f130af",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "This section provides helper functions for displaying formatted output in the Jupyter notebook, handling numerical rounding for LaTeX compatibility, and managing the caching of computationally intensive results to disk. These utilities streamline the workflow and ensure consistent presentation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation Parameters\n",
    "\n",
    "# Number of points for beta and delta sweeps (must be odd to ensure a middle point at zero).\n",
    "# A square grid is used with the same number of points for both beta and delta.\n",
    "# This ensures symmetry and accurate sampling around zero.\n",
    "DELTA_POINTS = 401\n",
    "BETA_POINTS = 1601\n",
    "\n",
    "# Range of angles for beta and delta sweeps (both positive and negative).\n",
    "# A relatively large range is selected to simulate noise effectively.\n",
    "# The same range is applied to both beta and delta for consistency in simulations.\n",
    "DELTA_RANGE = np.pi / 128\n",
    "BETA_RANGE = np.pi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6d0db",
   "metadata": {},
   "source": [
    "## Core Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bcfdbd",
   "metadata": {},
   "source": [
    "## Circuit Execution and Data Collection\n",
    "\n",
    "This section defines the functions responsible for executing quantum circuits and collecting pseudo-entropy data across the defined parameter sweeps. The `calculate_generalized_density_matrix` and `calculate_pseudo_entropy` functions are central to this process, handling the complex-valued calculations required for pseudo-entropy. The `run_circuits_and_calculate_entropy` orchestrates the simulation loop, calculating initial and final states and then computing pseudo-entropy for each $(\\beta, \\delta)$ pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_and_log_print(text):\n",
    "    \"\"\"\n",
    "    Replaces the print function to display the given text as Markdown in the Jupyter notebook \n",
    "    and log it to the 'results/output.txt' file.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to display and log.\n",
    "    \"\"\"\n",
    "    # Display the text as Markdown in the notebook\n",
    "    print(text)\n",
    "    \n",
    "    # Log the text to the output file\n",
    "    try:\n",
    "        with open('results/output.txt', 'a') as file:\n",
    "            file.write(text + '\\n\\n')  # Add a newline after each entry for readability\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")\n",
    "\n",
    "def to_latex(expression):\n",
    "    return sp.latex(expression, mul_symbol=\"dot\", min=3, max=3)\n",
    "\n",
    "def round_number(number):\n",
    "    # Handle special cases: NaN, Inf, and Zero\n",
    "    if number == 0:\n",
    "        return 0\n",
    "    if np.isinf(number):  # Check for Infinity\n",
    "        return np.inf if number > 0 else -np.inf\n",
    "    if np.isnan(number):  # Check for NaN\n",
    "        return np.nan\n",
    "    exponent = int(np.floor(np.log10(abs(number))))\n",
    "    mantissa = number / (10 ** exponent)\n",
    "    rounded_mantissa = round(mantissa, 3)\n",
    "    return rounded_mantissa * (10 ** exponent)\n",
    "\n",
    "def display_and_log_markdown(text):\n",
    "    \"\"\" \n",
    "    Displays the given text as Markdown in the Jupyter Notebook \n",
    "    and logs it to the 'results/output.txt' file.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to display and log.\n",
    "    \"\"\"\n",
    "    # Display the text as Markdown in the notebook\n",
    "    display(Markdown(text))\n",
    "    \n",
    "    # Log the text to the output file\n",
    "    try:\n",
    "        with open('results/output.txt', 'a') as file:\n",
    "            file.write(text + '\\n\\n')  # Add a newline after each entry for readability\n",
    "    except Exception as e:\n",
    "        display_and_log_print(f\"An error occurred while writing to the file: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d582686",
   "metadata": {},
   "source": [
    "## Quantum Circuit Implementation\n",
    "\n",
    "This section defines the quantum circuits used for initial state preparation and the measurement. The `create_initial_state_circuit` function prepares the initial state $\\ket{+1}$, while `create_measurement_circuit` constructs the main two-qubit circuit including the CNOT gate and the parameterized $R_Y$, $R_Z$, and $R_X$ rotations that model coherent errors. This section provides the *detailed Qiskit implementation* of these circuits, complementing the high-level descriptions and canonical circuit diagrams presented in the main thesis. The specifics of virtual gate construction are fully expressed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_calculate(file_path, calculate_function, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Load a value from a file if it exists; otherwise, calculate it, save it, and return the value.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file to load/save the value.\n",
    "        calculate_function (callable): Function to calculate the value if not loaded.\n",
    "        *args: Positional arguments for the calculate_function.\n",
    "        **kwargs: Keyword arguments for the calculate_function.\n",
    "\n",
    "    Returns:\n",
    "        Any: The loaded or calculated value.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    if os.path.exists(file_path):\n",
    "        display_and_log_print(f\"Loading from {file_path}...\")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            value = pickle.load(f)\n",
    "    else:\n",
    "        display_and_log_print(f\"Calculating and saving to {file_path}...\")\n",
    "        value = calculate_function(*args, **kwargs)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(value, f)\n",
    "    return value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787af0f6",
   "metadata": {},
   "source": [
    "# Core Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399dc09",
   "metadata": {},
   "source": [
    "## Circuit Execution and Data Collection\n",
    "\n",
    "This section defines the functions responsible for executing quantum circuits and collecting pseudo-entropy data across the defined parameter sweeps. The `calculate_generalized_density_matrix` and `calculate_pseudo_entropy` functions are central to this process, handling the complex-valued calculations required for pseudo-entropy. The `run_circuits_and_calculate_entropy` orchestrates the simulation loop, calculating initial and final states and then computing pseudo-entropy for each $(\\beta, \\delta)$ pair.\n",
    "\n",
    "**Note on Noise Model and Tomography Overhead:** While this notebook incorporates a `FakeBackend` for noise modeling in the simulation, the *detailed noise model parameters* and the *comprehensive analysis of tomography overhead* are offloaded to the supplementary materials. This notebook focuses on the computational implementation of the pseudo-entropy calculation under noisy conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_generalized_density_matrix(initial_state: Statevector, final_state: Statevector, regularization) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the generalized non-Hermitian matrix between initial and final states for post-selected measurements.\n",
    "    This matrix is intentionally non-Hermitian, which is a key feature of post-selected measurements.\n",
    "\n",
    "    Args:\n",
    "        initial_state (Statevector): Initial quantum state\n",
    "        final_state (Statevector): Final quantum state after measurement\n",
    "        regularization (float): Regularization parameter to handle numerical instabilities\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Generalized non-Hermitian matrix or None if states are invalid\n",
    "    \"\"\"\n",
    "    if initial_state.dim != final_state.dim:\n",
    "        display_and_log_print(\"Initial and final states have different dimensions.\")\n",
    "        return None\n",
    "\n",
    "    # Check for valid state vectors\n",
    "    if np.any(np.isnan(initial_state.data)) or np.any(np.isnan(final_state.data)):\n",
    "        display_and_log_print(\"Invalid state vectors: NaN detected.\")\n",
    "        return None\n",
    "        \n",
    "    inner_product = initial_state.inner(final_state)  # Complex inner product without abs()\n",
    "    \n",
    "    # Use a threshold for numerical stability, but don't enforce it to be real\n",
    "    if np.abs(inner_product) > regularization:\n",
    "        try:\n",
    "            # Construct the non-Hermitian generalized matrix\n",
    "            # \\hat{\\rho} = \\frac{|\\psi_i\\rangle \\langle \\psi_f|}{\\langle \\psi_f | \\psi_i \\rangle}\n",
    "            rho_generalized = np.outer(initial_state.data, final_state.conjugate().data) / inner_product\n",
    "                        \n",
    "            # Perform partial trace over subsystem B (second qubit)\n",
    "            rho_A = partial_trace(rho_generalized, [1])\n",
    "            \n",
    "            # Check for numerical instabilities but allow complex values\n",
    "            if np.any(np.isnan(rho_A.data)):\n",
    "                display_and_log_print(\"Numerical instability detected in the partial trace result.\")\n",
    "                return None\n",
    "                \n",
    "            return rho_A.data  # Return the numpy array, not the DensityMatrix object\n",
    "            \n",
    "        except (np.linalg.LinAlgError, ValueError) as e:\n",
    "            display_and_log_print(f\"Error in matrix calculation: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        display_and_log_print(\"Inner product is too small, returning None.\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a198cd4",
   "metadata": {},
   "source": [
    "## Analysis and Visualization Functions\n",
    "\n",
    "This section provides a suite of functions for analyzing and visualizing the simulation results. It includes tools for calculating descriptive statistics of pseudo-entropy components, generating various heatmaps (Cartesian, polar, and sensitivity maps), and saving figures in a format suitable for LaTeX inclusion. These functions are crucial for interpreting the complex behavior of pseudo-entropy across the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea22a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pseudo_entropy(initial_state: Statevector, final_state: Statevector, regularization=1e-8) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate the pseudo-entropy, its standard deviation, and the trace of the reduced transition matrix\n",
    "    between initial and final quantum states in a post-selected weak measurement context.\n",
    "\n",
    "    For non-Hermitian generalized density matrices, eigenvalues can be complex, leading to complex\n",
    "    pseudo-entropy values. The standard deviation is computed via the complex variance using:\n",
    "        |z|² = (Re(z))² + (Im(z))².\n",
    "\n",
    "    Args:\n",
    "        initial_state (Statevector): Initial quantum state.\n",
    "        final_state (Statevector): Final quantum state after post-selection.\n",
    "        regularization (float): Threshold to avoid log(0) and division errors.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pseudo_entropy (complex): May be complex due to non-Hermitian input.\n",
    "            - std_deviation (float): Real-valued uncertainty estimate.\n",
    "            - trace_rho (complex): Trace of the reduced (unnormalized) transition matrix.\n",
    "    \"\"\"\n",
    "    rho_generalized = calculate_generalized_density_matrix(initial_state, final_state, regularization)\n",
    "    \n",
    "    if rho_generalized is None:\n",
    "        display_and_log_print(\"Failed to calculate generalized density matrix; returning (0, 0, 0).\")\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    trace_rho = np.trace(rho_generalized)\n",
    "\n",
    "    # Use eigenvalue decomposition that supports non-Hermitian matrices\n",
    "    eigenvalues = np.linalg.eigvals(rho_generalized)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        log_eigenvalues = np.zeros_like(eigenvalues, dtype=complex)\n",
    "        non_zero = np.abs(eigenvalues) > regularization\n",
    "        log_eigenvalues[non_zero] = np.log2(eigenvalues[non_zero])\n",
    "\n",
    "        num_zero = np.sum(~non_zero)\n",
    "\n",
    "        if np.all(~non_zero):\n",
    "            # All eigenvalues are nonzero; nothing to report.\n",
    "            pass\n",
    "        elif num_zero == 1:\n",
    "            # One eigenvalue is zero (or below threshold); this is expected for a pure state. No warning needed.\n",
    "            pass\n",
    "        elif num_zero > 1:\n",
    "            display_and_log_print(f\"WARNING: {num_zero} eigenvalues are zero (or below threshold); pseudo-entropy may be ill-defined or degenerate.\")\n",
    "\n",
    "        # Optionally, still check for NaN or Inf in log_eigenvalues\n",
    "        if np.any(np.isnan(log_eigenvalues)):\n",
    "            display_and_log_print(\"WARNING: NaN encountered in log_eigenvalues!\")\n",
    "        if np.any(np.isinf(log_eigenvalues)):\n",
    "            display_and_log_print(\"WARNING: Inf encountered in log_eigenvalues!\")\n",
    "\n",
    "\n",
    "    # Calculate pseudo-entropy (can be complex)\n",
    "    pseudo_entropy = -np.sum(eigenvalues * log_eigenvalues)\n",
    "\n",
    "    # Reconstruct (log ρ)² using eigendecomposition\n",
    "    eigvecs = np.linalg.eig(rho_generalized)[1]\n",
    "    log_rho_squared = eigvecs @ np.diag(log_eigenvalues**2) @ np.linalg.inv(eigvecs)\n",
    "\n",
    "    # Expectation value of (log ρ)²\n",
    "    log_rho_squared_expectation = np.trace(log_rho_squared @ rho_generalized)\n",
    "\n",
    "    # Complex variance and real standard deviation\n",
    "    variance = log_rho_squared_expectation - pseudo_entropy**2\n",
    "    std_deviation = np.sqrt(np.real(variance)**2 + np.imag(variance)**2)\n",
    "\n",
    "    return pseudo_entropy, std_deviation, trace_rho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed2498",
   "metadata": {},
   "source": [
    "## Quantum Circuit Implementation\n",
    "Define the quantum circuit for post-selected measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd07e6",
   "metadata": {},
   "source": [
    "## Execute Experiments\n",
    "\n",
    "This section describes how the quantum circuits are executed and how the pseudo-entropy data is collected across the defined parameter sweeps. The `calculate_initial_state` and `calculate_final_state` functions prepare the necessary quantum states for pseudo-entropy computation. The `run_circuits_and_calculate_entropy` function then orchestrates the simulation loop, calculating pseudo-entropy and its standard deviation for each $(\\beta, \\delta)$ pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_state_circuit():\n",
    "    \"\"\"\n",
    "    Creates a parameterized circuit for the initial state preparation.\n",
    "    Returns:\n",
    "        QuantumCircuit: Circuit with Hadamard and X gates\n",
    "    \"\"\"\n",
    "    circuit = QuantumCircuit(2)\n",
    "    circuit.h(0)\n",
    "    circuit.x(1)\n",
    "    circuit.barrier(label='Initial State (|+1>)')\n",
    "    return circuit\n",
    "\n",
    "def calculate_initial_state(noise_model):\n",
    "    \"\"\"\n",
    "    Calculate the initial state with a rotation.\n",
    "\n",
    "    Args:\n",
    "        noise_model (NoiseModel): Quantum noise model\n",
    "    \n",
    "    Returns:\n",
    "        Statevector: The initial state\n",
    "    \"\"\"\n",
    "    initial_circuit = create_initial_state_circuit()\n",
    "    \n",
    "    simulator = Aer.get_backend('statevector_simulator')\n",
    "    transpiled_circuit = transpile(initial_circuit, simulator)\n",
    "    job = simulator.run(transpiled_circuit, noise_model=noise_model, shots=1)\n",
    "    result = job.result()\n",
    "    \n",
    "    return Statevector(result.get_statevector())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e949d",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "This section focuses on visualizing and interpreting the results of the quantum simulations. It includes functions for plotting the real, imaginary, magnitude, and phase components of the pseudo-entropy, providing a comprehensive view of its behavior across the parameter space. This section generates the full phase diagram plots, which are summarized and discussed in the main thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe85509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_measurement_circuit():\n",
    "    \"\"\"\n",
    "    Creates a parameterized circuit for the measurement.\n",
    "\n",
    "    Returns:\n",
    "        QuantumCircuit: Parameterized circuit with beta and delta rotations\n",
    "    \"\"\"\n",
    "    beta = Parameter('$\\\\beta$')\n",
    "    delta = Parameter('$\\\\delta$')\n",
    "    circuit = QuantumCircuit(2)\n",
    "    \n",
    "    circuit.barrier(label='Group 1: Entanglement')\n",
    "    circuit.cx(0, 1)\n",
    "\n",
    "    circuit.barrier(label='Group 2: Virtual Gates')\n",
    "    circuit.ry(delta, 1)\n",
    "    circuit.rz(beta+delta, 1)\n",
    "    circuit.rx(beta, 1)    \n",
    "\n",
    "    circuit.barrier(label='Group 3: Final State')\n",
    "    circuit.rz(np.pi/2, 1)\n",
    "\n",
    "    return circuit\n",
    "def calculate_final_state(beta, initial_state, noise_model, delta):\n",
    "    \"\"\"\n",
    "    Calculate the final state after entanglement.\n",
    "\n",
    "    Args:\n",
    "        beta (float): Angle for Ry and Rz rotations\n",
    "        initial_state (Statevector): The initial state of the system\n",
    "        noise_model (NoiseModel): Quantum noise model\n",
    "        delta (float): Angle for additional Rx rotation (coherent error)\n",
    "    \n",
    "    Returns:\n",
    "        Statevector: The final state\n",
    "    \"\"\"\n",
    "    measurement_circuit = create_measurement_circuit()\n",
    "    \n",
    "    # Initialize with the provided initial state\n",
    "    full_circuit = QuantumCircuit(2)\n",
    "    full_circuit.initialize(initial_state, [0, 1])\n",
    "    full_circuit = full_circuit.compose(measurement_circuit)\n",
    "    \n",
    "    # Assign parameters for beta and delta\n",
    "    assigned_circuit = full_circuit.assign_parameters({\n",
    "        measurement_circuit.parameters[0]: beta,\n",
    "        measurement_circuit.parameters[1]: delta\n",
    "    })\n",
    "    \n",
    "    simulator = Aer.get_backend('statevector_simulator')\n",
    "    transpiled_circuit = transpile(assigned_circuit, simulator)\n",
    "    job = simulator.run(transpiled_circuit, noise_model=noise_model, shots=1)\n",
    "    result = job.result()\n",
    "    final_state = Statevector(result.get_statevector())\n",
    "    \n",
    "    return final_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb77db42",
   "metadata": {},
   "source": [
    "## Classical vs Quantum Behavior Analysis\n",
    "\n",
    "This section focuses on identifying and visualizing regions where pseudo-entropy exhibits \"classical-like\" behavior versus \"quantum\" behavior. The `plot_classical_like_quantum` function applies a threshold to the imaginary component of pseudo-entropy to classify regions, generating heatmaps that visually distinguish these areas. It also provides statistical summaries of the proportion of classical-like, quantum, and undefined (NaN) points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_circuits_and_calculate_entropy(betas, deltas, noise_model):\n",
    "    \"\"\"\n",
    "    Execute quantum circuits and calculate pseudo-entropy and standard deviation for given parameters.\n",
    "    \n",
    "    Args:\n",
    "        betas (array): Array of angles for Ry and Rz rotations\n",
    "        deltas (array): Array of angles for additional Rx rotation (coherent error)\n",
    "        noise_model (NoiseModel): Quantum noise model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (pseudo_entropy, std_deviation) where each is a 2D array with shape (len(deltas), len(betas))\n",
    "    \"\"\"\n",
    "    pseudo_entropy = np.full((len(deltas), len(betas)), np.nan, dtype=complex)\n",
    "    std_deviation = np.full((len(deltas), len(betas)), np.nan)\n",
    "    traces_rho = np.full((len(deltas), len(betas)), np.nan, dtype=complex)\n",
    "    \n",
    "    initial_state = calculate_initial_state(noise_model)\n",
    "    \n",
    "    for j, beta in enumerate(betas):\n",
    "        for i, delta in enumerate(deltas):\n",
    "            try:\n",
    "                final_state = calculate_final_state(beta, initial_state, noise_model, delta)\n",
    "                entropy_value, std_value, trace_rho = calculate_pseudo_entropy(initial_state, final_state)\n",
    "                if np.isnan(entropy_value) or np.isnan(std_value) or np.isnan(trace_rho):\n",
    "                    display_and_log_markdown(fr\"Calculated values contain NaN, indicating a numerical issue in the pseudo-entropy calculation - $\\beta/\\pi$:{beta/np.pi}, $\\delta/\\pi$:{delta/np.pi}$\")\n",
    "                    continue\n",
    "                # Store the results in the respective arrays\n",
    "                pseudo_entropy[i, j] = entropy_value\n",
    "                std_deviation[i, j] = std_value\n",
    "                traces_rho[i, j] = trace_rho\n",
    "            except Exception as e:\n",
    "                display_and_log_print(f\"Error at beta {beta}, delta {delta}: {e}\")\n",
    "\n",
    "    return pseudo_entropy, std_deviation, traces_rho\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80fcb3",
   "metadata": {},
   "source": [
    "## Analysis and Visualization Functions\n",
    "Define functions for analyzing and plotting results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ab569",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis\n",
    "\n",
    "This section quantifies how changes in the $\\beta$ and $\\delta$ parameters affect the pseudo-entropy. It includes functions to calculate sensitivity maps (inverse of the gradient magnitude), which visually highlight regions of high and low responsiveness of pseudo-entropy to parameter variations. These maps are crucial for identifying optimal operating points for error detection and calibration. The detailed sub-subsection analysis of sensitivity methods is offloaded to the supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_circuit(circuit, filename_prefix, display_name, results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Process a single circuit: display as markdown, save PNG and LaTeX.\n",
    "    \n",
    "    Args:\n",
    "        circuit: QuantumCircuit object to process\n",
    "        filename_prefix: Prefix for saved files\n",
    "        display_name: Name to display in markdown\n",
    "        results_dir: Directory to save files\n",
    "    \n",
    "    Returns:\n",
    "        The processed circuit\n",
    "    \"\"\"\n",
    "    display_and_log_print(f\"\\n## {display_name}\")\n",
    "    \n",
    "    # Display circuit as markdown (text representation)\n",
    "    circuit_text = str(circuit.draw(output='text'))\n",
    "    display_and_log_print(\"```\")\n",
    "    display_and_log_print(circuit_text)\n",
    "    display_and_log_print(\"```\")\n",
    "    \n",
    "    # Save circuit as PNG image\n",
    "    try:\n",
    "        fig = circuit.draw(output='mpl', style='textbook')\n",
    "        png_path = os.path.join(results_dir, f'{filename_prefix}.png')\n",
    "        fig.savefig(png_path, dpi=300, bbox_inches='tight')\n",
    "        display_and_log_print(f\"Circuit saved as: {png_path}\")\n",
    "        plt.close(fig)\n",
    "    except Exception as e:\n",
    "        display_and_log_print(f\"Error saving {filename_prefix} PNG: {e}\")\n",
    "    \n",
    "    # Save LaTeX source\n",
    "    try:\n",
    "        latex_path = os.path.join(results_dir, f'{filename_prefix}.tex')\n",
    "        with open(latex_path, 'w') as f:\n",
    "            f.write(circuit.draw(output='latex_source'))\n",
    "        display_and_log_print(f\"LaTeX source saved as: {latex_path}\")\n",
    "    except Exception as e:\n",
    "        display_and_log_print(f\"Error saving {filename_prefix} LaTeX: {e}\")\n",
    "    \n",
    "    return circuit\n",
    "\n",
    "\n",
    "def create_concatenated_circuit(initial_circuit, measurement_circuit):\n",
    "    \"\"\"\n",
    "    Create a concatenated circuit from initial and measurement circuits.\n",
    "    \n",
    "    Args:\n",
    "        initial_circuit: QuantumCircuit for initial state preparation\n",
    "        measurement_circuit: QuantumCircuit for measurement operations\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: Concatenated circuit with barriers\n",
    "    \"\"\"\n",
    "    display_and_log_print(\"\\n## Creating Concatenated Circuit\")\n",
    "    \n",
    "    # Determine maximum dimensions needed\n",
    "    max_qubits = max(initial_circuit.num_qubits, measurement_circuit.num_qubits)\n",
    "    max_clbits = max(initial_circuit.num_clbits, measurement_circuit.num_clbits)\n",
    "    \n",
    "    # Create new circuit with unified register structure\n",
    "    concatenated_circuit = QuantumCircuit(max_qubits, max_clbits)\n",
    "    concatenated_circuit.name = \"Concatenated Circuit (Initial + Measurement)\"\n",
    "    \n",
    "    # Add barriers for clarity and compose circuits\n",
    "    concatenated_circuit.barrier()\n",
    "    concatenated_circuit.compose(initial_circuit, inplace=True)\n",
    "    concatenated_circuit.barrier()\n",
    "    concatenated_circuit.compose(measurement_circuit, inplace=True)\n",
    "    concatenated_circuit.barrier()\n",
    "    \n",
    "    return concatenated_circuit\n",
    "\n",
    "\n",
    "def display_all_circuits():\n",
    "    \"\"\"\n",
    "    Creates and returns all circuits for visualization using markdown rendering.\n",
    "    Saves rendered circuits to results/ folder as PNG and LaTeX source.\n",
    "    \"\"\"\n",
    "    results_dir = \"results\"\n",
    "    \n",
    "    # Create the individual circuits\n",
    "    initial_circuit = create_initial_state_circuit()\n",
    "    measurement_circuit = create_measurement_circuit()\n",
    "    \n",
    "    # Set circuit names\n",
    "    initial_circuit.name = \"Initial State Circuit\"\n",
    "    measurement_circuit.name = \"Pseudo-Entropy Measurement Circuit\"\n",
    "    \n",
    "    # Process each circuit\n",
    "    display_and_log_print(\"# Quantum Circuit Visualization\")\n",
    "    \n",
    "    initial_circuit = process_circuit(\n",
    "        initial_circuit, \n",
    "        'initial_state_circuit', \n",
    "        'Initial State Circuit',\n",
    "        results_dir\n",
    "    )\n",
    "    \n",
    "    measurement_circuit = process_circuit(\n",
    "        measurement_circuit, \n",
    "        'measurement_circuit', \n",
    "        'Measurement Circuit',\n",
    "        results_dir\n",
    "    )\n",
    "    \n",
    "    # Create and process concatenated circuit\n",
    "    concatenated_circuit = create_concatenated_circuit(initial_circuit, measurement_circuit)\n",
    "    concatenated_circuit = process_circuit(\n",
    "        concatenated_circuit, \n",
    "        'concatenated_circuit', \n",
    "        'Concatenated Circuit',\n",
    "        results_dir\n",
    "    )\n",
    "    \n",
    "    # Create comprehensive summary file\n",
    "    try:\n",
    "        summary_content = f\"\"\"# Quantum Circuit Summary\n",
    "\n",
    "## Circuit Statistics:\n",
    "- **Initial State Circuit**: {initial_circuit.num_qubits} qubits, depth {initial_circuit.depth()}\n",
    "- **Measurement Circuit**: {measurement_circuit.num_qubits} qubits, depth {measurement_circuit.depth()}\n",
    "- **Concatenated Circuit**: {concatenated_circuit.num_qubits} qubits, depth {concatenated_circuit.depth()}\n",
    "\n",
    "## Generated Files:\n",
    "- `initial_state_circuit.png` - Initial state circuit diagram\n",
    "- `initial_state_circuit.tex` - Initial state circuit LaTeX source\n",
    "- `measurement_circuit.png` - Measurement circuit diagram  \n",
    "- `measurement_circuit.tex` - Measurement circuit LaTeX source\n",
    "- `concatenated_circuit.png` - Concatenated circuit diagram\n",
    "- `concatenated_circuit.tex` - Concatenated circuit LaTeX source\n",
    "\n",
    "## Operation Counts:\n",
    "\n",
    "### Initial State Circuit:\n",
    "{dict(initial_circuit.count_ops())}\n",
    "\n",
    "### Measurement Circuit:\n",
    "{dict(measurement_circuit.count_ops())}\n",
    "\n",
    "### Concatenated Circuit:\n",
    "{dict(concatenated_circuit.count_ops())}\n",
    "\"\"\"\n",
    "        \n",
    "        summary_path = os.path.join(results_dir, 'circuit_summary.md')\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(summary_content)\n",
    "        display_and_log_print(f\"\\nSummary saved as: {summary_path}\")\n",
    "    except Exception as e:\n",
    "        display_and_log_print(f\"Error saving summary: {e}\")\n",
    "    \n",
    "    display_and_log_print(f\"\\n✅ All circuits processed and saved to '{results_dir}/' folder\")\n",
    "    \n",
    "    return {\n",
    "        'initial_circuit': initial_circuit,\n",
    "        'measurement_circuit': measurement_circuit,\n",
    "        'concatenated_circuit': concatenated_circuit\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_excel_formatting(worksheet, df):\n",
    "    \"\"\"\n",
    "    Apply Excel formatting to the worksheet based on column content and values.\n",
    "    Also formats the data as an Excel table.\n",
    "    \n",
    "    Args:\n",
    "        worksheet: openpyxl worksheet object\n",
    "        df (pd.DataFrame): DataFrame containing the data\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"    \n",
    "    # Create named styles for different formatting needs\n",
    "    workbook = worksheet.parent\n",
    "    \n",
    "    # Create and register styles (skip if already exists)\n",
    "    styles_to_create = [\n",
    "        (\"scientific\", '0.00E+00'),\n",
    "        (\"two_decimal\", '0.00'),\n",
    "        (\"percentage\", '0.00%'),\n",
    "        (\"percentage_append\", '0.00\"%\"'),\n",
    "        (\"percentage_scientific\", '0.00E+00'),\n",
    "        (\"integer\", '0')\n",
    "    ]\n",
    "    \n",
    "    for style_name, number_format in styles_to_create:\n",
    "        try:\n",
    "            style = NamedStyle(name=style_name)\n",
    "            style.number_format = number_format\n",
    "            workbook.add_named_style(style)\n",
    "        except ValueError:\n",
    "            # Style already exists, skip\n",
    "            pass\n",
    "    \n",
    "    # Apply formatting to each column\n",
    "    for col_idx, column_name in enumerate(df.columns, 1):\n",
    "        for row_idx in range(2, len(df) + 2):  # Start from row 2 (after header)\n",
    "            cell = worksheet.cell(row=row_idx, column=col_idx)\n",
    "            value = df.iloc[row_idx - 2, col_idx - 1]\n",
    "            \n",
    "            if pd.isna(value):\n",
    "                continue\n",
    "                \n",
    "            # Apply formatting based on column name and value\n",
    "            if column_name == 'Length (Cells)':\n",
    "                # Integer formatting for Length column\n",
    "                cell.style = \"integer\"\n",
    "            elif column_name == 'Threshold':\n",
    "                # Threshold column: percentage unless < 10^-3, then scientific\n",
    "                if abs(value) < 1e-3:\n",
    "                    cell.style = \"percentage_scientific\"\n",
    "                else:\n",
    "                    cell.style = \"percentage\"\n",
    "            elif '(%)' in column_name:\n",
    "                # Percentage columns: display as percentage\n",
    "                cell.style = \"percentage_append\"\n",
    "            elif isinstance(value, (int, float)):\n",
    "                # Other numeric columns: scientific if < 0.01 or > 1000, otherwise 2 decimals\n",
    "                if abs(value) < 0.01 or abs(value) > 1000:\n",
    "                    cell.style = \"scientific\"\n",
    "                else:\n",
    "                    cell.style = \"two_decimal\"\n",
    "    \n",
    "    # Format as Excel table if there's data\n",
    "    if len(df) > 0:\n",
    "        # Define table range (including headers)\n",
    "        table_range = f\"A1:{chr(64 + len(df.columns))}{len(df) + 1}\"\n",
    "        \n",
    "        # Create table with a unique name based on worksheet name\n",
    "        table_name = f\"Table_{worksheet.title}\"\n",
    "        table = Table(displayName=table_name, ref=table_range)\n",
    "        \n",
    "        # Add table style\n",
    "        style = TableStyleInfo(\n",
    "            name=\"TableStyleMedium9\", \n",
    "            showFirstColumn=False,\n",
    "            showLastColumn=False, \n",
    "            showRowStripes=True, \n",
    "            showColumnStripes=False\n",
    "        )\n",
    "        table.tableStyleInfo = style\n",
    "        \n",
    "        # Add the table to the worksheet\n",
    "        worksheet.add_table(table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90587e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complex_statistics(data):\n",
    "    \"\"\"\n",
    "    Calculate statistics for complex data including real, imaginary, magnitude, and phase parts.\n",
    "    Ignores NaN values in all calculations.\n",
    "\n",
    "    Args:\n",
    "        data (complex ndarray): 2D array of complex values\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing statistics for all components\n",
    "    \"\"\"\n",
    "    # Standard statistics with NaN handling\n",
    "    stats = {\n",
    "        'real': {\n",
    "            'max': np.nanmax(data.real),\n",
    "            'avg': np.nanmean(data.real),\n",
    "            'median': np.nanmedian(data.real),\n",
    "            'min': np.nanmin(data.real),\n",
    "            'std': np.nanstd(data.real),\n",
    "        },\n",
    "        'imaginary': {\n",
    "            'max': np.nanmax(data.imag),\n",
    "            'avg': np.nanmean(data.imag),\n",
    "            'median': np.nanmedian(data.imag),\n",
    "            'min': np.nanmin(data.imag),\n",
    "            'std': np.nanstd(data.imag), \n",
    "        },\n",
    "        'magnitude': {\n",
    "            'max': np.nanmax(np.abs(data)),\n",
    "            'avg': np.nanmean(np.abs(data)),\n",
    "            'median': np.nanmedian(np.abs(data)),\n",
    "            'min': np.nanmin(np.abs(data)),\n",
    "            'std': np.nanstd(np.abs(data)),\n",
    "        },\n",
    "        'phase': {\n",
    "            'max': np.nanmax(np.angle(data, deg=False) / np.pi),\n",
    "            'avg': np.nanmean(np.angle(data, deg=False) / np.pi),\n",
    "            'median': np.nanmedian(np.angle(data, deg=False) / np.pi),\n",
    "            'min': np.nanmin(np.angle(data, deg=False) / np.pi),\n",
    "            'std': np.nanstd(np.angle(data, deg=False) / np.pi),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "def statistics_to_dataframe(stats_dict, data_type):\n",
    "    \"\"\"\n",
    "    Converts the statistics dictionary into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df_data = []\n",
    "    for stat_name in ['max', 'avg', 'median', 'min', 'std']:\n",
    "        row = {\n",
    "            'Statistic': stat_name.capitalize(),\n",
    "            'Real Part': stats_dict['real'][stat_name],\n",
    "            'Imaginary Part': stats_dict['imaginary'][stat_name],\n",
    "            'Magnitude': stats_dict['magnitude'][stat_name],\n",
    "            'Phase/π': stats_dict['phase'][stat_name],\n",
    "            'Data Type': data_type\n",
    "        }\n",
    "        df_data.append(row)\n",
    "    return pd.DataFrame(df_data)\n",
    "\n",
    "def export_statistics_to_excel(stats_df, filename='results/statistics_results.xlsx'):\n",
    "    \"\"\"\n",
    "    Exports a pandas DataFrame of statistics to an Excel file, appending to existing sheets\n",
    "    or creating new ones.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    writer = pd.ExcelWriter(filename, engine='openpyxl')\n",
    "\n",
    "    sheet_name = stats_df['Data Type'].iloc[0] if not stats_df.empty else 'Sheet1'\n",
    "    \n",
    "    # If the sheet already exists, remove it to prevent duplicate headers\n",
    "    if sheet_name in writer.book.sheetnames:\n",
    "        idx = writer.book.sheetnames.index(sheet_name)\n",
    "        writer.book.remove(writer.book.worksheets[idx])\n",
    "    \n",
    "    stats_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    apply_excel_formatting(writer.sheets[sheet_name], stats_df) # Apply formatting\n",
    "    \n",
    "    writer.close() # Use close() instead of save() for openpyxl engine\n",
    "    display_and_log_print(f\"Statistics data exported to {filename} (Sheet: {sheet_name})\")\n",
    "\n",
    "\n",
    "def display_statistics_table(stats, data_type, data):\n",
    "    \"\"\"\n",
    "    Display calculated statistics in a formatted Markdown table, including middle value and required shots.\n",
    "    Also exports the statistics to an Excel file.\n",
    "\n",
    "    Args:\n",
    "        stats (dict): Statistics dictionary from calculate_complex_statistics.\n",
    "        data_type (str): Description of the analyzed data type.\n",
    "        data (complex ndarray): Original 2D array of complex values to extract middle point.\n",
    "    \"\"\"\n",
    "    # Calculate middle indices\n",
    "    mid_row = data.shape[0] // 2\n",
    "    mid_col = data.shape[1] // 2\n",
    "    mid_value = data[mid_row, mid_col]\n",
    "\n",
    "    # Format the table with standard deviation values\n",
    "    stats_table = (\n",
    "        f\"### {data_type} Statistics\\n\\n\"\n",
    "        \"| Statistic | Real Part | Imaginary Part | Magnitude | Phase/π |\\n\"\n",
    "        \"|-----------|-----------|----------------|-----------|----------|\\n\"\n",
    "        f\"| Maximum  | ${to_latex(round_number(stats['real']['max']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['imaginary']['max']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['magnitude']['max']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['phase']['max']))}$ |\\n\"\n",
    "        f\"| Average  | ${to_latex(round_number(stats['real']['avg']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['imaginary']['avg']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['magnitude']['avg']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['phase']['avg']))}$ |\\n\"\n",
    "        f\"| Median   | ${to_latex(round_number(stats['real']['median']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['imaginary']['median']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['magnitude']['median']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['phase']['median']))}$ |\\n\"\n",
    "        f\"| Minimum  | ${to_latex(round_number(stats['real']['min']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['imaginary']['min']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['magnitude']['min']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['phase']['min']))}$ |\\n\"\n",
    "        f\"| Standard Deviation | \"\n",
    "        f\"${to_latex(round_number(stats['real']['std']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['imaginary']['std']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['magnitude']['std']))}$ | \"\n",
    "        f\"${to_latex(round_number(stats['phase']['std']))}$ |\\n\"\n",
    "        f\"| Zero Point | \"\n",
    "        f\"${to_latex(round_number(mid_value.real))}$ | \"\n",
    "        f\"${to_latex(round_number(mid_value.imag))}$ | \"\n",
    "        f\"${to_latex(round_number(abs(mid_value)))}$ | \"\n",
    "        f\"${to_latex(round_number(np.angle(mid_value, deg=False) / np.pi))}$ |\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    display_and_log_markdown(stats_table)\n",
    "\n",
    "    # Export to Excel\n",
    "    stats_df = statistics_to_dataframe(stats, data_type)\n",
    "    export_statistics_to_excel(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a63d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_matplotlib_style(font_size=20, label_size=22, title_size=24):\n",
    "    \"\"\"\n",
    "    Configure the default matplotlib style for enhanced LaTeX rendering \n",
    "    and consistent font sizes in plots.\n",
    "    \"\"\"\n",
    "    plt.style.use('default')\n",
    "    plt.rcParams.update({\n",
    "        'text.usetex': True,\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['Computer Modern Roman'],\n",
    "        'axes.formatter.use_mathtext': True,\n",
    "        'font.size': font_size,\n",
    "        'axes.labelsize': label_size,\n",
    "        'axes.titlesize': title_size,\n",
    "        'xtick.labelsize': 18,\n",
    "        'ytick.labelsize': 18,\n",
    "        'text.color': 'black',\n",
    "        'axes.labelcolor': 'black',\n",
    "        'xtick.color': 'black',\n",
    "        'ytick.color': 'black',\n",
    "        'text.latex.preamble': r'\\usepackage{amsmath} \\usepackage{amssymb}'\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colormap(scenario_key):\n",
    "    \"\"\"\n",
    "    Select appropriate colormaps based on the given scenario.\n",
    "\n",
    "    Args:\n",
    "        scenario_key (str): Key indicating the simulation scenario.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Four colormaps for real, imaginary, magnitude, and phase plots.\n",
    "    \"\"\"\n",
    "    colormaps = {\n",
    "        'depolarization_impact': ('RdBu', 'RdBu', 'viridis', 'twilight_shifted'),\n",
    "        'theory_vs_simulation': ('seismic', 'seismic', 'viridis', 'twilight_shifted')\n",
    "    }\n",
    "    return colormaps.get(scenario_key, ('viridis', 'plasma', 'magma', 'twilight_shifted'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42667b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subplot(fig, ax, data, extent, cmap, title, colorbar_label):\n",
    "    \"\"\"\n",
    "    Create and format a single subplot with given data and settings.\n",
    "\n",
    "    Args:\n",
    "        fig (Figure): Matplotlib figure to hold the subplot.\n",
    "        ax (Axes): Matplotlib axes to plot on.\n",
    "        data (ndarray): Data to visualize in the subplot.\n",
    "        extent (list): Plot extent for axes.\n",
    "        cmap (str): Colormap name for the plot.\n",
    "        title (str): Title for the subplot.\n",
    "        colorbar_label (str): Label for the colorbar.\n",
    "    \n",
    "    Returns:\n",
    "        AxesImage: The plotted image.\n",
    "    \"\"\"\n",
    "    im = ax.imshow(data, extent=extent, aspect='auto', origin='lower', cmap=cmap)\n",
    "    ax.set_xlabel(r'Controlled Interaction Strength $\\beta/\\pi$')\n",
    "    ax.set_ylabel(r'Coherent Error Angle $\\delta/\\pi$')\n",
    "    ax.set_title(title)\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label(colorbar_label)\n",
    "    return im\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df61b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(fig, filename, suptitle, folder='results', width=0.9):\n",
    "    \"\"\"\n",
    "    Save a figure to a specified folder and generate LaTeX figure code.\n",
    "\n",
    "    Args:\n",
    "        fig (Figure): Matplotlib figure to save.\n",
    "        filename (str): File name for the figure.\n",
    "        suptitle (str): Caption for the figure in LaTeX.\n",
    "        folder (str): Target folder for saving the figure.\n",
    "        width (float): Width of the figure in LaTeX.\n",
    "    \"\"\"\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Save the figure\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    fig.savefig(filepath, bbox_inches='tight', dpi=300, format='png')\n",
    "\n",
    "    # Prepare LaTeX figure code\n",
    "    caption = suptitle.replace('\\n', '. ')  # Format caption for LaTeX\n",
    "    label = f\"fig:{os.path.splitext(filename)[0]}\"  # Extract name without extension\n",
    "    latex_code = (\n",
    "        f\"\\\\begin{{figure}}[ht]\\n\"\n",
    "        f\"\\\\centering\\n\"\n",
    "        f\"\\\\includegraphics[width={width}\\\\textwidth]{{{filepath}}}\\n\"\n",
    "        f\"\\\\caption{{{caption}}}\\n\"\n",
    "        f\"\\\\label{{{label}}}\\n\"\n",
    "        f\"\\\\end{{figure}}\"\n",
    "    )\n",
    "\n",
    "    # Display and log the LaTeX code\n",
    "    display_and_log_print(latex_code)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standard_plots(betas, deltas, data, extent, scenario_key, scenario_title, base_subtitle,\n",
    "                         cmap_real, cmap_imag, cmap_mag, cmap_phase):\n",
    "    \"\"\"\n",
    "    Create and save the standard (non-zoomed) plots for both Cartesian and Polar coordinates.\n",
    "    \n",
    "    Args:\n",
    "        betas, deltas: Arrays of measurement angles\n",
    "        data: Complex data array to plot\n",
    "        extent: Plot extents for the axes\n",
    "        scenario_key: Key for identifying the scenario\n",
    "        scenario_title: Title for the scenario\n",
    "        base_subtitle: Base subtitle for the plots\n",
    "        cmap_*: Colormaps for different components\n",
    "    \"\"\"\n",
    "    # Detailed subtitles for each figure\n",
    "    detailed_subtitle1 = f\"{scenario_title} (Cartesian Coordinates)\\n{base_subtitle}\"\n",
    "    detailed_subtitle2 = f\"{scenario_title} (Polar Coordinates)\\n{base_subtitle}\"\n",
    "    \n",
    "    # Figure 1: Cartesian representation (real and imaginary parts)\n",
    "    fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), dpi=300)\n",
    "    \n",
    "    create_subplot(fig1, ax1, data.real, extent, cmap_real, \n",
    "                   'Real Part',\n",
    "                   r'$\\mathrm{Re}(\\hat{S})$ [bits]')\n",
    "    \n",
    "    create_subplot(fig1, ax2, data.imag, extent, cmap_imag,\n",
    "                   'Imaginary Part',\n",
    "                   r'$\\mathrm{Im}(\\hat{S})$ [bits]')\n",
    "    \n",
    "    # Set and save the Cartesian plot\n",
    "    fig1.suptitle(detailed_subtitle1, fontsize=22)\n",
    "    fig1.tight_layout(rect=[0, 0, 1, 1])\n",
    "    save_figure(fig1, f'{scenario_key}_cartesian.png', detailed_subtitle1)\n",
    "    \n",
    "    # Figure 2: Polar representation (magnitude and phase)\n",
    "    fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), dpi=300)\n",
    "    \n",
    "    create_subplot(fig2, ax1, np.abs(data), extent, cmap_mag, \n",
    "                   'Magnitude',\n",
    "                   r'$|\\hat{S}|$ [bits]')\n",
    "    \n",
    "    create_subplot(fig2, ax2, np.angle(data) / np.pi, extent, cmap_phase,\n",
    "                   'Normalized Phase',\n",
    "                   r'$\\angle(\\hat{S}) / \\pi$')\n",
    "    \n",
    "    # Set and save the Polar plot\n",
    "    fig2.suptitle(detailed_subtitle2, fontsize=22)\n",
    "    fig2.tight_layout(rect=[0, 0, 1, 1])\n",
    "    save_figure(fig2, f'{scenario_key}_polar.png', detailed_subtitle2)\n",
    "\n",
    "def create_zoomed_plots(betas, deltas, data, extent, scenario_key, scenario_title, base_subtitle,\n",
    "                        cmap_real, cmap_imag, cmap_mag, cmap_phase, zoom_factor):\n",
    "    \"\"\"\n",
    "    Create and save zoomed plots for both Cartesian and Polar coordinates.\n",
    "    \n",
    "    Args:\n",
    "        betas, deltas: Arrays of measurement angles\n",
    "        data: Complex data array to plot\n",
    "        extent: Plot extents for the axes\n",
    "        scenario_key: Key for identifying the scenario\n",
    "        scenario_title: Title for the scenario\n",
    "        base_subtitle: Base subtitle for the plots\n",
    "        cmap_*: Colormaps for different components\n",
    "        zoom_factor: Factor to zoom in on the central region\n",
    "    \"\"\"\n",
    "    # Calculate zoomed extents\n",
    "    center_beta = (extent[0] + extent[1]) / 2\n",
    "    center_delta = (extent[2] + extent[3]) / 2\n",
    "    \n",
    "    # Calculate zoom width and height\n",
    "    zoom_width = (extent[1] - extent[0]) / zoom_factor\n",
    "    zoom_height = (extent[3] - extent[2]) / zoom_factor\n",
    "    \n",
    "    # Calculate new extents\n",
    "    zoomed_extent = [\n",
    "        center_beta - zoom_width/2,\n",
    "        center_beta + zoom_width/2,\n",
    "        center_delta - zoom_height/2,\n",
    "        center_delta + zoom_height/2\n",
    "    ]\n",
    "    \n",
    "    # Calculate indices for slicing the data array\n",
    "    beta_indices = np.logical_and(\n",
    "        betas/np.pi >= zoomed_extent[0],\n",
    "        betas/np.pi <= zoomed_extent[1]\n",
    "    )\n",
    "    delta_indices = np.logical_and(\n",
    "        deltas/np.pi >= zoomed_extent[2],\n",
    "        deltas/np.pi <= zoomed_extent[3]\n",
    "    )\n",
    "    \n",
    "    # Extract the zoomed data region\n",
    "    zoomed_data = data[np.ix_(delta_indices, beta_indices)]\n",
    "    \n",
    "    # Detailed subtitles for zoomed figures\n",
    "    zoom_subtitle1 = f\"{scenario_title} (Cartesian Coordinates - {zoom_factor}x Zoom)\\n{base_subtitle}\"\n",
    "    zoom_subtitle2 = f\"{scenario_title} (Polar Coordinates - {zoom_factor}x Zoom)\\n{base_subtitle}\"\n",
    "    \n",
    "    display_and_log_print(f\"Generating {zoom_factor}x zoomed plots centered at β/π = {center_beta:.3f}, δ/π = {center_delta:.3f}\")\n",
    "    \n",
    "    # Figure 3: Zoomed Cartesian representation\n",
    "    fig3, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), dpi=300)\n",
    "    \n",
    "    create_subplot(fig3, ax1, zoomed_data.real, zoomed_extent, cmap_real, \n",
    "                   f'Real Part (Zoomed {zoom_factor}x)',\n",
    "                   r'$\\mathrm{Re}(\\hat{S})$ [bits]')\n",
    "    \n",
    "    create_subplot(fig3, ax2, zoomed_data.imag, zoomed_extent, cmap_imag,\n",
    "                   f'Imaginary Part (Zoomed {zoom_factor}x)',\n",
    "                   r'$\\mathrm{Im}(\\hat{S})$ [bits]')\n",
    "    \n",
    "    # Set and save the zoomed Cartesian plot\n",
    "    fig3.suptitle(zoom_subtitle1, fontsize=22)\n",
    "    fig3.tight_layout(rect=[0, 0, 1, 1])\n",
    "    save_figure(fig3, f'{scenario_key}_cartesian_zoomed_{zoom_factor}x.png', zoom_subtitle1)\n",
    "    \n",
    "    # Figure 4: Zoomed Polar representation\n",
    "    fig4, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), dpi=300)\n",
    "    \n",
    "    create_subplot(fig4, ax1, np.abs(zoomed_data), zoomed_extent, cmap_mag, \n",
    "                   f'Magnitude (Zoomed {zoom_factor}x)',\n",
    "                   r'$|\\hat{S}|$ [bits]')\n",
    "    \n",
    "    create_subplot(fig4, ax2, np.angle(zoomed_data) / np.pi, zoomed_extent, cmap_phase,\n",
    "                   f'Normalized Phase (Zoomed {zoom_factor}x)',\n",
    "                   r'$\\angle(\\hat{S}) / \\pi$')\n",
    "    \n",
    "    # Set and save the zoomed Polar plot\n",
    "    fig4.suptitle(zoom_subtitle2, fontsize=22)\n",
    "    fig4.tight_layout(rect=[0, 0, 1, 1])\n",
    "    save_figure(fig4, f'{scenario_key}_polar_zoomed_{zoom_factor}x.png', zoom_subtitle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(betas, deltas, data, scenario_key, initial_state_string, zoom_factor=None):\n",
    "    \"\"\"\n",
    "    Generate visualizations of quantum measurement results across two coordinate systems:\n",
    "    1. Cartesian coordinates (real and imaginary parts)\n",
    "    2. Polar coordinates (magnitude and phase)\n",
    "\n",
    "    Args:\n",
    "        betas (array): Array of measurement angles for the Ry and Rz gates.\n",
    "        deltas (array): Array of coherent error angles for the Rx gate.\n",
    "        data (complex ndarray): Complex array containing pseudo-entropy or related values to plot.\n",
    "        scenario_key (str): Key identifying the type of data being analyzed, used to select titles and color schemes.\n",
    "        initial_state_string (str): String representation of the initial quantum state.\n",
    "        zoom_factor (float, optional): Factor to zoom in on central region of the plot. If None, no zoom is applied.\n",
    "\n",
    "    Description:\n",
    "        This function visualizes the results of quantum simulations for various scenarios. \n",
    "        Four figures can be generated depending on parameters:\n",
    "        - The first figure illustrates the **real** and **imaginary** components of the data.\n",
    "        - The second figure presents the **magnitude** and **phase** components.\n",
    "        - If zoom_factor is provided, two additional zoomed figures are generated.\n",
    "        Each plot uses colormaps to emphasize the structure of the data.\n",
    "\n",
    "        Results are saved as PNG files and displayed interactively. Statistics for the data are calculated\n",
    "        and presented in a tabular format.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Set up the global Matplotlib style for consistent aesthetics\n",
    "    setup_matplotlib_style()\n",
    "    \n",
    "    # Define plot extents and colormaps based on input parameters and scenario type\n",
    "    extent = [betas[0] / np.pi, betas[-1] / np.pi, deltas[0] / np.pi, deltas[-1] / np.pi]\n",
    "    cmap_real, cmap_imag, cmap_mag, cmap_phase = create_colormap(scenario_key)\n",
    "    \n",
    "    # Scenario-specific titles for plots\n",
    "    scenario_titles = {\n",
    "        'coherent_only': \"Coherent Routing Error Analysis\",\n",
    "        'coherent_and_depolarization': \"Combined Coherent Routing Error and Depolarization Noise Evaluation\",\n",
    "        'depolarization_impact': \"Depolarization Noise Comprehensive Impact Study\",\n",
    "        'theory_vs_simulation': \"Pseudo-Entropy: Approximity Modeling vs Simulation Comparison\"\n",
    "    }\n",
    "    scenario_title = scenario_titles.get(scenario_key, \"Unknown Scenario\")\n",
    "    \n",
    "    # Base subtitle shared across figures\n",
    "    base_subtitle = (\n",
    "        f\"Simulation of Error findings with Discrete Quantum Circuits\\n\"\n",
    "        f\"Pseudo-Entropy Calculation between Initial State and Final State\\n\"\n",
    "        f'Initial State: ${initial_state_string}$'\n",
    "    )\n",
    "    \n",
    "    # Generate standard plots\n",
    "    create_standard_plots(betas, deltas, data, extent, scenario_key, scenario_title, base_subtitle,\n",
    "                         cmap_real, cmap_imag, cmap_mag, cmap_phase)\n",
    "    \n",
    "    # Generate zoomed plots if zoom_factor is provided\n",
    "    if zoom_factor is not None:\n",
    "        create_zoomed_plots(betas, deltas, data, extent, scenario_key, scenario_title, base_subtitle,\n",
    "                         cmap_real, cmap_imag, cmap_mag, cmap_phase, zoom_factor)\n",
    "    \n",
    "    # Compute and display statistics for the data\n",
    "    stats = calculate_complex_statistics(data)\n",
    "    \n",
    "    # Display the figures interactively\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a9e36",
   "metadata": {},
   "source": [
    "# Data Analysis Functions\n",
    "\n",
    "This section orchestrates the entire simulation and analysis pipeline. It defines the parameter grids, runs the simulations, performs the pseudo-entropy calculations, and generates all the required plots and statistical outputs. It also includes functions for fitting the pseudo-entropy data to parabolic and linear models, calculating parameter errors, and exporting results to Excel for detailed external analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594ff6f",
   "metadata": {},
   "source": [
    "## Execute Experiments\n",
    "\n",
    "This subsection manages the execution or loading of simulation results. It ensures that the `results` directory exists and handles the caching of computationally intensive pseudo-entropy calculations to avoid redundant computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the results directory exists\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Create or clear the output.txt file\n",
    "with open('results/output.txt', 'w') as file:\n",
    "    pass  # This creates/clears the file without writing anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate angle ranges\n",
    "betas = np.linspace(-BETA_RANGE, BETA_RANGE, BETA_POINTS)\n",
    "deltas = np.linspace(-DELTA_RANGE, DELTA_RANGE, DELTA_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate step sizes\n",
    "beta_step = (betas[-1] - betas[0]) / (BETA_POINTS - 1)\n",
    "delta_step = (deltas[-1] - deltas[0]) / (DELTA_POINTS - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8475c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and display range and step size information as a Markdown table\n",
    "parameter_info_table = (\n",
    "    \"### Angle Range and Step Size Information\\n\\n\"\n",
    "    \"| Parameter | Minimum ($\\\\pi$) | Maximum ($\\\\pi$) | Step Size ($\\\\pi$) | Number of Points |\\n\"\n",
    "    \"|-----------|-----------------|-----------------|-------------------|------------------|\\n\"\n",
    "    f\"| $\\\\beta$  | ${to_latex(round_number(betas[0] / np.pi))}$ | ${to_latex(round_number(betas[-1] / np.pi))}$ | ${to_latex(round_number(beta_step / np.pi))}$ | {BETA_POINTS} |\\n\"\n",
    "    f\"| $\\\\delta$ | ${to_latex(round_number(deltas[0] / np.pi))}$ | ${to_latex(round_number(deltas[-1] / np.pi))}$ | ${to_latex(round_number(delta_step / np.pi))}$ | {DELTA_POINTS} |\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd5def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table\n",
    "display_and_log_markdown(parameter_info_table)\n",
    "# Print Machine epsilon\n",
    "display_and_log_markdown(f\"**Machine epsilon** ($\\\\epsilon$): ${to_latex(round_number(np.finfo(float).eps))}$\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2391127",
   "metadata": {},
   "source": [
    "## Run the full experiment with both numerical and analytical simulations\n",
    "\n",
    "This section orchestrates the execution of the full experiment, running or loading simulations for both numerical and (conceptually) analytical scenarios. The `load_or_run_entropy_calculation` function manages the persistence of simulation results, ensuring that lengthy computations are only performed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_run_entropy_calculation(file_path, betas, deltas, noise_model):\n",
    "    \"\"\"\n",
    "    Load pseudo-entropy and std results from a file if available; otherwise, execute quantum circuits\n",
    "    and calculate values, then save the results.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Pickle file to load/save results\n",
    "        betas (array): Array of angles for Ry and Rz rotations\n",
    "        deltas (array): Array of angles for additional Rx rotation (coherent error)\n",
    "        noise_model (NoiseModel): Quantum noise model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (pseudo_entropy, std_deviation) where each is a 2D array\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        display_and_log_print(f\"Loading results from {file_path}...\")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            result = pickle.load(f)\n",
    "    else:\n",
    "        display_and_log_print(f\"No file found at {file_path}. Calculating results...\")\n",
    "        result = run_circuits_and_calculate_entropy(betas, deltas, noise_model)\n",
    "        display_and_log_print(f\"Saving results to {file_path}...\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(result, f)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d624ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run or load simulations for both cases\n",
    "display_and_log_print(\"Running numerical simulations...\")\n",
    "numerical_results_path = \"data/numerical_results.pkl\"\n",
    "numerical_entropies, numerical_std_deviations, numerical_traces = load_or_run_entropy_calculation(\n",
    "    numerical_results_path, betas, deltas, None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290fd97",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "This section presents the core results of the pseudo-entropy simulations and their analysis. It includes displaying the quantum circuits, processing different simulation scenarios (e.g., coherent errors only), plotting pseudo-entropy components, and fitting the data to analytical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d9f76",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Display the quantum circuits used in the simulations\n",
    "display_and_log_print(\"\\n--- Step 1: Displaying the quantum circuits used in the simulations ---\")\n",
    "display_all_circuits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32808f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scenarios for analysis and plotting\n",
    "scenarios = [\n",
    "    (\"coherent_only\", numerical_entropies, \"numerical simulation with coherent noise only.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb99437",
   "metadata": {},
   "source": [
    "### Analyzing Coherent Routing Errors\n",
    "Now, we will analyze the impact of coherent routing errors on the quantum circuits. This involves quantifying the deviation of the actual circuit behavior from the ideal, numerical scenario. We'll use several metrics, including the pseudo-entropy, to characterize these errors and identify potential strategies for mitigating their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each scenario: display statistics and plot results\n",
    "for scenario_key, entropy_data, description in scenarios:\n",
    "    display_and_log_print(f\"\\n--- Step 2: Processing Scenario: {description} ---\")\n",
    "    \n",
    "    # Plot results\n",
    "    try:\n",
    "        display_and_log_print(f\"Plotting: {description}\")\n",
    "        plot_results(betas, deltas, entropy_data, scenario_key, \"|+1\\\\rangle\")\n",
    "        # Compute and display statistics for the data\n",
    "        stats = calculate_complex_statistics(entropy_data)\n",
    "        display_statistics_table(stats, scenario_key, entropy_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        display_and_log_print(f\"An error occurred while plotting {description}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794693b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_fit(fit_results, part):\n",
    "    \"\"\"\n",
    "    Determines the best fit (parabolic or linear) for a given part (real or imaginary) based on R-squared and p-value.\n",
    "\n",
    "    Args:\n",
    "        fit_results (dict): Dictionary containing fit results for different models.\n",
    "        part (str): Part of the entropy ('real' or 'imaginary').\n",
    "\n",
    "    Returns:\n",
    "        tuple: Best fit type ('parabolic' or 'linear') and reason for the choice.\n",
    "    \"\"\"\n",
    "    best_fit = None\n",
    "    best_r2 = -np.inf\n",
    "    best_p = 0\n",
    "    \n",
    "    for fit_type in ['parabolic', 'linear']:\n",
    "        if fit_type in fit_results[part]:\n",
    "            result = fit_results[part][fit_type]\n",
    "            r2 = result['r2']\n",
    "            p = result['p_value']\n",
    "            if r2 > best_r2 or (r2 == best_r2 and p < best_p):\n",
    "                best_fit = fit_type\n",
    "                best_r2 = r2\n",
    "                best_p = p\n",
    "    \n",
    "    if best_fit == 'parabolic':\n",
    "        reason = \"Parabolic fit is better (R² improvement: {}, p-value: {})\".format(best_r2 - fit_results[part]['linear']['r2'], best_p)\n",
    "    elif best_fit == 'linear':\n",
    "        reason = \"Linear fit preferred (similar performance, simpler model)\"\n",
    "    else:\n",
    "        reason = \"No successful fits available\"\n",
    "    \n",
    "    return best_fit, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41547de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_entropy_vs_beta_delta(beta, delta, entropy_data):\n",
    "    \"\"\"\n",
    "    Fits 2D parabolic and linear functions to the real and imaginary parts of the entropy data.\n",
    "    Handles NaN values by filtering them out before fitting.\n",
    "\n",
    "    Args:\n",
    "        beta (array-like): Array of beta values.\n",
    "        delta (array-like): Array of delta values.\n",
    "        entropy_data (complex ndarray): 2D array of complex entropy values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Fit results for both real and imaginary parts, including parameters, R-squared, p-value, and other metrics.\n",
    "\n",
    "    Fits the data to the following models:\n",
    "    - Parabolic: `S(beta, delta) = a * beta^2 + b * beta * delta + c * delta^2 + d * beta + e * delta + f`\n",
    "    - Linear: `S(beta, delta) = m1 * beta + m2 * delta + b`\n",
    "    \"\"\"\n",
    "    fit_results = {'real': {}, 'imaginary': {}}\n",
    "\n",
    "    for part in ['real', 'imaginary']:\n",
    "        data = entropy_data.real if part == 'real' else entropy_data.imag\n",
    "\n",
    "        # Create meshgrid for beta and delta values\n",
    "        beta_grid, delta_grid = np.meshgrid(beta, delta)\n",
    "        beta_flat = beta_grid.flatten()\n",
    "        delta_flat = delta_grid.flatten()\n",
    "        data_flat = data.flatten()\n",
    "\n",
    "        # Filter out NaN values\n",
    "        valid_mask = ~np.isnan(data_flat)\n",
    "        if np.sum(valid_mask) == 0:\n",
    "            display_and_log_print(f\"All data points are NaN for {part} part. Skipping fitting.\")\n",
    "            continue\n",
    "        elif np.sum(valid_mask) < 6:  # Need at least 6 points for parabolic fit (6 parameters)\n",
    "            display_and_log_print(f\"Insufficient valid data points ({np.sum(valid_mask)}) for {part} part. Skipping fitting.\")\n",
    "            continue\n",
    "\n",
    "        beta_valid = beta_flat[valid_mask]\n",
    "        delta_valid = delta_flat[valid_mask]\n",
    "        data_valid = data_flat[valid_mask]\n",
    "\n",
    "        # Store information about filtered data\n",
    "        n_total = len(data_flat)\n",
    "        n_valid = len(data_valid)\n",
    "        n_nan = n_total - n_valid\n",
    "        \n",
    "        display_and_log_print(f\"Fitting {part} part: {n_valid}/{n_total} valid points ({n_nan} NaN values filtered)\")\n",
    "\n",
    "        # Fit 2D parabolic function\n",
    "        try:\n",
    "            def parabolic(xy, a, b, c, d, e, f):\n",
    "                x, y = xy\n",
    "                return a * x**2 + b * x * y + c * y**2 + d * x + e * y + f\n",
    "\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                popt_para, pcov_para = curve_fit(parabolic, (beta_valid, delta_valid), data_valid)\n",
    "                para_warning = len(w) > 0\n",
    "\n",
    "                y_para = parabolic((beta_valid, delta_valid), *popt_para)\n",
    "                r2_para = r2_score(data_valid, y_para)\n",
    "\n",
    "                min_val = min(np.min(data_valid), np.min(y_para))\n",
    "                if min_val < 0:\n",
    "                    data_shifted = data_valid - min_val + 1\n",
    "                    y_para_shifted = y_para - min_val + 1\n",
    "                else:\n",
    "                    data_shifted = data_valid\n",
    "                    y_para_shifted = y_para\n",
    "\n",
    "                chi2_para, p_para = chisquare(data_shifted, y_para_shifted)\n",
    "\n",
    "                fit_results[part]['parabolic'] = {\n",
    "                    'params': popt_para,\n",
    "                    'cov': pcov_para,\n",
    "                    'r2': r2_para,\n",
    "                    'chi2': chi2_para,\n",
    "                    'p_value': p_para,\n",
    "                    'warning': para_warning,\n",
    "                    'predictions': y_para,\n",
    "                    'n_valid_points': n_valid,\n",
    "                    'n_nan_filtered': n_nan\n",
    "                }\n",
    "        except (RuntimeError, ValueError) as e:\n",
    "            display_and_log_print(f\"2D parabolic fitting failed for {part} part: {str(e)}\")\n",
    "\n",
    "        # Fit 2D linear function\n",
    "        try:\n",
    "            def linear(xy, m1, m2, b):\n",
    "                x, y = xy\n",
    "                return m1 * x + m2 * y + b\n",
    "\n",
    "            # Check if we have enough points for linear fit (3 parameters)\n",
    "            if n_valid < 3:\n",
    "                display_and_log_print(f\"Insufficient valid data points ({n_valid}) for linear fit of {part} part.\")\n",
    "                continue\n",
    "\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                popt_lin, pcov_lin = curve_fit(linear, (beta_valid, delta_valid), data_valid)\n",
    "                lin_warning = len(w) > 0\n",
    "\n",
    "                y_lin = linear((beta_valid, delta_valid), *popt_lin)\n",
    "                r2_lin = r2_score(data_valid, y_lin)\n",
    "\n",
    "                if min_val < 0:\n",
    "                    y_lin_shifted = y_lin - min_val + 1\n",
    "                else:\n",
    "                    y_lin_shifted = y_lin\n",
    "\n",
    "                chi2_lin, p_lin = chisquare(data_shifted, y_lin_shifted)\n",
    "\n",
    "                fit_results[part]['linear'] = {\n",
    "                    'params': popt_lin,\n",
    "                    'cov': pcov_lin,\n",
    "                    'r2': r2_lin,\n",
    "                    'chi2': chi2_lin,\n",
    "                    'p_value': p_lin,\n",
    "                    'warning': lin_warning,\n",
    "                    'predictions': y_lin,\n",
    "                    'n_valid_points': n_valid,\n",
    "                    'n_nan_filtered': n_nan\n",
    "                }\n",
    "        except (RuntimeError, ValueError) as e:\n",
    "            display_and_log_print(f\"2D linear fitting failed for {part} part: {str(e)}\")\n",
    "\n",
    "    if all(not fit_results[part] for part in ['real', 'imaginary']):\n",
    "        return None\n",
    "\n",
    "    # Determine best fit for each part (real and imaginary)\n",
    "    best_fit_real, reason_real = determine_best_fit(fit_results, 'real')\n",
    "    best_fit_imag, reason_imag = determine_best_fit(fit_results, 'imaginary')\n",
    "\n",
    "    # Add the best fit and reasons to the fit_results dictionary\n",
    "    fit_results['real']['best_fit'] = best_fit_real\n",
    "    fit_results['real']['reason'] = reason_real\n",
    "    fit_results['imaginary']['best_fit'] = best_fit_imag\n",
    "    fit_results['imaginary']['reason'] = reason_imag\n",
    "\n",
    "    return fit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameter_errors(fit_results):\n",
    "    \"\"\"\n",
    "    Calculates absolute and relative errors for the parameters of the fitted models.\n",
    "\n",
    "    Args:\n",
    "        fit_results (dict): Dictionary containing fit results for different models.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing absolute and relative errors for each parameter and fit type.\n",
    "    \"\"\"\n",
    "    errors = {}\n",
    "    for part in ['real', 'imaginary']:\n",
    "        errors[part] = {}\n",
    "        for fit_type in ['parabolic', 'linear']:\n",
    "            if fit_type in fit_results[part]:\n",
    "                result = fit_results[part][fit_type]\n",
    "                if result['warning']:\n",
    "                    params = result['params']\n",
    "                    errors[part][fit_type] = {\n",
    "                        'abs': np.full_like(params, np.nan),\n",
    "                        'rel': np.full_like(params, np.nan)\n",
    "                    }\n",
    "                else:\n",
    "                    params = result['params']\n",
    "                    abs_errors = np.sqrt(np.diag(result['cov']))\n",
    "                    rel_errors = np.abs(abs_errors / params * 100) if np.all(params != 0) else np.full_like(params, np.nan)\n",
    "                    errors[part][fit_type] = {\n",
    "                        'abs': abs_errors,\n",
    "                        'rel': rel_errors\n",
    "                    }\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markdown_parameter_table(result, error, param_names):\n",
    "    \"\"\"\n",
    "    Generate markdown parameter table for fit results.\n",
    "    \n",
    "    Args:\n",
    "        result (dict): Fit result dictionary\n",
    "        error (dict): Error dictionary\n",
    "        param_names (list): List of parameter names\n",
    "    \n",
    "    Returns:\n",
    "        str: Markdown formatted parameter table\n",
    "    \"\"\"\n",
    "    markdown_output = \"| Parameter | Value | Absolute Error | Relative Error (%) | Ratio to Last |\\n\"\n",
    "    markdown_output += \"|-----------|-------|----------------|--------------------|---------------|\\n\"\n",
    "    \n",
    "    # Last parameter for ratio calculations\n",
    "    last_param = result['params'][-1]\n",
    "    \n",
    "    for i, (param, name) in enumerate(zip(result['params'], param_names)):\n",
    "        abs_err = error['abs'][i]\n",
    "        rel_err = error['rel'][i]\n",
    "        \n",
    "        # Compute ratio to the last parameter\n",
    "        ratio_to_last = param / last_param if last_param != 0 else np.nan\n",
    "        \n",
    "        if np.isnan(abs_err):\n",
    "            markdown_output += f\"| ${name}$ | ${to_latex(round_number(param))}$ | undefined | uncertain | undefined |\\n\"\n",
    "        else:\n",
    "            markdown_output += (\n",
    "                f\"| ${name}$ | ${to_latex(round_number(param))}$ | \"\n",
    "                f\"${to_latex(round_number(abs_err))}$ | ${to_latex(round_number(param))}$% | \"\n",
    "                f\"${to_latex(round_number(ratio_to_last))}$ |\\n\"\n",
    "            )\n",
    "    \n",
    "    return markdown_output\n",
    "\n",
    "def generate_markdown_fit_metrics(result):\n",
    "    \"\"\"\n",
    "    Generate markdown goodness of fit metrics.\n",
    "    \n",
    "    Args:\n",
    "        result (dict): Fit result dictionary\n",
    "    \n",
    "    Returns:\n",
    "        str: Markdown formatted fit metrics\n",
    "    \"\"\"\n",
    "    markdown_output = \"\\n**Goodness of Fit Metrics:**\\n\\n\"\n",
    "    markdown_output += \"| Metric | Value |\\n\"\n",
    "    markdown_output += \"|-----------|-------|\\n\"\n",
    "    markdown_output += f\"| $R^2$ | ${to_latex(round_number(result['r2']))}$ |\\n\"\n",
    "    if 'chi2' in result:\n",
    "        markdown_output += f\"| $\\\\chi^2$ | ${to_latex(round_number(result['chi2']))}$ |\\n\"\n",
    "    \n",
    "    if 'p_value' in result:\n",
    "        markdown_output += f\"| p-value | ${to_latex(round_number(result['p_value']))}$ |\\n\"\n",
    "    \n",
    "    if result.get('warning', False):\n",
    "        markdown_output += \"\\n*Warning: Fit parameter uncertainties may be unreliable.*\\n\"\n",
    "    \n",
    "    return markdown_output\n",
    "\n",
    "def generate_latex_parameter_table(fit_results, errors, param_names, fit_type):\n",
    "    \"\"\"\n",
    "    Generate LaTeX parameter table for fit results with full horizontal lines.\n",
    "\n",
    "    Args:\n",
    "        fit_results (dict): Fit results dictionary.\n",
    "        errors (dict): Errors dictionary.\n",
    "        param_names (list): List of parameter names.\n",
    "        fit_type (str): The fit name/type to be used in caption and label.\n",
    "\n",
    "    Returns:\n",
    "        str: LaTeX formatted parameter table.\n",
    "    \"\"\"\n",
    "    latex_output = r\"\\begin{table}[H]\" + \"\\n\"\n",
    "    latex_output += r\"\\centering\" + \"\\n\"\n",
    "    \n",
    "    # Update caption to include fit_type\n",
    "    latex_output += fr\"\\caption{{Parameter Values and Errors - {fit_type.title()} Fit}}\\n\"\n",
    "    \n",
    "    # Add label using fit_type\n",
    "    latex_output += fr\"\\label{{tab:{fit_type.lower().replace(' ', '_')}}}\" + \"\\n\"\n",
    "\n",
    "    # Add table structure with full horizontal lines\n",
    "    latex_output += r\"\\begin{tabular}{|l|cc|cc|}\" + \"\\n\"\n",
    "    latex_output += r\"\\hline\" + \"\\n\"\n",
    "    latex_output += r\"Parameter & \\multicolumn{2}{c|}{Real Part} & \\multicolumn{2}{c|}{Imaginary Part} \\\\\" + \"\\n\"\n",
    "    latex_output += r\"\\hline\" + \"\\n\"\n",
    "    latex_output += r\" & Value & Abs. Error & Value & Abs. Error \\\\\" + \"\\n\"\n",
    "    latex_output += r\"\\hline\" + \"\\n\"\n",
    "    \n",
    "    for i, name in enumerate(param_names):\n",
    "        latex_output += (\n",
    "            f\"${name}$ & \"\n",
    "            f\"${to_latex(round_number(fit_results['real']['params'][i]))}$ & \"\n",
    "            f\"${to_latex(round_number(errors['real']['abs'][i]))}$ & \"\n",
    "            f\"${to_latex(round_number(fit_results['imaginary']['params'][i]))}$ & \"\n",
    "            f\"${to_latex(round_number(errors['imaginary']['abs'][i]))}$ \\\\\\\\\\n\"\n",
    "        )\n",
    "        latex_output += r\"\\hline\" + \"\\n\"\n",
    "\n",
    "    latex_output += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_output += r\"\\end{table}\" + \"\\n\"\n",
    "    \n",
    "    return latex_output\n",
    "\n",
    "def generate_latex_metrics_table(fit_results, fit_type):\n",
    "    \"\"\"\n",
    "    Generate LaTeX goodness of fit metrics table with full horizontal lines.\n",
    "    \n",
    "    Args:\n",
    "        fit_results (dict): Fit results dictionary.\n",
    "        fit_type (str): Type of fit (will be used in caption and label).\n",
    "    \n",
    "    Returns:\n",
    "        str: LaTeX formatted metrics table.\n",
    "    \"\"\"\n",
    "    # Remove '' or similar suffix if exists\n",
    "    clean_fit_type = fit_type.replace('_1d', '')\n",
    "\n",
    "    latex_output = r\"\\begin{table}[H]\" + \"\\n\"\n",
    "    latex_output += r\"\\centering\" + \"\\n\"\n",
    "    \n",
    "    # Use fit type in caption and label\n",
    "    latex_output += f\"\\\\caption{{Goodness of Fit Metrics for {clean_fit_type.title()} Fit}}\" + \"\\n\"\n",
    "    latex_output += f\"\\\\label{{tab:metrics_{clean_fit_type}}}\" + \"\\n\"\n",
    "    \n",
    "    # Define table structure with consistent lines\n",
    "    latex_output += r\"\\begin{tabular}{|l|c|c|}\" + \"\\n\"\n",
    "    latex_output += r\"\\hline\" + \"\\n\"\n",
    "    latex_output += r\"Metric & Real Part & Imaginary Part \\\\\" + \"\\n\"\n",
    "    latex_output += r\"\\hline\" + \"\\n\"\n",
    "    \n",
    "    # R² metric\n",
    "    latex_output += (\n",
    "        f\"$R^2$ & \"\n",
    "        f\"${to_latex(round_number(fit_results['real']['r2']))}$ & \"\n",
    "        f\"${to_latex(round_number(fit_results['imaginary']['r2']))}$ \\\\\\\\\\n\"\n",
    "    )\n",
    "    latex_output += r\"\\hline\" + \"\\n\"\n",
    "    \n",
    "    # χ² metric (if available)\n",
    "    if 'chi2' in fit_results['real']:\n",
    "        latex_output += (\n",
    "            f\"$\\\\chi^2$ & \"\n",
    "            f\"${to_latex(round_number(fit_results['real']['chi2']))}$ & \"\n",
    "            f\"${to_latex(round_number(fit_results['imaginary']['chi2']))}$ \\\\\\\\\\n\"\n",
    "        )\n",
    "        latex_output += r\"\\hline\" + \"\\n\"\n",
    "\n",
    "    # p-value metric (if available)\n",
    "    if 'p_value' in fit_results['real']:\n",
    "        latex_output += (\n",
    "            f\"p-value & \"\n",
    "            f\"${to_latex(round_number(fit_results['real']['p_value']))}$ & \"\n",
    "            f\"${to_latex(round_number(fit_results['imaginary']['p_value']))}$ \\\\\\\\\\n\"\n",
    "        )\n",
    "        latex_output += r\"\\hline\" + \"\\n\"\n",
    "    \n",
    "    # End table\n",
    "    latex_output += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_output += r\"\\end{table}\" + \"\\n\"\n",
    "    \n",
    "    return latex_output\n",
    "\n",
    "def create_excel_parameter_dataframe(fit_results, errors, param_names, fit_type):\n",
    "    \"\"\"\n",
    "    Create a pandas DataFrame for parameter values and errors.\n",
    "    \n",
    "    Args:\n",
    "        fit_results (dict): Fit results dictionary with 'real' and 'imaginary' keys\n",
    "        errors (dict): Errors dictionary with 'real' and 'imaginary' keys\n",
    "        param_names (list): List of parameter names\n",
    "        fit_type (str): Type of fit (parabolic, linear, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with parameter information\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for i, param_name in enumerate(param_names):\n",
    "        row = {\n",
    "            'Parameter': param_name,\n",
    "            'Real_Value': fit_results['real']['params'][i],\n",
    "            'Real_Abs_Error': errors['real']['abs'][i],\n",
    "            'Real_Rel_Error': errors['real']['rel'][i],\n",
    "            'Imaginary_Value': fit_results['imaginary']['params'][i],\n",
    "            'Imaginary_Abs_Error': errors['imaginary']['abs'][i],\n",
    "            'Imaginary_Rel_Error': errors['imaginary']['rel'][i],\n",
    "            'Fit_Type': fit_type\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_excel_metrics_dataframe(fit_results, fit_type):\n",
    "    \"\"\"\n",
    "    Create a pandas DataFrame for goodness of fit metrics.\n",
    "    \n",
    "    Args:\n",
    "        fit_results (dict): Fit results dictionary with 'real' and 'imaginary' keys\n",
    "        fit_type (str): Type of fit (parabolic, linear, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with metrics information\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # R² metric\n",
    "    data.append({\n",
    "        'Metric': 'R²',\n",
    "        'Real_Value': fit_results['real']['r2'],\n",
    "        'Imaginary_Value': fit_results['imaginary']['r2'],\n",
    "        'Fit_Type': fit_type\n",
    "    })\n",
    "    \n",
    "    # χ² metric (if available)\n",
    "    if 'chi2' in fit_results['real']:\n",
    "        data.append({\n",
    "            'Metric': 'χ²',\n",
    "            'Real_Value': fit_results['real']['chi2'],\n",
    "            'Imaginary_Value': fit_results['imaginary']['chi2'],\n",
    "            'Fit_Type': fit_type\n",
    "        })\n",
    "    \n",
    "    # p-value metric (if available)\n",
    "    if 'p_value' in fit_results['real']:\n",
    "        data.append({\n",
    "            'Metric': 'p-value',\n",
    "            'Real_Value': fit_results['real']['p_value'],\n",
    "            'Imaginary_Value': fit_results['imaginary']['p_value'],\n",
    "            'Fit_Type': fit_type\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_excel_summary_dataframe(fit_results, beta_range, n_betas_used, n_betas_total, n_deltas):\n",
    "    \"\"\"\n",
    "    Create a pandas DataFrame for fit summary information.\n",
    "    \n",
    "    Args:\n",
    "        fit_results (dict): Fit results dictionary\n",
    "        beta_range (tuple): Beta range used in fitting\n",
    "        n_betas_used (int): Number of beta points used\n",
    "        n_betas_total (int): Total number of beta points\n",
    "        n_deltas (int): Number of delta points\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with summary information\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    if beta_range is not None:\n",
    "        beta0_pi = beta_range[0] / np.pi\n",
    "        beta1_pi = beta_range[1] / np.pi\n",
    "        \n",
    "        data.append({\n",
    "            'Parameter': 'Beta Range Start',\n",
    "            'Value': beta0_pi,\n",
    "            'Unit': 'π',\n",
    "            'Description': f'Starting beta value'\n",
    "        })\n",
    "        \n",
    "        data.append({\n",
    "            'Parameter': 'Beta Range End',\n",
    "            'Value': beta1_pi,\n",
    "            'Unit': 'π',\n",
    "            'Description': f'Ending beta value'\n",
    "        })\n",
    "    \n",
    "    if n_betas_used is not None:\n",
    "        data.append({\n",
    "            'Parameter': 'Beta Points Used',\n",
    "            'Value': n_betas_used,\n",
    "            'Unit': 'count',\n",
    "            'Description': f'Number of beta points included in fit'\n",
    "        })\n",
    "    \n",
    "    if n_betas_total is not None:\n",
    "        data.append({\n",
    "            'Parameter': 'Beta Points Total',\n",
    "            'Value': n_betas_total,\n",
    "            'Unit': 'count',\n",
    "            'Description': f'Total number of beta points available'\n",
    "        })\n",
    "        \n",
    "        if n_betas_used is not None:\n",
    "            percent = 100 * n_betas_used / n_betas_total\n",
    "            data.append({\n",
    "                'Parameter': 'Beta Range Coverage',\n",
    "                'Value': percent,\n",
    "                'Unit': '%',\n",
    "                'Description': f'Percentage of beta range used'\n",
    "            })\n",
    "    \n",
    "    if n_deltas is not None:\n",
    "        data.append({\n",
    "            'Parameter': 'Delta Points',\n",
    "            'Value': n_deltas,\n",
    "            'Unit': 'count',\n",
    "            'Description': f'Number of delta points used'\n",
    "        })\n",
    "    \n",
    "    if n_betas_used is not None and n_deltas is not None:\n",
    "        n_points = n_betas_used * n_deltas\n",
    "        data.append({\n",
    "            'Parameter': 'Total Data Points',\n",
    "            'Value': n_points,\n",
    "            'Unit': 'count',\n",
    "            'Description': f'Total number of data points in fit'\n",
    "        })\n",
    "    \n",
    "    # Best fit information\n",
    "    for part in ['real', 'imaginary']:\n",
    "        if 'best_fit' in fit_results[part]:\n",
    "            data.append({\n",
    "                'Parameter': f'Best Fit ({part.title()})',\n",
    "                'Value': fit_results[part]['best_fit'],\n",
    "                'Unit': 'model',\n",
    "                'Description': f\"Best fit model for {part} part\"\n",
    "            })\n",
    "            \n",
    "            if 'reason' in fit_results[part]:\n",
    "                data.append({\n",
    "                    'Parameter': f'Best Fit Reason ({part.title()})',\n",
    "                    'Value': fit_results[part]['reason'],\n",
    "                    'Unit': 'text',\n",
    "                    'Description': f\"Reason for best fit selection ({part})\"\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def export_to_excel(fit_results, errors, beta_range, n_betas_used, n_betas_total, n_deltas, \n",
    "                   fit_vs=\"beta\"):\n",
    "    \"\"\"\n",
    "    Export fit results to Excel file with multiple sheets.\n",
    "    \n",
    "    Args:\n",
    "        fit_results (dict): Dictionary containing fit results for different models\n",
    "        errors (dict): Dictionary containing parameter errors\n",
    "        beta_range (tuple): Tuple (start_beta, end_beta) specifying the range of beta values\n",
    "        n_betas_used (int): Number of beta points included in the fit\n",
    "        n_betas_total (int): Total number of beta points available\n",
    "        n_deltas (int): Number of delta values used\n",
    "        fit_vs (str): Parameter being varied ('beta' or 'delta')\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the saved Excel file\n",
    "    \"\"\"\n",
    "    output_dir = \"results\"\n",
    "\n",
    "    # Define parameter names based on fit dimension\n",
    "    beta_symbol = \"β\"\n",
    "    delta_symbol = \"δ\"\n",
    "    \n",
    "    param_names = {\n",
    "        'parabolic': [f'{beta_symbol}²', f'{beta_symbol} {delta_symbol}', f'{delta_symbol}²', \n",
    "                     beta_symbol, delta_symbol, 'Constant'],\n",
    "        'linear': [f'm₁ ({beta_symbol})', f'm₂ ({delta_symbol})', 'Constant']\n",
    "    }\n",
    "    \n",
    "    # Generate timestamp for filename\n",
    "    filename = f\"fit_results.xlsx\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Create Excel writer\n",
    "    with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "        \n",
    "        # Summary sheet\n",
    "        summary_df = create_excel_summary_dataframe(fit_results, beta_range, n_betas_used, n_betas_total, n_deltas)\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        # Parameter sheets for each fit type\n",
    "        fit_types = ['parabolic', 'linear']\n",
    "        all_params_data = []\n",
    "        all_metrics_data = []\n",
    "        \n",
    "        for fit_type in fit_types:\n",
    "            if (fit_type in fit_results.get('real', {}) and \n",
    "                fit_type in fit_results.get('imaginary', {})):\n",
    "                \n",
    "                # Create parameter DataFrame\n",
    "                params_df = create_excel_parameter_dataframe(\n",
    "                    {'real': fit_results['real'][fit_type], \n",
    "                     'imaginary': fit_results['imaginary'][fit_type]},\n",
    "                    {'real': errors['real'][fit_type], \n",
    "                     'imaginary': errors['imaginary'][fit_type]},\n",
    "                    param_names[fit_type], fit_type\n",
    "                )\n",
    "                \n",
    "                # Create metrics DataFrame\n",
    "                metrics_df = create_excel_metrics_dataframe(\n",
    "                    {'real': fit_results['real'][fit_type], \n",
    "                     'imaginary': fit_results['imaginary'][fit_type]},\n",
    "                    fit_type\n",
    "                )\n",
    "                \n",
    "                # Save individual sheets\n",
    "                params_df.to_excel(writer, sheet_name=f'{fit_type.title()}_Parameters', index=False)\n",
    "                metrics_df.to_excel(writer, sheet_name=f'{fit_type.title()}_Metrics', index=False)\n",
    "                \n",
    "                # Collect for combined sheets\n",
    "                all_params_data.append(params_df)\n",
    "                all_metrics_data.append(metrics_df)\n",
    "        \n",
    "        # Combined sheets\n",
    "        if all_params_data:\n",
    "            combined_params_df = pd.concat(all_params_data, ignore_index=True)\n",
    "            combined_params_df.to_excel(writer, sheet_name='All_Parameters', index=False)\n",
    "        \n",
    "        if all_metrics_data:\n",
    "            combined_metrics_df = pd.concat(all_metrics_data, ignore_index=True)\n",
    "            combined_metrics_df.to_excel(writer, sheet_name='All_Metrics', index=False)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def print_fit_results(\n",
    "    fit_results, errors, beta_range, n_betas_used, n_betas_total, n_deltas, fit_vs=\"beta\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Prints a formatted summary of the fit results, including parameter values, uncertainties, \n",
    "    goodness-of-fit metrics, and the actual beta range and data subset used for fitting.\n",
    "    Also exports results to Excel file.\n",
    "\n",
    "    Args:\n",
    "        fit_results (dict): Dictionary containing fit results for different models.\n",
    "        errors (dict): Dictionary containing parameter errors.\n",
    "        beta_range (tuple): Tuple (start_beta, end_beta) specifying the range of beta values used for fitting.\n",
    "        n_betas_used (int): Number of beta points included in the fit.\n",
    "        n_betas_total (int): Total number of beta points available in the data.\n",
    "        n_deltas (int): Number of delta values used in the fit.\n",
    "        fit_vs (str, optional): Parameter being varied ('beta' or 'delta'). Default is 'beta'.\n",
    "\n",
    "    Prints:\n",
    "        - The actual beta range (in units of π) used for fitting.\n",
    "        - The number of beta and delta points included, and the percentage of the beta range used.\n",
    "        - Formatted tables of fit parameters, uncertainties, and goodness-of-fit metrics.\n",
    "    \"\"\"\n",
    "    if beta_range is not None and n_betas_used is not None and n_betas_total is not None:\n",
    "        beta0_pi = beta_range[0] / np.pi\n",
    "        beta1_pi = beta_range[1] / np.pi\n",
    "        n_points = n_betas_used * n_deltas\n",
    "        percent = 100 * n_betas_used / n_betas_total\n",
    "        display_and_log_markdown(f\"\"\"Fit performed over beta range: {beta0_pi:.3f}π to {beta1_pi:.3f}π \n",
    "                                 ({n_betas_used} beta points × {n_deltas} delta points = {n_points} total points, \n",
    "                                 {percent:.1f}% of beta range)\"\"\")\n",
    "    elif beta_range is not None:\n",
    "        beta0_pi = beta_range[0] / np.pi\n",
    "        beta1_pi = beta_range[1] / np.pi\n",
    "        display_and_log_markdown(f\"Fit performed over beta range: {beta0_pi:.3f}π to {beta1_pi:.3f}π\")\n",
    "\n",
    "    # Export to Excel\n",
    "    try:\n",
    "        excel_path = export_to_excel(fit_results, errors, beta_range, n_betas_used, n_betas_total, n_deltas, fit_vs)\n",
    "        display_and_log_print(f\"✓ Results exported to Excel: {excel_path}\")\n",
    "    except Exception as e:\n",
    "        display_and_log_print(f\"⚠ Warning: Could not export to Excel: {e}\")\n",
    "\n",
    "    # Set the parameter symbol dynamically based on fit_vs (e.g., \\beta or \\δ)\n",
    "    param_symbol = f\"\\\\{fit_vs}\"\n",
    "    \n",
    "    # Define parameter names based on fit dimension\n",
    "    beta_symbol = r\"\\beta\"\n",
    "    delta_symbol = r\"\\delta\"\n",
    "    \n",
    "    param_names = {\n",
    "        'parabolic': [f'{beta_symbol}^2', f'{beta_symbol} {delta_symbol}', f'{delta_symbol}^2', beta_symbol, delta_symbol, 'Constant'],\n",
    "        'linear': [f'm_1 ({beta_symbol})', f'm_2 ({delta_symbol})', 'Constant']\n",
    "    }\n",
    "    \n",
    "    markdown_output = \"\\n### Fit Analysis of Pseudo-Entropy $\\\\hat{S}$: Parabolic vs. Linear Models for $\\\\beta$ and $\\\\delta$ Parameters\\n\"\n",
    "    \n",
    "    # Explanation of fit equations\n",
    "    markdown_output += (\n",
    "        f\"\\nIn this analysis, the pseudo-entropy $\\\\hat{{S}}$ is fitted using two models:\\n\\n\"\n",
    "        f\"- **Parabolic Fit**: $\\\\hat{{S}}({beta_symbol}, {delta_symbol}) \\\\approx a {beta_symbol}^2 + b {beta_symbol} {delta_symbol} + c {delta_symbol}^2 + d {beta_symbol} + e {delta_symbol} + f$\\n\"\n",
    "        f\"- **Linear Fit**: $\\\\hat{{S}}({beta_symbol}, {delta_symbol}) \\\\approx m_1 {beta_symbol} + m_2 {delta_symbol} + b$\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # Prepare LaTeX output\n",
    "    latex_output = \"\"\n",
    "    \n",
    "    # Set fit types\n",
    "    fit_types = ['parabolic', 'linear']\n",
    "\n",
    "    # Iterate through real and imaginary parts for markdown\n",
    "    for part in ['real', 'imaginary']:\n",
    "        part_symbol = r\"\\Re \\hat{S}\" if part == 'real' else r\"\\Im \\hat{S}\"\n",
    "        best_fit = fit_results[part]['best_fit']\n",
    "        reason = fit_results[part]['reason']\n",
    "\n",
    "        markdown_output += f\"\\n#### ${part_symbol}$ Part:\\n\"\n",
    "        markdown_output += f\"**Best fit model**: *{best_fit.title()}* \\n\"\n",
    "        markdown_output += f\"**Reason**: {reason}\\n\\n\"\n",
    "            \n",
    "        \n",
    "        for fit_type in fit_types:\n",
    "            if fit_type in fit_results[part]:\n",
    "                result = fit_results[part][fit_type]\n",
    "                error = errors[part][fit_type]\n",
    "\n",
    "                markdown_output += f\"\\n**{fit_type.capitalize()} Fit:**\\n\\n\"\n",
    "\n",
    "                # Parameter Table\n",
    "                markdown_output += generate_markdown_parameter_table(result, error, param_names[fit_type])\n",
    "\n",
    "                # Goodness of Fit Metrics\n",
    "                markdown_output += generate_markdown_fit_metrics(result)\n",
    "\n",
    "    # Generate LaTeX Tables\n",
    "    for fit_type in fit_types:\n",
    "        latex_output += f\"\\\\section*{{{fit_type.capitalize()} Fit}}\\n\"\n",
    "        \n",
    "        # Mention best fit\n",
    "        if fit_type == fit_results['real']['best_fit']:\n",
    "            latex_output += (\n",
    "                \"\\\\noindent\\\\textbf{Best fit for} $\\\\Re \\\\hat{S}$: \"\n",
    "                f\"\\\\textit{{{fit_results['real']['best_fit'].capitalize()}}}\\\\\\\\\\n\"\n",
    "                f\"\\\\textbf{{Reason}}: {fit_results['real']['reason']}\\\\\\\\\\n\"\n",
    "            )\n",
    "\n",
    "        if fit_type == fit_results['imaginary']['best_fit']:\n",
    "            latex_output += (\n",
    "                \"\\\\noindent\\\\textbf{Best fit for} $\\\\Im \\\\hat{S}$: \"\n",
    "                f\"\\\\textit{{{fit_results['imaginary']['best_fit'].capitalize()}}}\\\\\\\\\\n\"\n",
    "                f\"\\\\textbf{{Reason}}: {fit_results['imaginary']['reason']}\\\\\\\\\\n\"\n",
    "            )\n",
    "        # Parameter Table\n",
    "        latex_output += generate_latex_parameter_table(\n",
    "            {'real': fit_results['real'][fit_type], 'imaginary': fit_results['imaginary'][fit_type]},\n",
    "            {'real': errors['real'][fit_type], 'imaginary': errors['imaginary'][fit_type]},\n",
    "            param_names[fit_type], fit_type\n",
    "        )\n",
    "\n",
    "        # Goodness of Fit Metrics Table\n",
    "        latex_output += generate_latex_metrics_table(\n",
    "            {'real': fit_results['real'][fit_type], 'imaginary': fit_results['imaginary'][fit_type]}, fit_type\n",
    "        )\n",
    "\n",
    "    # Display outputs\n",
    "    display_and_log_markdown(markdown_output)\n",
    "    display_and_log_print(latex_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d8315a",
   "metadata": {},
   "source": [
    "# Small Angle Approximation Derivation for Pseudo-Entropy\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This document provides a comprehensive derivation of the small angle approximation for pseudo-entropy in quantum circuits, focusing on the analytical treatment of coherent errors and weak interactions. The derivation demonstrates how the pseudo-entropy $\\check{S}(\\beta, \\delta)$ behaves under small perturbations in the interaction strength $\\beta$ and coherent error parameter $\\delta$.\n",
    "\n",
    "## Theoretical Framework\n",
    "\n",
    "### Initial Setup\n",
    "\n",
    "We consider a two-qubit system with initial state:\n",
    "$$\\ket{\\psi_i} = \\ket{+1} = \\frac{1}{\\sqrt{2}} \\left( \\ket{01} + \\ket{11} \\right)$$\n",
    "\n",
    "The system undergoes a controlled evolution with:\n",
    "- Interaction strength parameter $\\beta$ (assumed small)\n",
    "- Coherent error parameter $\\delta$ (assumed small)\n",
    "\n",
    "### Circuit Implementation\n",
    "\n",
    "The quantum circuit implements the following sequence:\n",
    "\n",
    "1. **CNOT gate application:**\n",
    "   $$\\text{CNOT} \\ket{\\psi_i} = \\frac{1}{\\sqrt{2}} \\left( \\ket{01} + \\ket{10} \\right)$$\n",
    "\n",
    "2. **Parameterized unitary evolution on qubit 1:**\n",
    "   $$U_{q_1}(\\beta, \\delta) = R_Z\\left( \\frac{\\pi}{2} \\right) R_Y(\\beta) R_Z(\\beta + \\delta) R_X(\\delta)$$\n",
    "\n",
    "3. **Final state:**\n",
    "   $$\\ket{\\psi_f} = \\left( I \\otimes U_{q_1}(\\beta, \\delta) \\right) \\text{CNOT} \\ket{\\psi_i}$$\n",
    "\n",
    "## Small Angle Approximation Derivation\n",
    "\n",
    "### Step 1: Rotation Matrix Expansions\n",
    "\n",
    "For small angles $\\theta$, the rotation matrices can be expanded using Taylor series:\n",
    "\n",
    "$$R_X(\\theta) = \\begin{pmatrix} \\cos(\\theta/2) & -i\\sin(\\theta/2) \\\\ -i\\sin(\\theta/2) & \\cos(\\theta/2) \\end{pmatrix} \\approx \\begin{pmatrix} 1 - \\frac{\\theta^2}{8} & -i\\frac{\\theta}{2} \\\\ -i\\frac{\\theta}{2} & 1 - \\frac{\\theta^2}{8} \\end{pmatrix}$$\n",
    "\n",
    "$$R_Y(\\theta) = \\begin{pmatrix} \\cos(\\theta/2) & -\\sin(\\theta/2) \\\\ \\sin(\\theta/2) & \\cos(\\theta/2) \\end{pmatrix} \\approx \\begin{pmatrix} 1 - \\frac{\\theta^2}{8} & -\\frac{\\theta}{2} \\\\ \\frac{\\theta}{2} & 1 - \\frac{\\theta^2}{8} \\end{pmatrix}$$\n",
    "\n",
    "$$R_Z(\\theta) = \\begin{pmatrix} e^{-i\\theta/2} & 0 \\\\ 0 & e^{i\\theta/2} \\end{pmatrix} \\approx \\begin{pmatrix} 1 - i\\frac{\\theta}{2} & 0 \\\\ 0 & 1 + i\\frac{\\theta}{2} \\end{pmatrix}$$\n",
    "\n",
    "### Step 2: Combined Unitary Expansion\n",
    "\n",
    "The combined unitary $U_{q_1}(\\beta, \\delta)$ can be expanded to second order in $\\beta$ and $\\delta$:\n",
    "\n",
    "$$U_{q_1}(\\beta, \\delta) \\approx U_0 + \\beta U_1 + \\delta U_2 + \\beta^2 U_{11} + \\beta\\delta U_{12} + \\delta^2 U_{22} + \\mathcal{O}(\\beta^3, \\delta^3)$$\n",
    "\n",
    "where:\n",
    "- $U_0 = R_Z(\\pi/2)$ (zeroth-order term)\n",
    "- $U_1, U_2$ are first-order coefficient matrices\n",
    "- $U_{11}, U_{12}, U_{22}$ are second-order coefficient matrices\n",
    "\n",
    "### Step 3: State Evolution Under Small Perturbations\n",
    "\n",
    "The final state can be written as:\n",
    "$$\\ket{\\psi_f} = \\ket{\\psi_0} + \\beta \\ket{\\psi_1} + \\delta \\ket{\\psi_2} + \\beta^2 \\ket{\\psi_{11}} + \\beta\\delta \\ket{\\psi_{12}} + \\delta^2 \\ket{\\psi_{22}} + \\mathcal{O}(\\beta^3, \\delta^3)$$\n",
    "\n",
    "where $\\ket{\\psi_0}$ is the unperturbed final state and $\\ket{\\psi_1}, \\ket{\\psi_2}, \\ldots$ are the perturbation terms.\n",
    "\n",
    "### Step 4: Overlap Calculation\n",
    "\n",
    "The overlap $\\langle\\psi_f|\\psi_i\\rangle$ expands as:\n",
    "$$\\langle\\psi_f|\\psi_i\\rangle = \\alpha_0 + \\beta \\alpha_1 + \\delta \\alpha_2 + \\beta^2 \\alpha_{11} + \\beta\\delta \\alpha_{12} + \\delta^2 \\alpha_{22} + \\mathcal{O}(\\beta^3, \\delta^3)$$\n",
    "\n",
    "### Step 5: Generalized Density Matrix\n",
    "\n",
    "The generalized transition matrix is:\n",
    "$$\\hat{\\rho} = \\frac{\\ket{\\psi_i}\\bra{\\psi_f}}{\\langle\\psi_f|\\psi_i\\rangle}$$\n",
    "\n",
    "Expanding the denominator using the geometric series:\n",
    "$$\\frac{1}{\\langle\\psi_f|\\psi_i\\rangle} = \\frac{1}{\\alpha_0} \\left(1 - \\frac{\\beta \\alpha_1 + \\delta \\alpha_2}{\\alpha_0} + \\frac{(\\beta \\alpha_1 + \\delta \\alpha_2)^2}{\\alpha_0^2} + \\ldots\\right)$$\n",
    "\n",
    "### Step 6: Reduced Density Matrix\n",
    "\n",
    "The reduced density matrix for qubit 0 is:\n",
    "$$\\hat{\\rho}_{q_0} = \\text{Tr}_{q_1}(\\hat{\\rho}) = \\rho_0 + \\beta \\rho_1 + \\delta \\rho_2 + \\beta^2 \\rho_{11} + \\beta\\delta \\rho_{12} + \\delta^2 \\rho_{22} + \\mathcal{O}(\\beta^3, \\delta^3)$$\n",
    "\n",
    "### Step 7: Eigenvalue Expansion\n",
    "\n",
    "The eigenvalues $\\lambda_k$ of $\\hat{\\rho}_{q_0}$ can be expanded as:\n",
    "$$\\lambda_k = \\lambda_k^{(0)} + \\beta \\lambda_k^{(1)} + \\delta \\lambda_k^{(2)} + \\beta^2 \\lambda_k^{(11)} + \\beta\\delta \\lambda_k^{(12)} + \\delta^2 \\lambda_k^{(22)} + \\mathcal{O}(\\beta^3, \\delta^3)$$\n",
    "\n",
    "### Step 8: Pseudo-Entropy Expansion\n",
    "\n",
    "The pseudo-entropy is:\n",
    "$$\\check{S}(\\beta, \\delta) = -\\sum_k \\lambda_k \\log_2 \\lambda_k$$\n",
    "\n",
    "Using the Taylor expansion of the logarithm:\n",
    "$$\\log_2 \\lambda_k = \\log_2 \\lambda_k^{(0)} + \\frac{1}{\\lambda_k^{(0)} \\ln 2}\\left(\\beta \\lambda_k^{(1)} + \\delta \\lambda_k^{(2)}\\right) + \\frac{1}{2\\lambda_k^{(0)} \\ln 2}\\left(\\beta^2 \\lambda_k^{(11)} + \\beta\\delta \\lambda_k^{(12)} + \\delta^2 \\lambda_k^{(22)}\\right) + \\mathcal{O}(\\beta^3, \\delta^3)$$\n",
    "\n",
    "## Final Result: Parabolic Approximation\n",
    "\n",
    "Combining all terms up to second order, the pseudo-entropy takes the form:\n",
    "\n",
    "$$\\check{S}(\\beta, \\delta) \\approx a \\beta^2 + b \\beta \\delta + c \\delta^2 + d \\beta + e \\delta + f$$\n",
    "\n",
    "where the coefficients are:\n",
    "- $a = -\\sum_k \\left[\\lambda_k^{(11)} \\log_2 \\lambda_k^{(0)} + \\frac{(\\lambda_k^{(1)})^2}{\\lambda_k^{(0)} \\ln 2}\\right]$\n",
    "- $b = -\\sum_k \\left[\\lambda_k^{(12)} \\log_2 \\lambda_k^{(0)} + \\frac{2\\lambda_k^{(1)}\\lambda_k^{(2)}}{\\lambda_k^{(0)} \\ln 2}\\right]$\n",
    "- $c = -\\sum_k \\left[\\lambda_k^{(22)} \\log_2 \\lambda_k^{(0)} + \\frac{(\\lambda_k^{(2)})^2}{\\lambda_k^{(0)} \\ln 2}\\right]$\n",
    "- $d = -\\sum_k \\left[\\lambda_k^{(1)} \\log_2 \\lambda_k^{(0)} + \\frac{\\lambda_k^{(1)}}{\\ln 2}\\right]$\n",
    "- $e = -\\sum_k \\left[\\lambda_k^{(2)} \\log_2 \\lambda_k^{(0)} + \\frac{\\lambda_k^{(2)}}{\\ln 2}\\right]$\n",
    "- $f = -\\sum_k \\lambda_k^{(0)} \\log_2 \\lambda_k^{(0)}$\n",
    "\n",
    "## Linear Approximation\n",
    "\n",
    "In the regime where quadratic terms are negligible, the pseudo-entropy reduces to:\n",
    "\n",
    "$$\\check{S}(\\beta, \\delta) \\approx m_1 \\beta + m_2 \\delta + b$$\n",
    "\n",
    "where:\n",
    "- $m_1 = d$ (coefficient of $\\beta$)\n",
    "- $m_2 = e$ (coefficient of $\\delta$)\n",
    "- $b = f$ (constant term)\n",
    "\n",
    "## Special Cases\n",
    "\n",
    "### Case 1: $\\beta = 0$ (No Interaction)\n",
    "\n",
    "When $\\beta = 0$, the pseudo-entropy becomes:\n",
    "$$\\check{S}(0, \\delta) = c \\delta^2 + e \\delta + f$$\n",
    "\n",
    "Analytical calculation shows that $\\text{Im}[\\check{S}(0, \\delta)] = 0$ for all $\\delta$, confirming that coherent errors alone do not introduce imaginary components in the absence of interactions.\n",
    "\n",
    "### Case 2: $\\delta = 0$ (No Coherent Error)\n",
    "\n",
    "When $\\delta = 0$, the pseudo-entropy becomes:\n",
    "$$\\check{S}(\\beta, 0) = a \\beta^2 + d \\beta + f$$\n",
    "\n",
    "This represents the pure interaction effect without coherent errors.\n",
    "\n",
    "### Case 3: Near-Orthogonal States ($\\beta \\approx \\pi/2$, $\\delta \\approx 0$)\n",
    "\n",
    "In this regime, $\\langle\\psi_f|\\psi_i\\rangle \\approx 0$, leading to:\n",
    "$$\\check{S}(\\pi/2, \\delta) \\sim \\frac{1}{\\delta} + \\mathcal{O}(\\delta^0)$$\n",
    "\n",
    "This singular behavior is handled numerically as NaN when $\\delta = 0$ exactly.\n",
    "\n",
    "## Validity and Limitations\n",
    "\n",
    "The small angle approximation is valid when:\n",
    "1. $|\\beta| \\ll 1$ and $|\\delta| \\ll 1$\n",
    "2. The system remains in the perturbative regime\n",
    "3. Higher-order terms $(>\\mathcal{O}(\\beta^2, \\delta^2))$ are negligible\n",
    "\n",
    "The approximation breaks down near:\n",
    "- Orthogonal state configurations\n",
    "- Large parameter values\n",
    "- Regions where the overlap $\\langle\\psi_f|\\psi_i\\rangle$ approaches zero\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The small angle approximation provides a powerful analytical framework for understanding pseudo-entropy behavior in quantum circuits with weak interactions and coherent errors. The derived parabolic form captures the essential physics while remaining computationally tractable for fitting and analysis purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fitted_entropy(fit_results, beta, delta):\n",
    "    \"\"\"\n",
    "    Computes the fitted entropy values using the best fit models for real and imaginary parts.\n",
    "    \n",
    "    Args:\n",
    "        fit_results (dict): Dictionary containing fit results from fit_entropy_vs_beta_delta.\n",
    "        beta (array-like): Array of beta values.\n",
    "        delta (array-like): Array of delta values.\n",
    "        \n",
    "    Returns:\n",
    "        complex ndarray: 2D array of fitted complex entropy values.\n",
    "    \"\"\"\n",
    "    # Create meshgrid for beta and delta values\n",
    "    beta_grid, delta_grid = np.meshgrid(beta, delta)\n",
    "    beta_flat = beta_grid.flatten()\n",
    "    delta_flat = delta_grid.flatten()\n",
    "    \n",
    "    fitted_values = np.zeros((len(delta), len(beta)), dtype=complex)\n",
    "    \n",
    "    for part in ['real', 'imaginary']:\n",
    "        # Get the best fit type for this part\n",
    "        best_fit = fit_results[part]['best_fit']\n",
    "        if best_fit is None:\n",
    "            continue\n",
    "            \n",
    "        # Get the parameters for the best fit\n",
    "        params = fit_results[part][best_fit]['params']\n",
    "        \n",
    "        # Compute fitted values based on the model type\n",
    "        if best_fit == 'parabolic':\n",
    "            # S(beta, delta) = a * beta^2 + b * beta * delta + c * delta^2 + d * beta + e * delta + f\n",
    "            a, b, c, d, e, f = params\n",
    "            values = (a * beta_grid**2 + \n",
    "                     b * beta_grid * delta_grid + \n",
    "                     c * delta_grid**2 + \n",
    "                     d * beta_grid + \n",
    "                     e * delta_grid + \n",
    "                     f)\n",
    "        else:  # linear\n",
    "            # S(beta, delta) = m1 * beta + m2 * delta + b\n",
    "            m1, m2, b = params\n",
    "            values = m1 * beta_grid + m2 * delta_grid + b\n",
    "        \n",
    "        # Add to the complex array\n",
    "        if part == 'real':\n",
    "            fitted_values += values\n",
    "        else:\n",
    "            fitted_values += 1j * values\n",
    "    \n",
    "    return fitted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b175c4",
   "metadata": {},
   "source": [
    "### Consistency Analysis and Model Performance Assessment\n",
    "\n",
    "This section performs a detailed consistency analysis and assesses the performance of the theoretical model against simulation results. This includes fitting the pseudo-entropy data to parabolic and linear models and evaluating the goodness-of-fit. The detailed sub-subsections covering fitted parameters, consistency verification, model performance comparison, parameter hierarchy, statistical validity assessment, and visual validation are offloaded to the supplementary materials for in-depth review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e53ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pseudo_entropy_data(betas, deltas, entropy_data):\n",
    "    \"\"\"\n",
    "    Analyzes 2D pseudo entropy data by fitting parabolic and linear models to the real and imaginary parts,\n",
    "    calculates statistical metrics for the residuals, and plots the residuals.\n",
    "    Uses only a chosen central window of beta values for analysis.\n",
    "\n",
    "    Args:\n",
    "        betas (array-like): Array of beta values.\n",
    "        deltas (array-like): Array of delta values.\n",
    "        entropy_data (complex ndarray): 2D array of complex pseudo entropy values.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    n_betas = len(betas)\n",
    "    n_deltas = len(deltas)\n",
    "    # Use a narrow (empirically chosen) central window for high-quality fits\n",
    "    start_idx = 7 * n_betas // 16\n",
    "    end_idx = 9 * n_betas // 16\n",
    "\n",
    "    betas_subset = betas[start_idx:end_idx]\n",
    "    entropy_data_subset = entropy_data[:, start_idx:end_idx]\n",
    "\n",
    "    print(f\"Using beta range: {betas_subset[0]:.6g} to {betas_subset[-1]:.6g}\")\n",
    "    print(f\"Total betas: {n_betas}, Using: {len(betas_subset)} ({len(betas_subset)/n_betas*100:.1f}%)\")\n",
    "\n",
    "    # Fit and analyze using the subset\n",
    "    fit_results = load_or_calculate(\n",
    "        \"data/fit_results.pkl\",\n",
    "        fit_entropy_vs_beta_delta,\n",
    "        betas_subset, deltas, entropy_data_subset\n",
    "    )\n",
    "\n",
    "    errors = load_or_calculate(\n",
    "        \"data/parameter_errors.pkl\",\n",
    "        calculate_parameter_errors,\n",
    "        fit_results\n",
    "    )\n",
    "\n",
    "    # Pass the actual beta range used to print_fit_results\n",
    "    print_fit_results(\n",
    "        fit_results, errors,\n",
    "        beta_range=(betas_subset[0], betas_subset[-1]),\n",
    "        n_betas_used=len(betas_subset), n_betas_total=n_betas,\n",
    "        n_deltas=n_deltas\n",
    "    )\n",
    "\n",
    "    # Compute fitted values using the best models\n",
    "    fitted_entropy = load_or_calculate(\n",
    "        \"data/fitted_entropy.pkl\",\n",
    "        compute_fitted_entropy,\n",
    "        fit_results, betas_subset, deltas\n",
    "    )\n",
    "\n",
    "    # Plot the residuals using the subset\n",
    "    plot_results(betas_subset, deltas, entropy_data_subset - fitted_entropy, 'theory_vs_simulation', \"|+1\\\\rangle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzes 2D pseudo entropy data\n",
    "analyze_pseudo_entropy_data(betas, deltas, numerical_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7366ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_calculate(file_path, calculate_function, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Load a value from a file if it exists; otherwise, calculate it, save it, and return the value.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file to load/save the value.\n",
    "        calculate_function (callable): Function to calculate the value if not loaded.\n",
    "        *args: Positional arguments for the calculate_function.\n",
    "        **kwargs: Keyword arguments for the calculate_function.\n",
    "\n",
    "    Returns:\n",
    "        Any: The loaded or calculated value.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    if os.path.exists(file_path):\n",
    "        display_and_log_print(f\"Loading from {file_path}...\")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            value = pickle.load(f)\n",
    "    else:\n",
    "        display_and_log_print(f\"Calculating and saving to {file_path}...\")\n",
    "        value = calculate_function(*args, **kwargs)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(value, f)\n",
    "    return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d21c01",
   "metadata": {},
   "source": [
    "### Focusing on Key Regions of Continuous Classical-like Behaviour\n",
    "\n",
    "This section delves into the analysis of continuous \"classical-like\" segments within the pseudo-entropy parameter space. It identifies and visualizes these segments, which are crucial for understanding the robustness and sensitivity of the pseudo-entropy method to coherent errors. The analysis includes calculating Clopper-Pearson confidence intervals for the detection rates within these segments, providing a statistically rigorous assessment of the method's performance.\n",
    "\n",
    "While this notebook provides summary metrics and visualizations, the *detailed tables and statistics per beta/delta* are saved in results directory and can be explored further. The analysis is designed to be flexible, allowing for both direct threshold values and exponent-based thresholds, enabling a comprehensive exploration of the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_segments(\n",
    "    classical_mask,\n",
    "    axis_values1,\n",
    "    axis_values2,\n",
    "    white_percentage_threshold=0.3,\n",
    "    is_vertical=True,\n",
    "    confidence_level=0.95\n",
    "):\n",
    "    \"\"\"\n",
    "    Identifies continuous segments in either vertical or horizontal direction.\n",
    "    Calculates confidence intervals for the proportion of True values within each segment,\n",
    "    ignoring np.nan values and reporting the percentage of NaNs per segment in the output.\n",
    "\n",
    "    Parameters:\n",
    "        classical_mask (ndarray): 2D boolean array (with possible np.nan for undefined values)\n",
    "                                  axis_values1 corresponds to rows, axis_values2 to columns.\n",
    "        axis_values1 (array): Values for the first axis (typically betas) - corresponds to rows.\n",
    "        axis_values2 (array): Values for the second axis (typically deltas) - corresponds to columns.\n",
    "        white_percentage_threshold (float): Minimum fraction of True values (excluding NaNs) to qualify a segment.\n",
    "        is_vertical (bool): \n",
    "            If True, finds vertical segments (constant axis_values2, i.e., constant beta, varying delta).\n",
    "            If False, finds horizontal segments (constant axis_values1, i.e., constant delta, varying beta).\n",
    "        confidence_level (float): Confidence level for binomial confidence intervals.\n",
    "\n",
    "    Returns:\n",
    "        segment_info (list of dict): Each dictionary contains information about a continuous qualified segment:\n",
    "            - 'start_pi': Start value of the segment in units of π.\n",
    "            - 'end_pi': End value of the segment in units of π.\n",
    "            - 'average_thickness_pi': Average thickness of the segment in units of π.\n",
    "            - 'average_true_percentage': Average percentage of True (classical-like) values in the segment (excluding NaNs).\n",
    "            - 'average_true_percentage_ci_lower': Lower confidence interval bound (in percent).\n",
    "            - 'average_true_percentage_ci_upper': Upper confidence interval bound (in percent).\n",
    "            - 'length_cells': Number of axis points in the segment.\n",
    "            - 'nan_percentage': Percentage of NaN (undefined) entries within the segment.\n",
    "    \"\"\"\n",
    "    if is_vertical:\n",
    "        # Vertical segments: analyze columns (constant beta, varying delta)\n",
    "        mask_to_analyze = classical_mask  # rows=delta, cols=beta\n",
    "        num_segments = classical_mask.shape[1]  # number of beta values\n",
    "        segment_axis_values = axis_values1  # beta values\n",
    "    else:\n",
    "        # Horizontal segments: analyze rows (constant delta, varying beta)\n",
    "        mask_to_analyze = classical_mask.T  # transpose so rows=beta, cols=delta\n",
    "        num_segments = classical_mask.shape[0]  # number of delta values\n",
    "        segment_axis_values = axis_values2  # delta values\n",
    "\n",
    "    qualified_indices = []\n",
    "\n",
    "    for j in range(num_segments):\n",
    "        line = mask_to_analyze[:, j]\n",
    "        valid = ~np.isnan(line)\n",
    "        true_count = np.sum(line[valid])\n",
    "        segment_length = np.sum(valid)\n",
    "        nan_count = np.sum(np.isnan(line))\n",
    "        nan_percentage = (nan_count / len(line) * 100) if len(line) > 0 else 0\n",
    "        white_percentage = (true_count / segment_length) if segment_length > 0 else 0\n",
    "        if white_percentage >= white_percentage_threshold:\n",
    "            qualified_indices.append(j)\n",
    "\n",
    "    segment_info = []\n",
    "    if qualified_indices:\n",
    "        start_index = qualified_indices[0]\n",
    "        for i in range(1, len(qualified_indices)):\n",
    "            if qualified_indices[i] > qualified_indices[i - 1] + 1:\n",
    "                # End of a continuous segment\n",
    "                end_index = qualified_indices[i - 1]\n",
    "                segment_indices = range(start_index, end_index + 1)\n",
    "                segment_submask = mask_to_analyze[:, segment_indices]\n",
    "                valid = ~np.isnan(segment_submask)\n",
    "                total_cells_in_segment = np.sum(valid)\n",
    "                true_count_in_segment = np.sum(segment_submask[valid])\n",
    "                nan_count_segment = np.sum(np.isnan(segment_submask))\n",
    "                nan_percentage_segment = (nan_count_segment / segment_submask.size * 100) if segment_submask.size > 0 else 0\n",
    "                avg_percentage = (true_count_in_segment / total_cells_in_segment) * 100 if total_cells_in_segment > 0 else 0\n",
    "                ci_lower, ci_upper = calculate_binomial_ci(true_count_in_segment, total_cells_in_segment, confidence_level)\n",
    "                if len(segment_axis_values) > 1:\n",
    "                    average_thickness_pi = np.abs(segment_axis_values[1] - segment_axis_values[0]) / np.pi\n",
    "                else:\n",
    "                    average_thickness_pi = 0\n",
    "                start_value = segment_axis_values[start_index] / np.pi\n",
    "                end_value = segment_axis_values[end_index] / np.pi\n",
    "                segment_info.append({\n",
    "                    'start_pi': start_value,\n",
    "                    'end_pi': end_value,\n",
    "                    'average_thickness_pi': average_thickness_pi,\n",
    "                    'average_true_percentage': avg_percentage,\n",
    "                    'average_true_percentage_ci_lower': ci_lower * 100,\n",
    "                    'average_true_percentage_ci_upper': ci_upper * 100,\n",
    "                    'length_cells': len(segment_indices),\n",
    "                    'nan_percentage': nan_percentage_segment\n",
    "                })\n",
    "                start_index = qualified_indices[i]\n",
    "\n",
    "        # Handle the last segment\n",
    "        end_index = qualified_indices[-1]\n",
    "        segment_indices = range(start_index, end_index + 1)\n",
    "        segment_submask = mask_to_analyze[:, segment_indices]\n",
    "        valid = ~np.isnan(segment_submask)\n",
    "        total_cells_in_segment = np.sum(valid)\n",
    "        true_count_in_segment = np.sum(segment_submask[valid])\n",
    "        nan_count_segment = np.sum(np.isnan(segment_submask))\n",
    "        nan_percentage_segment = (nan_count_segment / segment_submask.size * 100) if segment_submask.size > 0 else 0\n",
    "        avg_percentage = (true_count_in_segment / total_cells_in_segment) * 100 if total_cells_in_segment > 0 else 0\n",
    "        ci_lower, ci_upper = calculate_binomial_ci(true_count_in_segment, total_cells_in_segment, confidence_level)\n",
    "        if len(segment_axis_values) > 1:\n",
    "            average_thickness_pi = np.abs(segment_axis_values[1] - segment_axis_values[0]) / np.pi\n",
    "        else:\n",
    "            average_thickness_pi = 0\n",
    "        start_value = segment_axis_values[start_index] / np.pi\n",
    "        end_value = segment_axis_values[end_index] / np.pi\n",
    "        segment_info.append({\n",
    "            'start_pi': start_value,\n",
    "            'end_pi': end_value,\n",
    "            'average_thickness_pi': average_thickness_pi,\n",
    "            'average_true_percentage': avg_percentage,\n",
    "            'average_true_percentage_ci_lower': ci_lower * 100,\n",
    "            'average_true_percentage_ci_upper': ci_upper * 100,\n",
    "            'length_cells': len(segment_indices),\n",
    "            'nan_percentage': nan_percentage_segment\n",
    "        })\n",
    "\n",
    "    return segment_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3344cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_binomial_ci(k, n, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculates the Clopper-Pearson confidence interval for a binomial proportion.\n",
    "    This method is robust for proportions near 0 or 1, and for small N.\n",
    "\n",
    "    Args:\n",
    "        k (int): Number of successes (True values).\n",
    "        n (int): Total number of trials (total cells).\n",
    "        confidence_level (float): Desired confidence level (e.g., 0.95 for 95%).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (lower_bound, upper_bound) of the confidence interval (as proportions).\n",
    "               Returns (np.nan, np.nan) if n is 0 or k is invalid.\n",
    "    \"\"\"\n",
    "    if n == 0 or not (0 <= k <= n):\n",
    "        return np.nan, np.nan\n",
    "    alpha = 1 - confidence_level\n",
    "    \n",
    "    # Clopper-Pearson interval using scipy.stats.beta.ppf\n",
    "    # This approach is generally recommended for its good coverage properties.\n",
    "    lower = beta.ppf(alpha / 2, k, n - k + 1)\n",
    "    upper = beta.ppf(1 - alpha / 2, k + 1, n - k)\n",
    "\n",
    "    # Ensure bounds are within [0, 1] due to potential numerical precision at extremes\n",
    "    return max(0.0, lower), min(1.0, upper)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd6d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_segments_for_zoom(segments, original_coords, zoomed_coords, zoomed_mask, zoomed_betas, zoomed_deltas, is_vertical):\n",
    "    \"\"\"\n",
    "    Adjusts segment coordinates for zoomed view and recalculates percentages.\n",
    "\n",
    "    Args:\n",
    "        segments (list): List of segment dictionaries containing 'start_pi' and 'end_pi' coordinates.\n",
    "        original_coords (ndarray): Original coordinate array (betas or deltas).\n",
    "        zoomed_coords (ndarray): Zoomed coordinate array (subset of original).\n",
    "        zoomed_mask (ndarray): The zoomed classical mask.\n",
    "        zoomed_betas (ndarray): Zoomed beta coordinates.\n",
    "        zoomed_deltas (ndarray): Zoomed delta coordinates.\n",
    "        is_vertical (bool): True if adjusting vertical segments, False for horizontal.\n",
    "\n",
    "    Returns:\n",
    "        list: Adjusted segments with updated coordinates and recalculated percentages.\n",
    "    \"\"\"\n",
    "    # Check if arrays are empty\n",
    "    if len(original_coords) == 0 or len(zoomed_coords) == 0:\n",
    "        return []\n",
    "    o_start, o_end = original_coords[0], original_coords[-1]\n",
    "    z_start, z_end = zoomed_coords[0], zoomed_coords[-1]\n",
    "    scale = (z_end - z_start) / (o_end - o_start) if (o_end - o_start) != 0 else 1\n",
    "    offset = z_start - o_start * scale\n",
    "    adjusted_segments = []\n",
    "    for seg_info in segments:\n",
    "        new_start_pi = seg_info['start_pi'] * scale + offset\n",
    "        new_end_pi = seg_info['end_pi'] * scale + offset\n",
    "        # Recalculate average percentage based on the zoomed mask\n",
    "        if is_vertical:\n",
    "            # For vertical segments\n",
    "            start_idx = np.argmin(np.abs(zoomed_betas - new_start_pi * np.pi))\n",
    "            end_idx = np.argmin(np.abs(zoomed_betas - new_end_pi * np.pi))\n",
    "            if start_idx > end_idx:\n",
    "                start_idx, end_idx = end_idx, start_idx\n",
    "            segment_mask = zoomed_mask[:, start_idx:end_idx+1]\n",
    "        else:\n",
    "            # For horizontal segments\n",
    "            start_idx = np.argmin(np.abs(zoomed_deltas - new_start_pi * np.pi))\n",
    "            end_idx = np.argmin(np.abs(zoomed_deltas - new_end_pi * np.pi))\n",
    "            if start_idx > end_idx:\n",
    "                start_idx, end_idx = end_idx, start_idx\n",
    "            segment_mask = zoomed_mask[start_idx:end_idx+1, :]\n",
    "        true_count = np.sum(segment_mask)\n",
    "        total_count = segment_mask.size\n",
    "        avg_percentage = (true_count / total_count) * 100 if total_count > 0 else 0\n",
    "        adjusted_segments.append({**seg_info, 'start_pi': new_start_pi, 'end_pi': new_end_pi, 'average_true_percentage': avg_percentage})\n",
    "    return adjusted_segments\n",
    "\n",
    "\n",
    "def _create_continuous_segments_plot(classical_mask, betas, deltas, vertical_segments, horizontal_segments, imag_threshold, is_imag_threshold_exponent, zoom_factor=None, filename_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Helper function to create and save a visualization of continuous classical-like segments.\n",
    "\n",
    "    Args:\n",
    "        classical_mask (ndarray): Boolean mask from pseudo-entropy thresholding.\n",
    "        betas (array): Values for β angles.\n",
    "        deltas (array): Values for δ angles.\n",
    "        vertical_segments (list): Vertical continuous segment info.\n",
    "        horizontal_segments (list): Horizontal continuous segment info.\n",
    "        imag_threshold (int or float): The imaginary threshold. If is_imag_threshold_exponent is True,\n",
    "                                      it's an exponent (int), otherwise, it's the direct threshold value (float).\n",
    "        is_imag_threshold_exponent (bool): If True, treat imag_threshold as an exponent (10^imag_threshold),\n",
    "                                           if False, treat it as the direct threshold value.\n",
    "        zoom_factor (float, optional): Factor to zoom in on central region of the plot. If None, no zoom is applied.\n",
    "        filename_suffix (str, optional): Additional text to append to the filename. Defaults to \"\".\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    if is_imag_threshold_exponent:\n",
    "        threshold_label = f\"{np.abs(imag_threshold):.0f}\"\n",
    "        title_threshold_str = f\"10^{{{imag_threshold}}}\"\n",
    "        filename_threshold_str = f\"{np.abs(imag_threshold)}\"\n",
    "    else:\n",
    "        threshold_label = f\"{imag_threshold*100:.2f}\\\\%\"\n",
    "        title_threshold_str = f\"{threshold_label}\"\n",
    "        filename_threshold_str = threshold_label.replace(\".\", \"_\").replace(\"-\", \"\").replace(\"\\\\%\", \"\")\n",
    "\n",
    "\n",
    "    if zoom_factor is not None and zoom_factor > 1:\n",
    "        center_delta_index = len(deltas) // 2\n",
    "        center_beta_index = len(betas) // 2\n",
    "\n",
    "        delta_half_range = int((len(deltas) // (2 * zoom_factor)))\n",
    "        beta_half_range = int((len(betas) // (2 * zoom_factor)))\n",
    "\n",
    "        delta_start = max(0, center_delta_index - delta_half_range)\n",
    "        delta_end = min(len(deltas), center_delta_index + delta_half_range)\n",
    "        beta_start = max(0, center_beta_index - beta_half_range)\n",
    "        beta_end = min(len(betas), center_beta_index + beta_half_range) # Corrected line\n",
    "\n",
    "        zoomed_classical_mask = classical_mask[delta_start:delta_end,\n",
    "                                              beta_start:beta_end]\n",
    "        zoomed_betas = betas[beta_start:beta_end]\n",
    "        zoomed_deltas = deltas[delta_start:delta_end]\n",
    "        extent = [zoomed_betas[0] / np.pi, zoomed_betas[-1] / np.pi,\n",
    "                  zoomed_deltas[0] / np.pi, zoomed_deltas[-1] / np.pi]\n",
    "        im = ax.imshow(zoomed_classical_mask, cmap='gray', extent=extent,\n",
    "                      origin='lower', aspect='auto')\n",
    "\n",
    "        zoomed_vertical_segments = adjust_segments_for_zoom(\n",
    "            vertical_segments, deltas, zoomed_deltas,\n",
    "            zoomed_classical_mask, zoomed_betas, zoomed_deltas, True\n",
    "        )\n",
    "\n",
    "        zoomed_horizontal_segments = adjust_segments_for_zoom(\n",
    "            horizontal_segments, betas, zoomed_betas,\n",
    "            zoomed_classical_mask, zoomed_betas, zoomed_deltas, False\n",
    "        )\n",
    "\n",
    "        all_zoomed_segments = zoomed_vertical_segments + zoomed_horizontal_segments\n",
    "        percentages = [info['average_true_percentage']\n",
    "                       for info in all_zoomed_segments]\n",
    "        norm = plt.Normalize(min(percentages) if percentages else 0,\n",
    "                             max(percentages) if percentages else 1)\n",
    "        v_cmap = plt.cm.Blues\n",
    "        h_cmap = plt.cm.Greens\n",
    "\n",
    "        for info in zoomed_vertical_segments:\n",
    "            color = v_cmap(norm(info['average_true_percentage']))\n",
    "            ax.axvspan(info['start_pi'], info['end_pi'], color=color, alpha=0.5)\n",
    "\n",
    "        for info in zoomed_horizontal_segments:\n",
    "            color = h_cmap(norm(info['average_true_percentage']))\n",
    "            ax.axhspan(info['start_pi'], info['end_pi'], color=color, alpha=0.5)\n",
    "\n",
    "    else:\n",
    "        extent = [betas[0] / np.pi, betas[-1] / np.pi, deltas[0] / np.pi,\n",
    "                  deltas[-1] / np.pi]\n",
    "        im = ax.imshow(classical_mask, cmap='gray', extent=extent,\n",
    "                      origin='lower', aspect='auto')\n",
    "        all_segments = vertical_segments + horizontal_segments\n",
    "        percentages = [info['average_true_percentage']\n",
    "                       for info in all_segments]\n",
    "        norm = plt.Normalize(min(percentages) if percentages else 0,\n",
    "                             max(percentages) if percentages else 1)\n",
    "        v_cmap = plt.cm.Blues\n",
    "        h_cmap = plt.cm.Greens\n",
    "\n",
    "        for info in vertical_segments:\n",
    "            color = v_cmap(norm(info['average_true_percentage']))\n",
    "            ax.axvspan(info['start_pi'], info['end_pi'], color=color, alpha=0.5)\n",
    "\n",
    "        for info in horizontal_segments:\n",
    "            color = h_cmap(norm(info['average_true_percentage']))\n",
    "            ax.axhspan(info['start_pi'], info['end_pi'], color=color, alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel(r'Controlled Interaction Strength $\\beta / \\pi$')\n",
    "    ax.set_ylabel(r'Coherent Error Angle $\\delta / \\pi$')\n",
    "    title_suffix = f\" (Zoomed-in Center {zoom_factor}x)\" if zoom_factor is not None else \"\"\n",
    "    ax.set_title(\n",
    "        f'Continuous Classical-like Regions ($|\\\\Im(\\\\check{{S}})| < {title_threshold_str}$){title_suffix}',\n",
    "        fontsize=16\n",
    "    )\n",
    "\n",
    "    v_sm = plt.cm.ScalarMappable(cmap=v_cmap, norm=norm)\n",
    "    h_sm = plt.cm.ScalarMappable(cmap=h_cmap, norm=norm)\n",
    "\n",
    "    plt.colorbar(v_sm, ax=ax, location='left', pad=0.1,\n",
    "                         label='Vertical Segments Average True Percentage (\\\\%)')\n",
    "    plt.colorbar(h_sm, ax=ax, location='right', pad=0.1,\n",
    "                         label='Horizontal Segments Average True Percentage (\\\\%)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'results/continuous_classical_regions_thresh_{filename_threshold_str}{filename_suffix}.png'\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    latex_import_figure = (\n",
    "        \"\\\\begin{figure}[ht]\\n\"\n",
    "        \"    \\\\centering\\n\"\n",
    "        f\"    \\\\includegraphics[width=0.8\\\\textwidth]{{{filename}}}\\n\"\n",
    "        f\"    \\\\caption{{Continuous Classical-like Regions (Colored by Direction and Average True Percentage) \"\n",
    "    )\n",
    "    latex_import_figure += f\"with $|\\\\Im(\\\\check{{S}})| < {title_threshold_str}${title_suffix}}}\\n\"\n",
    "\n",
    "    latex_import_figure += f\"    \\\\label{{fig:continuous_classical_regions_{filename_threshold_str}{filename_suffix.replace('-', '_')}}}\\n\"\n",
    "    latex_import_figure += \"    \\\\end{figure}\"\n",
    "\n",
    "    display_and_log_print(latex_import_figure)\n",
    "\n",
    "def visualize_segments(classical_mask, betas, deltas, vertical_segments,\n",
    "                       horizontal_segments, imag_threshold, is_imag_threshold_exponent, zoom_factor=None):\n",
    "    \"\"\"\n",
    "    Visualizes the classical mask and highlights continuous segments with colors based on their average true percentage.\n",
    "    Optionally, creates additional zoomed-in views of the central region of the plot.\n",
    "\n",
    "    Args:\n",
    "        classical_mask (ndarray): Boolean mask from pseudo-entropy thresholding (True for classical-like).\n",
    "        betas (array): Values for β angles (x-axis).\n",
    "        deltas (array): Values for δ angles (y-axis).\n",
    "        vertical_segments (list): List of dictionaries, each containing information about a continuous vertical segment,\n",
    "                                  including 'start_pi', 'end_pi', and 'average_true_percentage'.\n",
    "        horizontal_segments (list): List of dictionaries, each containing information about a continuous horizontal segment,\n",
    "                                    including 'start_pi', 'end_pi', and 'average_true_percentage'.\n",
    "        imag_threshold (int or float): The imaginary threshold. If is_imag_threshold_exponent is True,\n",
    "                                      it's an exponent (int), otherwise, it's the direct threshold value (float).\n",
    "        is_imag_threshold_exponent (bool): If True, treat imag_threshold as an exponent (10^imag_threshold),\n",
    "                                           if False, treat it as the direct threshold value.\n",
    "        zoom_factor (float, optional): Factor to zoom in on central region of the plot. If None, no zoom is applied.\n",
    "\n",
    "    Description:\n",
    "        This function displays the classical_mask as a grayscale image and overlays colored spans\n",
    "        to highlight continuous segments of classical-like behavior. Vertical segments are colored\n",
    "        using a blue colormap, and horizontal segments are colored using a green colormap. The\n",
    "        intensity of the color indicates the average_true_percentage of the classical-like\n",
    "        region within that segment.\n",
    "\n",
    "        If zoom_factor is provided, an additional plot focusing on the central portion of the\n",
    "        parameter space is generated, allowing for a closer inspection of specific regions and their\n",
    "        identified continuous segments. The zoomed plot is saved with a \"_zoom_[zoom_factor]\" suffix\n",
    "        in the filename.\n",
    "    \"\"\"\n",
    "    _create_continuous_segments_plot(classical_mask, betas, deltas,\n",
    "                                     vertical_segments, horizontal_segments,\n",
    "                                     imag_threshold, is_imag_threshold_exponent)\n",
    "    if zoom_factor is not None and zoom_factor > 1:  # Changed condition to zoom_factor > 1\n",
    "        _create_continuous_segments_plot(classical_mask, betas, deltas,\n",
    "                                         vertical_segments, horizontal_segments,\n",
    "                                         imag_threshold, is_imag_threshold_exponent, zoom_factor,\n",
    "                                         f\"_zoom_{zoom_factor}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segment_table(segments, direction, title_threshold_str):\n",
    "    \"\"\"\n",
    "    Create a markdown table for either vertical or horizontal segments, including confidence intervals and NaN percentage.\n",
    "    \n",
    "    Args:\n",
    "        segments (list): List of segment dictionaries containing segment info.\n",
    "        direction (str): Direction type (\"vertical\" or \"horizontal\").\n",
    "        title_threshold_str (str): Formatted threshold string for the title.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted markdown table as a string.\n",
    "    \"\"\"\n",
    "    if not segments:\n",
    "        return f\"No continuous {direction} white lines found for $|\\\\Im(\\\\check{{S}})| < {title_threshold_str}$.\\n\\n\"\n",
    "\n",
    "    table = (\n",
    "        f\"**{direction.capitalize()} Segments (Threshold $|\\\\Im(\\\\check{{S}})| < \"\n",
    "        f\"{title_threshold_str}$):**\\n\\n\"\n",
    "    )\n",
    "\n",
    "    table += (\n",
    "        r\"| Value/π Start | Value/π End | Avg. Thickness (Value/π) | Length (Cells) | Avg. True (%) | 95\\% CI Lower (%) | 95\\% CI Upper (%) | NaN (%) |\" + \"\\n\"\n",
    "        \"| :------------: | :-----------: | :-----------------------: | :------------: | :------------: | :---------------: | :---------------: | :-----: |\\n\"\n",
    "    )\n",
    "    \n",
    "    for info in segments:\n",
    "        table += (\n",
    "            f\"| {to_latex(round_number(info['start_pi']))} \"\n",
    "            f\"| {to_latex(round_number(info['end_pi']))} \"\n",
    "            f\"| ${to_latex(round_number(info['average_thickness_pi']))}$ \"\n",
    "            f\"| {info['length_cells']:>14d} \"\n",
    "            f\"| {info['average_true_percentage']:>13.2f} \"\n",
    "            f\"| {info['average_true_percentage_ci_lower']:>17.2f} \"\n",
    "            f\"| {info['average_true_percentage_ci_upper']:>17.2f} \"\n",
    "            f\"| {info['nan_percentage']:>7.2f} |\\n\"\n",
    "        )\n",
    "    \n",
    "    return table + \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segments_to_dataframe(segments, threshold_value):\n",
    "    \"\"\"\n",
    "    Convert segment data to a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        segments (list): List of segment dictionaries containing segment info.\n",
    "        threshold_value (float): The threshold value to include in the DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing segment data with proper column names.\n",
    "    \"\"\"\n",
    "    if not segments:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert segments to DataFrame\n",
    "    df_data = []\n",
    "    for segment in segments:\n",
    "        row = {\n",
    "            'Threshold': threshold_value,\n",
    "            'Value/π Start': segment['start_pi'],\n",
    "            'Value/π End': segment['end_pi'],\n",
    "            'Avg. Thickness (Value/π)': segment['average_thickness_pi'],\n",
    "            'Length (Cells)': segment['length_cells'],\n",
    "            'Avg. True (%)': segment['average_true_percentage'],\n",
    "            '95% CI Lower (%)': segment['average_true_percentage_ci_lower'],\n",
    "            '95% CI Upper (%)': segment['average_true_percentage_ci_upper'],\n",
    "            'NaN (%)': segment['nan_percentage']\n",
    "        }\n",
    "        df_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(df_data)\n",
    "\n",
    "\n",
    "def write_segments_to_excel(vertical_segments, horizontal_segments, imag_threshold, is_imag_threshold_exponent):\n",
    "    \"\"\"\n",
    "    Write segment data to Excel file with separate worksheets for horizontal and vertical segments.\n",
    "    Creates a new file, overwriting any existing file.\n",
    "    Args:\n",
    "        vertical_segments (list): List of vertical segment dictionaries containing segment info.\n",
    "        horizontal_segments (list): List of horizontal segment dictionaries containing segment info.\n",
    "        imag_threshold (int or float): The imaginary threshold value.\n",
    "        is_imag_threshold_exponent (bool): If True, treat imag_threshold as an exponent (10^imag_threshold).\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    excel_file_path='results/segments.xlsx'\n",
    "    # Determine threshold value for Excel\n",
    "    if is_imag_threshold_exponent:\n",
    "        threshold_value = 10**imag_threshold\n",
    "    else:\n",
    "        threshold_value = imag_threshold\n",
    "    \n",
    "    # Create DataFrames\n",
    "    vertical_df = segments_to_dataframe(vertical_segments, threshold_value)\n",
    "    horizontal_df = segments_to_dataframe(horizontal_segments, threshold_value)\n",
    "    \n",
    "    # Write to Excel with formatting\n",
    "    with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:\n",
    "        if not horizontal_df.empty:\n",
    "            horizontal_df.to_excel(writer, sheet_name='Horizontal', index=False)\n",
    "            # Apply Excel formatting\n",
    "            apply_excel_formatting(writer.sheets['Horizontal'], horizontal_df)\n",
    "            \n",
    "        if not vertical_df.empty:\n",
    "            vertical_df.to_excel(writer, sheet_name='Vertical', index=False)\n",
    "            # Apply Excel formatting\n",
    "            apply_excel_formatting(writer.sheets['Vertical'], vertical_df)\n",
    "    \n",
    "    display_and_log_print(f\"Segment data written to {excel_file_path}\")\n",
    "\n",
    "\n",
    "def append_segments_to_excel(vertical_segments, horizontal_segments, imag_threshold, is_imag_threshold_exponent):\n",
    "    \"\"\"\n",
    "    Append segment data to existing Excel file. If file doesn't exist, creates a new one.\n",
    "    Preserves existing data and adds new segment data.\n",
    "    \n",
    "    Args:\n",
    "        vertical_segments (list): List of vertical segment dictionaries containing segment info.\n",
    "        horizontal_segments (list): List of horizontal segment dictionaries containing segment info.\n",
    "        imag_threshold (int or float): The imaginary threshold value.\n",
    "        is_imag_threshold_exponent (bool): If True, treat imag_threshold as an exponent (10^imag_threshold).\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    excel_file_path='results/segments.xlsx'\n",
    "    # Determine threshold value for Excel\n",
    "    if is_imag_threshold_exponent:\n",
    "        threshold_value = 10**imag_threshold\n",
    "    else:\n",
    "        threshold_value = imag_threshold\n",
    "    \n",
    "    # Create DataFrames for new data\n",
    "    new_vertical_df = segments_to_dataframe(vertical_segments, threshold_value)\n",
    "    new_horizontal_df = segments_to_dataframe(horizontal_segments, threshold_value)\n",
    "    \n",
    "    # Read existing data if file exists\n",
    "    existing_vertical_df = pd.DataFrame()\n",
    "    existing_horizontal_df = pd.DataFrame()\n",
    "    \n",
    "    if os.path.exists(excel_file_path):\n",
    "        try:\n",
    "            # Read existing horizontal segments first\n",
    "            try:\n",
    "                existing_horizontal_df = pd.read_excel(excel_file_path, sheet_name='Horizontal')\n",
    "            except (ValueError, KeyError):\n",
    "                pass  # Sheet doesn't exist yet\n",
    "            \n",
    "            # Read existing vertical segments\n",
    "            try:\n",
    "                existing_vertical_df = pd.read_excel(excel_file_path, sheet_name='Vertical')\n",
    "            except (ValueError, KeyError):\n",
    "                pass  # Sheet doesn't exist yet\n",
    "        except Exception as e:\n",
    "            display_and_log_print(f\"Warning: Could not read existing Excel file: {e}\")\n",
    "    \n",
    "    # Combine existing and new data\n",
    "    if not existing_horizontal_df.empty and not new_horizontal_df.empty:\n",
    "        combined_horizontal_df = pd.concat([existing_horizontal_df, new_horizontal_df], ignore_index=True)\n",
    "    elif not new_horizontal_df.empty:\n",
    "        combined_horizontal_df = new_horizontal_df\n",
    "    else:\n",
    "        combined_horizontal_df = existing_horizontal_df\n",
    "    \n",
    "    if not existing_vertical_df.empty and not new_vertical_df.empty:\n",
    "        combined_vertical_df = pd.concat([existing_vertical_df, new_vertical_df], ignore_index=True)\n",
    "    elif not new_vertical_df.empty:\n",
    "        combined_vertical_df = new_vertical_df\n",
    "    else:\n",
    "        combined_vertical_df = existing_vertical_df\n",
    "    \n",
    "    # Sort the combined DataFrames before writing to Excel\n",
    "    if not combined_horizontal_df.empty:\n",
    "        combined_horizontal_df = combined_horizontal_df.sort_values(\n",
    "            by=['Threshold', 'Value/π Start'], \n",
    "            ascending=[True, False], \n",
    "            ignore_index=True\n",
    "        )\n",
    "    \n",
    "    if not combined_vertical_df.empty:\n",
    "        combined_vertical_df = combined_vertical_df.sort_values(\n",
    "            by=['Threshold', 'Value/π Start'], \n",
    "            ascending=[True, False], \n",
    "            ignore_index=True\n",
    "        )\n",
    "    \n",
    "    # Write to Excel with formatting\n",
    "    with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:\n",
    "        if not combined_horizontal_df.empty:\n",
    "            combined_horizontal_df.to_excel(writer, sheet_name='Horizontal', index=False)\n",
    "            apply_excel_formatting(writer.sheets['Horizontal'], combined_horizontal_df)\n",
    "            \n",
    "        if not combined_vertical_df.empty:\n",
    "            combined_vertical_df.to_excel(writer, sheet_name='Vertical', index=False)\n",
    "            apply_excel_formatting(writer.sheets['Vertical'], combined_vertical_df)\n",
    "    \n",
    "    display_and_log_print(f\"Segment data appended to {excel_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a715684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_segment_markdown_table(vertical_segments, horizontal_segments,\n",
    "                                    imag_threshold, is_imag_threshold_exponent):\n",
    "    \"\"\"\n",
    "    Generate Markdown tables for vertical and horizontal segments, with threshold noted.\n",
    "\n",
    "    Args:\n",
    "        vertical_segments (list): List of vertical segment dictionaries containing segment info.\n",
    "        horizontal_segments (list): List of horizontal segment dictionaries containing segment info.\n",
    "        imag_threshold (int or float): The imaginary threshold value. If is_imag_threshold_exponent is True,\n",
    "                                      it's an exponent (int), otherwise, it's the direct threshold value (float).\n",
    "        is_imag_threshold_exponent (bool): If True, treat imag_threshold as an exponent (10^imag_threshold),\n",
    "                                           if False, treat it as the direct threshold value.\n",
    "    \n",
    "    Returns:\n",
    "        str: Combined markdown tables for both vertical and horizontal segments.\n",
    "    \"\"\"\n",
    "    if is_imag_threshold_exponent:\n",
    "        title_threshold_str = f\"10^{{{imag_threshold}}}\"\n",
    "    else:\n",
    "        title_threshold_str = f\"{imag_threshold*100:.2f}%\"\n",
    "\n",
    "    full_table = create_segment_table(vertical_segments, \"vertical\", title_threshold_str)\n",
    "    full_table += create_segment_table(horizontal_segments, \"horizontal\", title_threshold_str)\n",
    "    \n",
    "    return full_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85bbe0",
   "metadata": {},
   "source": [
    "### Hardware Calibration and Threshold Selection\n",
    "\n",
    "The `classical_like_segments` function plays a crucial role in **hardware calibration and threshold selection** for coherent error detection. This function processes the pseudo-entropy data to identify and analyze \"classical-like\" regions within the parameter space. These regions are characterized by a positive real part and a very small imaginary component of the pseudo-entropy, indicating that the quantum system behaves predictably, similar to a classical system, under specific conditions.\n",
    "\n",
    "The primary purpose of this analysis is to:\n",
    "\n",
    "1.  **Identify Stable Operating Points:** Pinpoint regions where coherent errors are minimal or where the system's response is well-behaved, which can inform optimal operating parameters for quantum hardware.\n",
    "2.  **Establish Error Detection Thresholds:** By analyzing the boundaries between \"classical-like\" and \"quantum\" (error-affected) regions, we can define practical thresholds for detecting coherent errors. The function allows for both direct numerical thresholds and exponent-based thresholds (e.g., $10^{-X}$), providing flexibility in defining sensitivity.\n",
    "3.  **Quantify Robustness:** The analysis of continuous segments within these classical-like regions, along with Clopper-Pearson confidence intervals, provides a statistically robust measure of the system's resilience to coherent errors. Longer and more robust segments indicate greater stability against variations in $\\beta$ and $\\delta$.\n",
    "4.  **Inform Calibration Strategies:** Understanding the landscape of classical-like behavior helps in designing more efficient calibration protocols, focusing efforts on regions where the system is most sensitive to coherent errors or where fine-tuning yields the greatest improvement in fidelity.\n",
    "\n",
    "The function iterates through a range of imaginary thresholds (or a list of specific thresholds) to generate:\n",
    "\n",
    "* **Binary Plots:** Visualizations that clearly delineate classical-like, quantum, and undefined (NaN) regions.\n",
    "* **Segment Analysis:** Identification and characterization of continuous segments of classical-like behavior.\n",
    "* **Markdown Tables:** Detailed statistical summaries of these segments, including their length, average \"true\" percentage, and confidence intervals.\n",
    "* **Excel Export:** All segment data is systematically saved to an Excel file for further external analysis and reporting.\n",
    "\n",
    "This systematic approach provides a comprehensive diagnostic tool for characterizing and mitigating coherent errors in quantum computing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classical_quantum_plot(data_to_plot, plot_extent, imag_threshold, is_imag_threshold_exponent, title_suffix=\"\", filename_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Helper function to create and save a binary plot of classical-like vs quantum regions.\n",
    "\n",
    "    Args:\n",
    "        data_to_plot (bool ndarray): The boolean mask to plot (True for classical-like).\n",
    "        plot_extent (list): The plot boundaries [x_min, x_max, y_min, y_max].\n",
    "        imag_threshold (int or float): The imaginary threshold. If is_imag_threshold_exponent is True,\n",
    "                                      it's an exponent (int), otherwise, it's the direct threshold value (float).\n",
    "        is_imag_threshold_exponent (bool): If True, treat imag_threshold as an exponent (10^imag_threshold),\n",
    "                                           if False, treat it as the direct threshold value.\n",
    "        title_suffix (str, optional): Additional text to append to the plot title. Defaults to \"\".\n",
    "        filename_suffix (str, optional): Additional text to append to the filename. Defaults to \"\".\n",
    "    \"\"\"\n",
    "    if is_imag_threshold_exponent:\n",
    "        threshold_label = f\"{np.abs(imag_threshold):.0f}\"\n",
    "        title_threshold_str = f\"10^{{{imag_threshold}}}\"\n",
    "        filename_threshold_str = f\"{np.abs(imag_threshold)}\"\n",
    "    else:\n",
    "        threshold_label = f\"{imag_threshold*100:.2f}\"\n",
    "        title_threshold_str = f\"{threshold_label}\\\\%\"\n",
    "        filename_threshold_str = threshold_label.replace(\".\", \"_\").replace(\"-\", \"\")\n",
    "\n",
    "    binary_cmap = mcolors.ListedColormap(['black', 'white'])\n",
    "\n",
    "    plt.figure(figsize=(12, 10), dpi=300)\n",
    "    plt.imshow(data_to_plot, extent=plot_extent, origin='lower', aspect='auto', cmap=binary_cmap)\n",
    "\n",
    "    cbar = plt.colorbar(ticks=[0.25, 0.75])\n",
    "    cbar.ax.set_yticklabels(['Quantum Region', 'Classical-like Region'])\n",
    "\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "    plt.xlabel(r'Controlled Interaction Strength $\\beta / \\pi$', fontsize=14)\n",
    "    plt.ylabel(r'Coherent Error Angle $\\delta / \\pi$', fontsize=14)\n",
    "    plt.title(\n",
    "        rf'Classical-like vs Quantum Behavior Regions of Pseudo-Entropy $\\check{{S}}$ (Threshold: $|Im(\\check{{S}})| < {title_threshold_str}$)'\n",
    "        + title_suffix,\n",
    "        fontsize=16\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    filename = f'results/classical_vs_quantum_regions_thresh_{filename_threshold_str}{filename_suffix}.png'\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    latex_import = (\n",
    "        \"\\\\begin{figure}[ht]\\n\"\n",
    "        \"    \\\\centering\\n\"\n",
    "        f\"    \\\\includegraphics[width=0.8\\\\textwidth]{{{filename}}}\\n\"\n",
    "        f\"    \\\\caption{{Classical-like vs Quantum Behavior Regions of Pseudo-Entropy $\\\\check{{S}}$ \"\n",
    "        f\"with $|\\\\Im(\\\\check{{S}})| < {title_threshold_str}$\" + title_suffix + \"}\\n\"\n",
    "        f\"    \\\\label{{fig:classical_quantum_regions_{filename_threshold_str.replace('.', '_').replace('-', '')}{filename_suffix.replace('-', '_')}}}\\n\"\n",
    "        \"\\\\end{figure}\"\n",
    "    )\n",
    "    display_and_log_print(latex_import)\n",
    "\n",
    "\n",
    "def plot_classical_like_quantum(betas, deltas, data, imag_threshold, is_imag_threshold_exponent, zoom_factor=None):\n",
    "    \"\"\"\n",
    "    Generate a binary visualization highlighting regions where pseudo-entropy behaves more\n",
    "    classically (positive real part and small imaginary component) versus purely quantum regions,\n",
    "    while also identifying undefined (NaN) regions. Optionally, create additional zoomed-in views.\n",
    "\n",
    "    Args:\n",
    "        betas (array): Array of measurement angles.\n",
    "        deltas (array): Array of coherent error angles.\n",
    "        data (complex ndarray): Complex array containing pseudo-entropy or related values to plot.\n",
    "        imag_threshold (int or float): The imaginary threshold. If is_imag_threshold_exponent is True,\n",
    "                                      it's an exponent (int), otherwise, it's the direct threshold value (float).\n",
    "        is_imag_threshold_exponent (bool): If True, treat imag_threshold as an exponent (10^imag_threshold),\n",
    "                                           if False, treat it as the direct threshold value.\n",
    "        zoom_factor (float, optional): Factor to zoom in on central region of the plot. If None, no zoom is applied.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A boolean mask with shape (len(deltas), len(betas)), where the\n",
    "                       rows correspond to the values in the deltas array and the\n",
    "                       columns correspond to the values in the betas array.\n",
    "                       True indicates classical-like regions and False indicates\n",
    "                       quantum regions for the corresponding ($\\\\delta$, $\\\\beta$) parameter pair.\n",
    "\n",
    "    Description:\n",
    "        This function creates one or more visualizations with three distinct regions:\n",
    "        - **White regions**: Classical-like behavior (Re(Ŝ) > 0 and |Im(Ŝ)| < threshold)\n",
    "        - **Black regions**: Quantum behavior (conditions not met)\n",
    "        - **Gray regions**: Undefined/NaN regions (calculation failed)\n",
    "\n",
    "        If zoom_factor is provided, an additional plot focusing on the central portion is generated.\n",
    "    \"\"\"\n",
    "    # Threshold handling\n",
    "    if is_imag_threshold_exponent:\n",
    "        threshold = 10 ** imag_threshold\n",
    "        threshold_label = f\"10^{{{imag_threshold}}}\"\n",
    "    else:\n",
    "        threshold = imag_threshold\n",
    "        threshold_label = f\"{imag_threshold*100:.2f}\\\\%\"\n",
    "\n",
    "    # Create masks\n",
    "    nan_mask = np.isnan(data.real) | np.isnan(data.imag)\n",
    "    classical_mask = (~nan_mask) & (data.real > 0) & (np.abs(data.imag) < threshold)\n",
    "    quantum_mask = (~nan_mask) & ~classical_mask\n",
    "\n",
    "    # Define plot extents\n",
    "    extent = [betas[0] / np.pi, betas[-1] / np.pi, deltas[0] / np.pi, deltas[-1] / np.pi]\n",
    "\n",
    "    # Create plots\n",
    "    create_classical_quantum_plot(classical_mask, extent, imag_threshold, is_imag_threshold_exponent)\n",
    "\n",
    "    # Zoomed plot handling (unchanged)\n",
    "    if zoom_factor is not None:\n",
    "        center_delta_index = len(deltas) // 2\n",
    "        center_beta_index = len(betas) // 2\n",
    "\n",
    "        # Invert zoom logic: larger zoom_factor means smaller view window\n",
    "        view_window_factor = 1.0 / zoom_factor\n",
    "\n",
    "        delta_half_range = int((view_window_factor * len(deltas)) // 2)\n",
    "        beta_half_range = int((view_window_factor * len(betas)) // 2)\n",
    "\n",
    "        delta_start = max(0, center_delta_index - delta_half_range)\n",
    "        delta_end = min(len(deltas), center_delta_index + delta_half_range)\n",
    "        beta_start = max(0, center_beta_index - beta_half_range)\n",
    "        beta_end = min(len(betas), center_beta_index + beta_half_range)\n",
    "\n",
    "        zoomed_classical_mask = classical_mask[delta_start:delta_end, beta_start:beta_end]\n",
    "        zoomed_deltas = deltas[delta_start:delta_end]\n",
    "        zoomed_betas = betas[beta_start:beta_end]\n",
    "        zoomed_extent = [zoomed_betas[0] / np.pi, zoomed_betas[-1] / np.pi,\n",
    "                         zoomed_deltas[0] / np.pi, zoomed_deltas[-1] / np.pi]\n",
    "\n",
    "        # Add zoom factor to the title\n",
    "        zoom_title_suffix = f\" (Zoomed {zoom_factor}x)\"\n",
    "        zoom_filename_suffix = f\"_zoom_{zoom_factor}x\"\n",
    "        create_classical_quantum_plot(\n",
    "            zoomed_classical_mask,\n",
    "            zoomed_extent,\n",
    "            imag_threshold,\n",
    "            is_imag_threshold_exponent,\n",
    "            zoom_title_suffix,\n",
    "            zoom_filename_suffix\n",
    "        )\n",
    "\n",
    "    # Enhanced statistics with NaN percentage\n",
    "    total_points = data.size\n",
    "    classical_points = np.sum(classical_mask)\n",
    "    quantum_points = np.sum(quantum_mask)\n",
    "    nan_points = np.sum(nan_mask)\n",
    "    \n",
    "    classical_percentage = (classical_points / total_points) * 100\n",
    "    quantum_percentage = (quantum_points / total_points) * 100\n",
    "    nan_percentage = (nan_points / total_points) * 100\n",
    "\n",
    "    # Enhanced markdown table with NaN\n",
    "    latex_table_stats = (\n",
    "        \"$$\\n\"\n",
    "        \"\\\\begin{array}{|l|c|c|} \\\\hline \"\n",
    "        \"\\\\textbf{Region Type} & \\\\textbf{Point Count} & \\\\textbf{Percentage (\\\\%)} \\\\\\\\ \\\\hline \"\n",
    "        f\"\\\\text{{Classical-like}} & {classical_points} & {classical_percentage:.2f} \\\\\\\\ \\\\hline \"\n",
    "        f\"\\\\text{{Quantum}} & {quantum_points} & {quantum_percentage:.2f} \\\\\\\\ \\\\hline \"\n",
    "        f\"\\\\text{{Undefined (NaN)}} & {nan_points} & {nan_percentage:.2f} \\\\\\\\ \\\\hline \"\n",
    "        f\"\\\\textbf{{Total}} & \\\\textbf{{{total_points}}} & \\\\textbf{{100.00}} \\\\\\\\ \\\\hline \"\n",
    "        \"\\\\end{array}\\n\"\n",
    "        \"$$\"\n",
    "    )\n",
    "\n",
    "    # Enhanced informational text\n",
    "    info_text = (\n",
    "        r\"**Region Legend:**\"\n",
    "        \"\\n\\n\"\n",
    "        r\"- **White:** Classical-like ($\\Re(\\check{S}) > 0$ AND $|\\Im(\\check{S})| < \" + f\"{threshold_label}$)\"\n",
    "        \"\\n\"\n",
    "        r\"- **Black:** Quantum regions\"\n",
    "        \"\\n\"\n",
    "        r\"- **Gray:** Undefined/NaN (calculation failed)\"\n",
    "        \"\\n\\n\"\n",
    "        r\"**Note:** NaN regions occur where initial and final states are orthogonal (post-selection probability ≈ 0)\"\n",
    "    )\n",
    "\n",
    "    display_and_log_markdown(info_text)\n",
    "    display_and_log_markdown(latex_table_stats)\n",
    "\n",
    "    return classical_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277520e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_like_segments(betas, deltas, numerical_entropies, threshold_source, is_imag_threshold_exponent, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Iterates through a range of imaginary thresholds (or a list of thresholds) and generates plots,\n",
    "    segment analysis, and Markdown tables, including confidence intervals for segment proportions.\n",
    "\n",
    "    Args:\n",
    "        betas (array): Array of measurement angles.\n",
    "        deltas (array): Array of coherent error angles.\n",
    "        numerical_entropies (complex ndarray): Complex array containing pseudo-entropy or related values.\n",
    "        threshold_source (range or list or float): If is_imag_threshold_exponent is True, this is a range of exponents.\n",
    "                                          If False, this is a list of floats (or a single float) representing the direct threshold.\n",
    "        is_imag_threshold_exponent (bool): If True, treat imag_threshold as an exponent (10^imag_threshold),\n",
    "                                           if False, treat it as the direct threshold value.\n",
    "        confidence_level (float, optional): The confidence level for segment proportion CIs (e.g., 0.95 for 95%).\n",
    "    \"\"\"\n",
    "    if isinstance(threshold_source, (range, list, tuple)):\n",
    "        threshold_values_to_iterate = threshold_source\n",
    "    else:\n",
    "        threshold_values_to_iterate = [threshold_source]  # Wrap single value in list for iteration\n",
    "\n",
    "    for imag_threshold in threshold_values_to_iterate:\n",
    "        # Run function with your data\n",
    "        classical_mask = plot_classical_like_quantum(\n",
    "            betas, deltas, numerical_entropies, imag_threshold, is_imag_threshold_exponent, 4\n",
    "        )\n",
    "\n",
    "        # Analyze vertical and horizontal segments, passing the confidence_level\n",
    "        vertical_segments = analyze_segments(\n",
    "            classical_mask, betas, deltas, white_percentage_threshold=0.3, is_vertical=True, confidence_level=confidence_level\n",
    "        )\n",
    "\n",
    "        horizontal_segments = analyze_segments(\n",
    "            classical_mask, betas, deltas, white_percentage_threshold=0.3, is_vertical=False, confidence_level=confidence_level\n",
    "        )\n",
    "\n",
    "        # Generate and display Markdown table\n",
    "        markdown_table = generate_segment_markdown_table(\n",
    "            vertical_segments, horizontal_segments, imag_threshold, is_imag_threshold_exponent\n",
    "        )\n",
    "        display_and_log_markdown(markdown_table)\n",
    "\n",
    "\n",
    "        # Write to Excel file\n",
    "        append_segments_to_excel(\n",
    "            vertical_segments, horizontal_segments, imag_threshold, is_imag_threshold_exponent\n",
    "        )\n",
    "\n",
    "        # Visualize segments\n",
    "        visualize_segments(\n",
    "            classical_mask, betas, deltas, vertical_segments, horizontal_segments,\n",
    "            imag_threshold, is_imag_threshold_exponent, 4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f843cc1",
   "metadata": {},
   "source": [
    "### Using a range of exponents\n",
    "\n",
    "This section initiates the analysis of classical-like segments by iterating through a predefined range of imaginary thresholds, expressed as exponents of 10. This approach allows for a systematic exploration of how the definition of \"classical-like\" behavior (i.e., the tolerance for the imaginary component of pseudo-entropy) impacts the identification and characterization of stable operating regions. The results from this iterative analysis are visualized in binary plots, summarized in Markdown tables, and exported to Excel for detailed review, providing insights into the robustness and sensitivity of the pseudo-entropy method across different error tolerances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a range of exponents\n",
    "display_and_log_print(\"\\n--- Running analysis for a range of exponents ---\")\n",
    "classical_like_segments(betas, deltas, numerical_entropies, range(-5, -1), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18594544",
   "metadata": {},
   "source": [
    "### Using CSV Data for Threshold Values\n",
    "\n",
    "This section extends the analysis of classical-like segments by incorporating threshold values directly from a CSV file. This allows for the use of empirically derived or hardware-specific error rates as thresholds for identifying classical-like behavior. By leveraging external data sources, this approach enables a more practical and context-aware assessment of stable operating regions and error detection capabilities. The process involves reading the 'Total_Readout_Error_Percent' from the `optimal_qubit_groups.csv` file, converting these percentages to decimal thresholds, and then applying them to the `classical_like_segments` function. This provides a direct link between experimental hardware characteristics and the theoretical framework of pseudo-entropy for coherent error detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CSV data for a single threshold\n",
    "display_and_log_print(\"\\n--- Running analysis with threshold from CSV data ---\")\n",
    "csv_thresholds = []\n",
    "csv_is_imag_threshold_exponent = False # Assume direct value from CSV\n",
    "\n",
    "try:\n",
    "    with open('hardware/optimal_qubit_groups.csv', 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if 'Total_Readout_Error_Percent' in row and row['Total_Readout_Error_Percent']:\n",
    "                readout_error_str = row['Total_Readout_Error_Percent'].replace('%', '')\n",
    "                try:\n",
    "                    csv_imag_threshold = float(readout_error_str) / 100.0 # Convert to decimal\n",
    "                    csv_thresholds.append(csv_imag_threshold)\n",
    "                except ValueError:\n",
    "                    display_and_log_print(f\"Warning: Could not convert 'Total_Readout_Error_Percent' = '{row['Total_Readout_Error_Percent']}' to float from CSV. Skipping this row.\")\n",
    "except FileNotFoundError:\n",
    "    display_and_log_print(\"Warning: CSV file 'results/optimal_qubit_groups.csv' not found. Cannot use CSV threshold.\")\n",
    "except KeyError:\n",
    "    display_and_log_print(\"Warning: Column 'Total_Readout_Error_Percent' not found in CSV. Cannot use CSV threshold.\")\n",
    "\n",
    "if csv_thresholds:\n",
    "    classical_like_segments(betas, deltas, numerical_entropies, csv_thresholds, csv_is_imag_threshold_exponent)\n",
    "else:\n",
    "    display_and_log_print(\"No valid CSV thresholds found, skipping CSV-based plot.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74117403",
   "metadata": {},
   "source": [
    "### Unpacking Parameter Sensitivity and Partial Derivatives\n",
    "\n",
    "This section explores the sensitivity of the pseudo-entropy to changes in the $\\beta$ and $\\delta$ parameters by calculating and visualizing their partial derivatives. These \"sensitivity maps\" reveal which parameters have the most significant impact on the pseudo-entropy's behavior, guiding intuition for parameter selection and highlighting areas where fine-tuning is most effective. The *detailed sub-subsection analysis of sensitivity methods* and the *full phase diagram plots* (if distinct from the sensitivity maps) are offloaded to the supplementary materials for in-depth review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928ea38",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_sensitivity(\n",
    "    numerical_entropies, \n",
    "    betas, \n",
    "    deltas,  \n",
    "    segment_info, \n",
    "    is_vertical=True,\n",
    "    zero_threshold=1e-3, \n",
    "    proximity_factor=0.5,\n",
    "    log_base=np.e,\n",
    "    epsilon=1e-15  \n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate sensitivity of imaginary part near classical-like regions for either vertical or horizontal segments.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    numerical_entropies : numpy.ndarray\n",
    "        2D array of imaginary part of pseudo-entropy\n",
    "    betas : numpy.ndarray\n",
    "        Beta values corresponding to columns\n",
    "    deltas : numpy.ndarray\n",
    "        Delta values corresponding to rows\n",
    "    segment_info : list of dict\n",
    "        Pre-existing segments information\n",
    "    is_vertical : bool, optional\n",
    "        If True, analyze vertical segments; if False, analyze horizontal segments\n",
    "    zero_threshold : float, optional\n",
    "        Threshold for considering values \"near zero\"\n",
    "    proximity_factor : float, optional\n",
    "        Factor to determine proximity to segment gaps\n",
    "    log_base : float, optional\n",
    "        Base for logarithmic sensitivity calculation\n",
    "    epsilon : float, optional\n",
    "        Small value to prevent log(0)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sensitivity_info : list of dict\n",
    "        Sensitivity information for each classical-like region\n",
    "    \"\"\"\n",
    "    sensitivity_info = []\n",
    "\n",
    "    # Sort segments by start position\n",
    "    if is_vertical:\n",
    "        # For vertical segments, sort by beta (column) position\n",
    "        axis_start_key = 'start_pi'\n",
    "        axis_end_key = 'end_pi'\n",
    "        primary_axis_values = betas\n",
    "    else:\n",
    "        # For horizontal segments, sort by delta (row) position\n",
    "        axis_start_key = 'start_pi'\n",
    "        axis_end_key = 'end_pi'\n",
    "        primary_axis_values = deltas\n",
    "        # For horizontal analysis, transpose the entropy array\n",
    "        numerical_entropies = numerical_entropies.T\n",
    "\n",
    "    # Sort segments by their start position\n",
    "    sorted_segments = sorted(segment_info, key=lambda s: s[axis_start_key])\n",
    "\n",
    "    for i, segment in enumerate(sorted_segments):\n",
    "        segment_start_pi = segment[axis_start_key]\n",
    "        segment_end_pi = segment[axis_end_key]\n",
    "\n",
    "        # Compute gaps separately\n",
    "        prev_segment_end = sorted_segments[i - 1][axis_end_key] if i > 0 else None\n",
    "        next_segment_start = sorted_segments[i + 1][axis_start_key] if i < len(sorted_segments) - 1 else None\n",
    "\n",
    "        gap_prev = segment_start_pi - prev_segment_end if prev_segment_end is not None else np.inf\n",
    "        gap_next = next_segment_start - segment_end_pi if next_segment_start is not None else np.inf\n",
    "\n",
    "        # Apply separate proximity factors\n",
    "        proximity_range_prev = proximity_factor * gap_prev\n",
    "        proximity_range_next = proximity_factor * gap_next\n",
    "\n",
    "        # Find the central index for this segment\n",
    "        central_index = np.argmin(np.abs(primary_axis_values / np.pi - (segment_start_pi + segment_end_pi) / 2))\n",
    "        central_value = primary_axis_values[central_index] / np.pi\n",
    "\n",
    "        # Extract the line of imaginary entropy for this position\n",
    "        imaginary_line = np.abs(numerical_entropies[:, central_index])\n",
    "\n",
    "        # Find indices close to zero\n",
    "        near_zero_indices = np.where(imaginary_line < zero_threshold)[0]\n",
    "\n",
    "        # Calculate the valid range in primary axis space (with proximity)\n",
    "        start_with_proximity = segment_start_pi - proximity_range_prev\n",
    "        end_with_proximity = segment_end_pi + proximity_range_next\n",
    "\n",
    "        # Map these to indices in the primary axis array\n",
    "        valid_indices = np.where(\n",
    "            (primary_axis_values / np.pi >= start_with_proximity) & \n",
    "            (primary_axis_values / np.pi <= end_with_proximity)\n",
    "        )[0]\n",
    "\n",
    "        # Filter the near_zero_indices based on the valid indices\n",
    "        valid_near_zero_indices = np.intersect1d(near_zero_indices, valid_indices)\n",
    "\n",
    "        # Ensure enough valid points for meaningful sensitivity calculation\n",
    "        if len(valid_near_zero_indices) < 2:\n",
    "            continue  # Skip segment if insufficient points\n",
    "\n",
    "        # Extract absolute imaginary part only\n",
    "        imag_part = np.abs(np.imag(numerical_entropies[valid_near_zero_indices, central_index]))\n",
    "\n",
    "        # Apply log without division and ensure that values are greater than epsilon\n",
    "        log_sensitivity = np.full_like(imag_part, np.nan, dtype=float)\n",
    "\n",
    "        # Only compute log for values greater than epsilon\n",
    "        log_mask = imag_part > epsilon\n",
    "        log_sensitivity[log_mask] = np.log(imag_part[log_mask]) / np.log(log_base)\n",
    "\n",
    "        # Create a result dictionary with all the info\n",
    "        result = {\n",
    "            'central_value_pi': central_value,\n",
    "            'log_sensitivity': log_sensitivity,\n",
    "            'valid_indices': valid_near_zero_indices,\n",
    "            'proximity_range_prev': proximity_range_prev,\n",
    "            'proximity_range_next': proximity_range_next,\n",
    "            'is_vertical': is_vertical,\n",
    "            **segment  # Preserve all original segment information\n",
    "        }\n",
    "        \n",
    "        # Add specific keys based on orientationß\n",
    "        if is_vertical:\n",
    "            result['central_beta_pi'] = central_value\n",
    "        else:\n",
    "            result['central_delta_pi'] = central_value\n",
    "            \n",
    "        sensitivity_info.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06876cf0",
   "metadata": {},
   "source": [
    "With our sensitivities calculated, let's see how these calculations can show up visually to guide our intution! We will use these images to help improve how parameters are selected, and make changes in the right direction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_symmetric_log_norm(data):\n",
    "    \"\"\"Creates a symmetric log norm with improved separation around zero.\"\"\"\n",
    "    # Filter out non-finite values (NaN, inf) before calculations\n",
    "    finite_data = data[np.isfinite(data)]\n",
    "    \n",
    "    if finite_data.size == 0:\n",
    "        display_and_log_print(\"Warning: No finite data for normalization. Using default normalization.\")\n",
    "        return None\n",
    "    \n",
    "    vabs_max = np.abs(finite_data).max()\n",
    "    \n",
    "    # Check for constant data\n",
    "    if vabs_max == 0:\n",
    "        display_and_log_print(\"Warning: All finite data is zero. Using default normalization.\")\n",
    "        return None\n",
    "    \n",
    "    # Use finite data for median calculation\n",
    "    data_median = np.median(np.abs(finite_data))\n",
    "    \n",
    "    # Ensure median is not zero (which could cause issues)\n",
    "    if data_median == 0:\n",
    "        # Use a small fraction of the max value instead\n",
    "        data_median = vabs_max * 0.01\n",
    "    \n",
    "    try:\n",
    "        return mcolors.SymLogNorm(linthresh=data_median, linscale=data_median,\n",
    "                                vmin=-vabs_max, vmax=vabs_max)\n",
    "    except ValueError as e:\n",
    "        display_and_log_print(f\"Warning: Failed to create SymLogNorm: {e}. Using default normalization.\")\n",
    "        return None\n",
    "\n",
    "def display_heatmap(data, x_coords, y_coords, title, x_label, y_label,\n",
    "                    color_map='viridis', normalization=None, filename=None,\n",
    "                    results_dir=\"results\", cbar_label=None):\n",
    "    \"\"\"Displays a heatmap with consistent formatting and saves it to the specified folder.\"\"\"\n",
    "\n",
    "    # Check for all-NaN data\n",
    "    if np.all(np.isnan(data)):\n",
    "        display_and_log_print(\"Warning: All data is NaN. Skipping heatmap plot.\")\n",
    "        return None\n",
    "\n",
    "    # Check for constant data (excluding NaNs)\n",
    "    finite_data = data[np.isfinite(data)]\n",
    "    if finite_data.size == 0:\n",
    "        display_and_log_print(\"Warning: No finite data. Skipping heatmap plot.\")\n",
    "        return None\n",
    "    if np.nanmax(data) == np.nanmin(data):\n",
    "        display_and_log_print(\"Warning: Data is constant. Heatmap may not be informative.\")\n",
    "        # Set normalization to None for constant data\n",
    "        normalization = None\n",
    "\n",
    "    figure, axes = plt.subplots(figsize=(12, 9))\n",
    "    masked_data = np.ma.masked_invalid(data)\n",
    "    \n",
    "    try:\n",
    "        image = axes.imshow(masked_data, cmap=color_map, norm=normalization, aspect='auto')\n",
    "    except ValueError as e:\n",
    "        display_and_log_print(f\"Warning: imshow failed with normalization ({e}). Using default normalization.\")\n",
    "        image = axes.imshow(masked_data, cmap=color_map, norm=None, aspect='auto')\n",
    "\n",
    "    axes.set_xlabel(x_label)\n",
    "    axes.set_ylabel(y_label)\n",
    "    axes.set_title(title)\n",
    "\n",
    "    num_x_ticks = 10\n",
    "    num_y_ticks = 10\n",
    "    x_tick_indices = np.linspace(0, len(x_coords) - 1, num_x_ticks).astype(int)\n",
    "    y_tick_indices = np.linspace(0, len(y_coords) - 1, num_y_ticks).astype(int)\n",
    "\n",
    "    axes.set_xticks(x_tick_indices)\n",
    "    axes.set_yticks(y_tick_indices)\n",
    "    axes.set_xticklabels(np.round(x_coords[x_tick_indices], 2), rotation=45, ha='right')\n",
    "    axes.set_yticklabels(np.round(y_coords[y_tick_indices], 3))\n",
    "\n",
    "    try:\n",
    "        color_bar = plt.colorbar(image, ax=axes, orientation=\"horizontal\")\n",
    "        color_bar.ax.tick_params(rotation=45, labelsize=8, pad=15)\n",
    "        if cbar_label:\n",
    "            color_bar.set_label(cbar_label)\n",
    "    except ValueError as e:\n",
    "        display_and_log_print(f\"Warning: Colorbar creation failed ({e}). Plot created without colorbar.\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        filepath = os.path.join(results_dir, filename)\n",
    "        plt.savefig(filepath)\n",
    "    plt.show()\n",
    "    plt.close(figure)\n",
    "    return filename.replace(\".png\", \"\") if filename else None\n",
    "\n",
    "def generate_latex_figure_tag(filename_base, caption, results_dir=\"results\"):\n",
    "    \"\"\"Generates a LaTeX figure environment string.\"\"\"\n",
    "    filename = f\"{filename_base}.png\"\n",
    "    label = f\"fig:{filename_base.replace('.', '_')}\"  # Make label LaTeX-friendly\n",
    "    latex_tag = (\n",
    "        f\"\\\\begin{{figure}}[htbp]\\n\"\n",
    "        f\"    \\\\centering\\n\"\n",
    "        f\"    \\\\includegraphics[width=\\\\textwidth]{{results/{filename}}}\\n\"\n",
    "        f\"    \\\\caption{{{caption}}}\\n\"\n",
    "        f\"    \\\\label{{{label}}}\\n\"\n",
    "        f\"\\\\end{{figure}}\\n\"\n",
    "    )\n",
    "    return latex_tag\n",
    "\n",
    "def plot_pseudo_entropy_derivatives(entropies, beta_step, delta_step, betas, deltas, results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Plots the absolute value and differentiation of the imaginary and real parts\n",
    "    of pseudo entropy and saves the figures to the specified folder.\n",
    "    The x and y axes represent the angles divided by pi (dimensionless).\n",
    "    The derivatives have units of bits.\n",
    "\n",
    "    Args:\n",
    "        entropies (np.ndarray): 2D array of pseudo-entropy values (complex, bits).\n",
    "        beta_step (float): Step size in beta (radians).\n",
    "        delta_step (float): Step size in delta (radians).\n",
    "        betas (np.ndarray): 1D array of beta values (radians).\n",
    "        deltas (np.ndarray): 1D array of delta values (radians).\n",
    "        results_dir (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    beta_step_norm = beta_step / np.pi\n",
    "    delta_step_norm = delta_step / np.pi\n",
    "    betas_norm = betas / np.pi\n",
    "    deltas_norm = deltas / np.pi\n",
    "\n",
    "    imaginary_abs = np.abs(entropies.imag)  # Units: bits\n",
    "    real_part_abs = np.abs(entropies.real)   # Units: bits\n",
    "\n",
    "    d_imag_abs_d_delta = np.gradient(imaginary_abs, delta_step_norm, axis=0)  # Units: bits\n",
    "    d_imag_abs_d_beta = np.gradient(imaginary_abs, beta_step_norm, axis=1)    # Units: bits\n",
    "    d_real_abs_d_delta = np.gradient(real_part_abs, delta_step_norm, axis=0)  # Units: bits\n",
    "    d_real_abs_d_beta = np.gradient(real_part_abs, beta_step_norm, axis=1)    # Units: bits\n",
    "\n",
    "    latex_tags = []\n",
    "\n",
    "    # Plot |dIm/dBeta|\n",
    "    filename = \"d_abs_imag_pseudo_entropy_d_beta.png\"\n",
    "    plot_title = r\"Imaginary Component Sensitivity to $\\beta$: $\\left|\\frac{\\partial \\Im(\\check{S})}{\\partial \\beta}\\right|$\"\n",
    "    plot_caption = r\"Absolute value of partial derivative of imaginary pseudo entropy with respect to $\\beta$. This shows the sensitivity of the imaginary component to changes in controlled interaction strength, measured in bits.\"\n",
    "    display_heatmap(d_imag_abs_d_beta, betas_norm, deltas_norm,\n",
    "                    plot_title,\n",
    "                    r\"Controlled Interaction Strength $\\beta / \\pi$\", r\"Coherent Error Angle $\\delta / \\pi$\", color_map='coolwarm',\n",
    "                    normalization=create_symmetric_log_norm(d_imag_abs_d_beta),\n",
    "                    filename=filename, results_dir=results_dir, cbar_label='Magnitude (bits)')\n",
    "    latex_tags.append(generate_latex_figure_tag(filename.replace(\".png\", \"\"), plot_caption, results_dir=results_dir))\n",
    "\n",
    "    # Plot |dIm/dDelta|\n",
    "    filename = \"d_abs_imag_pseudo_entropy_d_delta.png\"\n",
    "    plot_title = r\"Imaginary Component Sensitivity to $\\delta$: $\\left|\\frac{\\partial \\Im(\\check{S})}{\\partial \\delta}\\right|$\"\n",
    "    plot_caption = r\"Absolute value of partial derivative of imaginary pseudo entropy with respect to $\\delta$. This shows the sensitivity of the imaginary component to changes in coherent error angle, measured in bits.\"\n",
    "    display_heatmap(d_imag_abs_d_delta, betas_norm, deltas_norm,\n",
    "                    plot_title,\n",
    "                    r\"Controlled Interaction Strength $\\beta / \\pi$\", r\"Coherent Error Angle $\\delta / \\pi$\", color_map='coolwarm',\n",
    "                    normalization=create_symmetric_log_norm(d_imag_abs_d_delta),\n",
    "                    filename=filename, results_dir=results_dir, cbar_label='Magnitude (bits)')\n",
    "    latex_tags.append(generate_latex_figure_tag(filename.replace(\".png\", \"\"), plot_caption, results_dir=results_dir))\n",
    "\n",
    "    # Plot |dRe/dBeta|\n",
    "    filename = \"d_abs_real_pseudo_entropy_d_beta.png\"\n",
    "    plot_title = r\"Real Component Sensitivity to $\\beta$: $\\left|\\frac{\\partial \\Re(\\check{S})}{\\partial \\beta}\\right|$\"\n",
    "    plot_caption = r\"Absolute value of partial derivative of real pseudo entropy with respect to $\\beta$. This shows the sensitivity of the real component to changes in controlled interactions strength, measured in bits.\"\n",
    "    display_heatmap(d_real_abs_d_beta, betas_norm, deltas_norm,\n",
    "                    plot_title,\n",
    "                    r\"Controlled Interaction Strength $\\beta / \\pi$\", r\"Coherent Error Angle $\\delta / \\pi$\", color_map='coolwarm',\n",
    "                    normalization=create_symmetric_log_norm(d_real_abs_d_beta),\n",
    "                    filename=filename, results_dir=results_dir, cbar_label='Magnitude (bits)')\n",
    "    latex_tags.append(generate_latex_figure_tag(filename.replace(\".png\", \"\"), plot_caption, results_dir=results_dir))\n",
    "\n",
    "    # Plot |dRe/dDelta|\n",
    "    filename = \"d_abs_real_pseudo_entropy_d_delta.png\"\n",
    "    plot_title = r\"Real Component Sensitivity to $\\delta$: $\\left|\\frac{\\partial \\Re(\\check{S})}{\\partial \\delta}\\right|$\"\n",
    "    plot_caption = r\"Absolute value of partial derivative of real pseudo entropy with respect to $\\delta$. This shows the sensitivity of the real component to changes in coherent error angle, measured in bits.\"\n",
    "    display_heatmap(d_real_abs_d_delta, betas_norm, deltas_norm,\n",
    "                    plot_title,\n",
    "                    r\"Controlled Interaction Strength $\\beta / \\pi$\", r\"Coherent Error Angle $\\delta / \\pi$\", color_map='coolwarm',\n",
    "                    normalization=create_symmetric_log_norm(d_real_abs_d_delta),\n",
    "                    filename=filename, results_dir=results_dir, cbar_label='Magnitude (bits)')\n",
    "    latex_tags.append(generate_latex_figure_tag(filename.replace(\".png\", \"\"), plot_caption, results_dir=results_dir))\n",
    "\n",
    "    display_and_log_print(\"\\nLaTeX Figure Tags:\")\n",
    "    for tag in latex_tags:\n",
    "        display_and_log_print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddd8f5",
   "metadata": {},
   "source": [
    "### Visualizing Partial Derivatives of Pseudo-Entropy\n",
    "\n",
    "To gain a deeper understanding of how the parameters $\\beta$ and $\\delta$ influence the pseudo-entropy, we'll now calculate and visualize the partial derivatives of the pseudo-entropy with respect to each parameter. These derivatives will reveal the sensitivity of the pseudo-entropy to changes in these parameters.\n",
    "\n",
    "By calculating and visualizing the derivatives, we can visually identify and pinpoint regions in which parameter adjustment will be the most influential for fine-tuning results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pseudo_entropy_derivatives(numerical_entropies, beta_step, delta_step, betas, deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c18d8a",
   "metadata": {},
   "source": [
    "### Analyzing Sensitivity in Pseudo-Entropy\n",
    "\n",
    "This section delves into analyzing the quantum circuit's behavior using a sensitivity metric. This sensitivity analysis tells us how much the parameters, $\\beta$ and $\\delta$, need to change to produce a noticeable variation in the pseudo-entropy $\\check{S}$. A lower sensitivity value implies that small changes in the parameter lead to significant changes in pseudo-entropy, indicating stronger responsiveness of the system to that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76014a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_min_max(array):\n",
    "    \"\"\"Compute min and max while handling NaN values.\"\"\"\n",
    "    if array is None or len(array) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    valid_values = array[~np.isnan(array)]\n",
    "    \n",
    "    if len(valid_values) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    return np.min(valid_values), np.max(valid_values)\n",
    "\n",
    "def calculate_sensitivity(betas, deltas, pseudo_entropies, std_deviations):\n",
    "    \"\"\"\n",
    "    Calculates sensitivity for beta and delta parameters based on the\n",
    "    pseudo-entropy values, their derivatives, and their standard deviations.\n",
    "\n",
    "    Args:\n",
    "        betas (np.ndarray): Array of beta values.\n",
    "        deltas (np.ndarray): Array of delta values.\n",
    "        pseudo_entropies (np.ndarray): NumPy array of complex pseudo-entropies\n",
    "                                      (delta as rows, betas as columns).\n",
    "        std_deviations (np.ndarray): NumPy array of standard deviations for pseudo-entropies\n",
    "                                    (delta as rows, betas as columns).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Sensitivity metrics and arrays for beta and delta.\n",
    "    \"\"\"\n",
    "    # Calculate derivatives using numpy.gradient\n",
    "    dSdbeta = np.gradient(pseudo_entropies, betas, axis=1)\n",
    "    dSddelta = np.gradient(pseudo_entropies, deltas, axis=0)\n",
    "    \n",
    "    # Use absolute values of derivatives for meaningful sensitivity\n",
    "    abs_dSdbeta = np.abs(dSdbeta)\n",
    "    abs_dSddelta = np.abs(dSddelta)\n",
    "    \n",
    "    # Use the calculated standard deviations for each point instead of a global uncertainty\n",
    "    # Calculate sensitivity arrays (pixel-wise) using the formula:\n",
    "    # Sensitivity_S(θ)^2 = σ_S^2 / |∂S/∂θ|^2\n",
    "    \n",
    "    # Add small epsilon to avoid division by zero\n",
    "    eps = np.finfo(float).eps\n",
    "    \n",
    "    # Calculate sensitivity arrays using the proper formula\n",
    "    sensitivity_beta_array = np.abs(std_deviations) / (abs_dSdbeta + eps)\n",
    "    sensitivity_delta_array = np.abs(std_deviations) / (abs_dSddelta + eps)\n",
    "    \n",
    "    # Calculate summary statistics using NaN-aware functions\n",
    "    sensitivity_beta = np.nanmedian(sensitivity_beta_array)\n",
    "    sensitivity_delta = np.nanmedian(sensitivity_delta_array)\n",
    "    \n",
    "    # Use the mean standard deviation as the general uncertainty value for reporting\n",
    "    uncertainty = np.nanmean(np.abs(std_deviations))\n",
    "    \n",
    "    return sensitivity_beta, sensitivity_delta, sensitivity_beta_array, sensitivity_delta_array, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity(betas, deltas, sensitivity_beta_array, sensitivity_delta_array):\n",
    "    \"\"\"\n",
    "    Plots Sensitivity heatmaps for beta and delta parameters.\n",
    "    The Sensitivity values have units of radians.\n",
    "    The x and y axes represent the angles divided by pi (dimensionless).\n",
    "\n",
    "    Args:\n",
    "        betas (np.ndarray): Array of beta values (radians).\n",
    "        deltas (np.ndarray): Array of delta values (radians).\n",
    "        sensitivity_beta_array (np.ndarray): 2D array of sensitivity values for beta (radians).\n",
    "        sensitivity_delta_array (np.ndarray): 2D array of sensitivity values for delta (radians).\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Use percentile-based limits for better visualization\n",
    "    vmin_beta, vmax_beta = np.percentile(sensitivity_beta_array, [5, 95])\n",
    "    vmin_delta, vmax_delta = np.percentile(sensitivity_delta_array, [5, 95])\n",
    "\n",
    "    # Create heatmaps for sensitivity\n",
    "    im1 = axes[0].imshow(sensitivity_beta_array,\n",
    "                         extent=[betas.min() / np.pi, betas.max() / np.pi,\n",
    "                                 deltas.min() / np.pi, deltas.max() / np.pi],\n",
    "                         origin='lower', aspect='auto', cmap='viridis',\n",
    "                         vmin=vmin_beta, vmax=vmax_beta)\n",
    "    axes[0].set_xlabel(r'Controlled Interaction Strength $\\beta / \\pi$')\n",
    "    axes[0].set_ylabel(r'Coherent Error Angle $\\delta / \\pi$')\n",
    "    axes[0].set_title('Sensitivity for $\\\\beta$ (radians)', pad=20)  # Add vertical padding\n",
    "    fig.colorbar(im1, ax=axes[0], label='(radians)')\n",
    "\n",
    "    im2 = axes[1].imshow(sensitivity_delta_array,\n",
    "                         extent=[betas.min() / np.pi, betas.max() / np.pi,\n",
    "                                 deltas.min() / np.pi, deltas.max() / np.pi],\n",
    "                         origin='lower', aspect='auto', cmap='viridis',\n",
    "                         vmin=vmin_delta, vmax=vmax_delta)\n",
    "    axes[1].set_xlabel(r'Controlled Interaction Strength $\\beta / \\pi$')\n",
    "    axes[1].set_ylabel(r'Coherent Error Angle $\\delta / \\pi$')\n",
    "    axes[1].set_title('Sensitivity for $\\\\delta$ (radians)', pad=20)\n",
    "    fig.colorbar(im2, ax=axes[1], label='(radians)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88)  # Adjusts space at the top if needed\n",
    "    plt.savefig(\"results/variables_sensitivity.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df52543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_min_max(array):\n",
    "    \"\"\"Compute min and max while handling NaN values.\"\"\"\n",
    "    if array is None or len(array) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    valid_values = array[~np.isnan(array)]\n",
    "    \n",
    "    if len(valid_values) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    return np.min(valid_values), np.max(valid_values)\n",
    "    \n",
    "def sensitivity_to_dataframe(sensitivity_beta, sensitivity_delta, uncertainty, \n",
    "                                  sensitivity_beta_array, sensitivity_delta_array):\n",
    "    \"\"\"\n",
    "    Converts sensitivity and uncertainty data into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    sensitivity_beta_min, sensitivity_beta_max = safe_min_max(sensitivity_beta_array)\n",
    "    sensitivity_delta_min, sensitivity_delta_max = safe_min_max(sensitivity_delta_array)\n",
    "\n",
    "    df_data = [\n",
    "        {'Metric': 'Global standard deviation of pseudo entropy', 'Symbol': r'$\\sigma_{\\check{S}}$',\n",
    "         'Mean Value': uncertainty, 'Min Value': np.nan, 'Max Value': np.nan, 'Units': 'bits'},\n",
    "        {'Metric': r'Sensitivity over $\\beta$', 'Symbol': r'$S_{\\beta}$',\n",
    "         'Mean Value': sensitivity_beta, 'Min Value': sensitivity_beta_min, 'Max Value': sensitivity_beta_max, 'Units': 'radians'},\n",
    "        {'Metric': r'Sensitivity over $\\delta$', 'Symbol': r'$S_{\\delta}$',\n",
    "         'Mean Value': sensitivity_delta, 'Min Value': sensitivity_delta_min, 'Max Value': sensitivity_delta_max, 'Units': 'radians'}\n",
    "    ]\n",
    "    return pd.DataFrame(df_data)\n",
    "\n",
    "def export_sensitivity_to_excel(sensitivity_df, filename='results/sensitivity_results.xlsx'):\n",
    "    \"\"\"\n",
    "    Exports a pandas DataFrame of sensitivity data to an Excel file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    writer = pd.ExcelWriter(filename, engine='openpyxl')\n",
    "\n",
    "    sheet_name = 'Sensitivity_Analysis'\n",
    "    \n",
    "    # If the sheet already exists, remove it to prevent duplicate headers\n",
    "    if sheet_name in writer.book.sheetnames:\n",
    "        idx = writer.book.sheetnames.index(sheet_name)\n",
    "        writer.book.remove(writer.book.worksheets[idx])\n",
    "\n",
    "    sensitivity_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    apply_excel_formatting(writer.sheets[sheet_name], sensitivity_df) # Apply formatting\n",
    "    \n",
    "    writer.close()\n",
    "    display_and_log_print(f\"Sensitivity data exported to {filename} (Sheet: {sheet_name})\")\n",
    "\n",
    "\n",
    "    \n",
    "def sensitivity_uncertainty_table(sensitivity_beta, sensitivity_delta, uncertainty, \n",
    "                                  sensitivity_beta_array, sensitivity_delta_array):\n",
    "    \"\"\"\n",
    "    Prints a markdown summary table of sensitivity and uncertainty results using LaTeX symbols and including units.\n",
    "    Also exports the sensitivity and uncertainty data to an Excel file.\n",
    "    \n",
    "    Handles NaN values gracefully by filtering them out before computing statistics.\n",
    "\n",
    "    Args:\n",
    "        sensitivity_beta (float): Mean sensitivity with respect to beta (units: radians).\n",
    "        sensitivity_delta (float): Mean sensitivity with respect to delta (units: radians).\n",
    "        uncertainty (float): Standard deviation of the pseudo-entropy (units: bits).\n",
    "        sensitivity_beta_array (np.ndarray): Full array of sensitivity values for beta (units: radians).\n",
    "        sensitivity_delta_array (np.ndarray): Full array of sensitivity values for delta (units: radians).\n",
    "\n",
    "    Returns:\n",
    "        None: Displays and logs a summary in Markdown format with LaTeX formatting.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute min and max for sensitivity arrays with NaN handling\n",
    "    sensitivity_beta_min, sensitivity_beta_max = safe_min_max(sensitivity_beta_array)\n",
    "    sensitivity_delta_min, sensitivity_delta_max = safe_min_max(sensitivity_delta_array)\n",
    "\n",
    "    # Prepare the table data with proper NaN handling\n",
    "    table_data = [\n",
    "        [\"Metric\", \"Symbol\", \"Mean Value\", \"Min Value\", \"Max Value\", \"Units\"],\n",
    "        [r\"Global standard deviation of pseudo entropy\", r\"$\\sigma_{\\check{S}}$\",\n",
    "         to_latex(round_number(uncertainty)), \"-\", \"-\", \"bits\"],\n",
    "        [r\"Sensitivity over $\\beta$\",\n",
    "         r\"$S_{\\beta} = \\sigma_{\\check{S}} / \\|\\frac{{\\partial \\check{{S}}}}{{\\partial \\beta}}\\|$\",\n",
    "         to_latex(round_number(sensitivity_beta)),\n",
    "         to_latex(round_number(sensitivity_beta_min)),\n",
    "         to_latex(round_number(sensitivity_beta_max)), \"radians\"],\n",
    "        [r\"Sensitivity over $\\delta$\",\n",
    "         r\"$S_{\\delta} = \\sigma_{\\check{S}} / \\|\\frac{{\\partial \\check{{S}}}}{{\\partial \\delta}}\\|$\",\n",
    "         to_latex(round_number(sensitivity_delta)),\n",
    "         to_latex(round_number(sensitivity_delta_min)),\n",
    "         to_latex(round_number(sensitivity_delta_max)), \"radians\"]\n",
    "    ]\n",
    "\n",
    "    # Create and display the markdown table\n",
    "    markdown_table = tabulate(table_data, headers=\"firstrow\", tablefmt=\"pipe\")\n",
    "    display_and_log_markdown(\"**Sensitivity and Uncertainty Analysis**\\n\" + markdown_table)\n",
    "\n",
    "    # Export to Excel\n",
    "    sensitivity_df = sensitivity_to_dataframe(sensitivity_beta, sensitivity_delta, uncertainty,\n",
    "                                              sensitivity_beta_array, sensitivity_delta_array)\n",
    "    export_sensitivity_to_excel(sensitivity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5aae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sensitivity_latex():\n",
    "    \"\"\"\n",
    "    Prints LaTeX code for including Sensitivity figure in a LaTeX document.\n",
    "    \"\"\"\n",
    "    latex_code = r\"\"\"\n",
    "\\begin{figure}[htbp]\n",
    "    \\centering\n",
    "    \\includegraphics[width=\\textwidth]{results/variables_sensitivity.png}\n",
    "    \\caption{Measurement sensitivity for controlled interaction strength ($\\beta$) and coherent error angle ($\\delta$) \n",
    "    parameters. Lower values (darker blue) indicate higher precision.}\n",
    "    \\label{fig:variables_sensitivity}\n",
    "\\end{figure}\n",
    "\"\"\"\n",
    "    display_and_log_print(\"\\nLaTeX code for Sensitivity figure:\" + latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Sensitivity\n",
    "sensitivity_beta, sensitivity_delta, sensitivity_beta_array, sensitivity_delta_array, uncertainty = calculate_sensitivity(betas, deltas, numerical_entropies, numerical_std_deviations)\n",
    "\n",
    "# Plotting and table output\n",
    "plot_sensitivity(betas, deltas, sensitivity_beta_array, sensitivity_delta_array)\n",
    "sensitivity_uncertainty_table(sensitivity_beta, sensitivity_delta, uncertainty, \n",
    "                                     sensitivity_beta_array, sensitivity_delta_array)\n",
    "\n",
    "# Print figure tag\n",
    "print_sensitivity_latex()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
