{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83be72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install required packages\n",
    "#%pip install qiskit-ibm-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24e72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime.fake_provider import FakeProviderForBackendV2\n",
    "from qiskit_aer.noise import NoiseModel\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import csv\n",
    "import os\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a887959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hardware_data():\n",
    "    \"\"\"Extract hardware data from all backends and qubits.\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "\n",
    "    for backend in FakeProviderForBackendV2().backends():\n",
    "        # Extract noise model from real hardware (well, fake in this case!)\n",
    "        noise_model = NoiseModel.from_backend(backend)\n",
    "\n",
    "        # Get detailed error rates for ALL qubits (no limitation)\n",
    "        properties = backend.properties()\n",
    "        #print(f\"{backend.name.replace(\"fake_\", \"\").title()} - {noise_model.noise_instructions}\")\n",
    "        n_qubits = backend.configuration().n_qubits\n",
    "\n",
    "        for qubit in range(n_qubits):\n",
    "            readout_error = properties.readout_error(qubit)\n",
    "\n",
    "            # Store data for table\n",
    "            all_data.append({\n",
    "                'Backend': backend.name.replace(\"fake_\", \"\").title(),\n",
    "                'Num_Qubits': n_qubits,\n",
    "                'Qubit': qubit,\n",
    "                'Readout_Error_Percent': round(readout_error * 100, 2),\n",
    "                'Basis_Gates_Count': len(noise_model.basis_gates) if noise_model.basis_gates else 0,\n",
    "                'Noise_Instructions_Count': len(noise_model.noise_instructions)\n",
    "            })\n",
    "\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_latex_table(data):\n",
    "    \"\"\"Convert data to LaTeX table format.\"\"\"\n",
    "    \n",
    "    latex_table = \"\"\"\\\\begin{table}[htbp]\n",
    "\\\\centering\n",
    "\\\\caption{Quantum Hardware Characteristics}\n",
    "\\\\label{tab:hardware_data}\n",
    "\\\\begin{tabular}{|l|c|c|c|c|}\n",
    "\\\\hline\n",
    "\\\\textbf{Backend} & \\\\textbf{Qubit} & \\\\textbf{Readout Error} & \\\\textbf{Basis Gates Count} & \\\\textbf{Noise Instructions Count} \\\\\\\\\n",
    "\\\\hline\n",
    "\"\"\"\n",
    "\n",
    "    for row in data:\n",
    "        latex_table += f\"{row['Backend']} & {row['Qubit']} & {row['Readout_Error_Percent']}\\\\% & {row['Basis_Gates_Count']} & {row['Noise_Instructions_Count']} \\\\\\\\ \\\\hline \\n\"\n",
    "\n",
    "    latex_table += \"\"\"\n",
    "\\\\end{tabular}\n",
    "\\\\end{table}\"\"\"\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "def save_data_to_csv(data, filepath='hardware/hardware.csv'):\n",
    "    \"\"\"Save data to CSV file.\"\"\"\n",
    "    \n",
    "    # Ensure hardware directory exists\n",
    "    os.makedirs('hardware', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Backend', 'Qubit', 'Readout_Error_Percent', 'Basis_Gates_Count', 'Noise_Instructions_Count']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        \n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write data rows\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def run_hardware_analysis(data):\n",
    "    \"\"\"Extract data, print LaTeX table and save CSV.\"\"\"\n",
    "    \n",
    "    # Convert to LaTeX and print\n",
    "    latex_output = data_to_latex_table(data)\n",
    "    print(latex_output)\n",
    "    \n",
    "    # Save to CSV\n",
    "    save_data_to_csv(data)\n",
    "    print(f\"\\nData saved to: hardware/hardware.csv\")\n",
    "    print(f\"Total rows: {len(data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_qubit_groups(data, max_group_size=5, min_group_size=2):\n",
    "    \"\"\"Find optimal qubit groups with lowest readout errors per backend.\"\"\"\n",
    "    \n",
    "    # Group data by backend\n",
    "    backend_data = {}\n",
    "    for row in data:\n",
    "        backend = row['Backend']\n",
    "        if backend not in backend_data:\n",
    "            backend_data[backend] = []\n",
    "        backend_data[backend].append({\n",
    "            'qubit': row['Qubit'],\n",
    "            'error': row['Readout_Error_Percent']\n",
    "        })\n",
    "    \n",
    "    optimal_groups = {}\n",
    "    \n",
    "    for backend, qubits in backend_data.items():\n",
    "        # Sort qubits by readout error (lowest first)\n",
    "        qubits_sorted = sorted(qubits, key=lambda x: x['error'])\n",
    "        \n",
    "        best_groups = []\n",
    "        \n",
    "        # Try different group sizes (larger groups preferred)\n",
    "        for group_size in range(max_group_size, min_group_size - 1, -1):\n",
    "            if len(qubits_sorted) < group_size:\n",
    "                continue\n",
    "                \n",
    "            # Find all possible combinations of this size\n",
    "            for combo in combinations(qubits_sorted[:min(10, len(qubits_sorted))], group_size):\n",
    "                # Calculate statistics for this group\n",
    "                errors = [q['error'] for q in combo]\n",
    "                avg_error = sum(errors) / len(errors)\n",
    "                min_error = min(errors)\n",
    "                max_error = max(errors)\n",
    "                \n",
    "                group_info = {\n",
    "                    'qubits': [q['qubit'] for q in combo],\n",
    "                    'errors': errors,\n",
    "                    'avg_error': round(avg_error, 3),\n",
    "                    'min_error': round(min_error, 3),\n",
    "                    'max_error': round(max_error, 3),\n",
    "                    'size': group_size\n",
    "                }\n",
    "                \n",
    "                best_groups.append(group_info)\n",
    "        \n",
    "        # Sort by group size (descending) then by maximum error (ascending)\n",
    "        best_groups.sort(key=lambda x: (-x['size'], x['max_error']))\n",
    "        \n",
    "        # Take the best group\n",
    "        if best_groups:\n",
    "            optimal_groups[backend] = best_groups[0]\n",
    "    \n",
    "    return optimal_groups\n",
    "\n",
    "def optimal_groups_to_latex(optimal_groups):\n",
    "    \"\"\"Convert optimal qubit groups to LaTeX table.\"\"\"\n",
    "    \n",
    "    latex_table = \"\"\"\\\\begin{table}[htbp]\n",
    "\\\\centering\n",
    "\\\\caption{Optimal Qubit Groups by Backend (Lowest Readout Errors)}\n",
    "\\\\label{tab:optimal_qubit_groups}\n",
    "\\\\begin{tabular}{|l|c|c|c|c|c|c|}\n",
    "\\\\hline\n",
    "\\\\textbf{Backend} & \\\\textbf{Group Size} & \\\\textbf{Qubits} & \\\\textbf{Individual Errors} & \\\\textbf{Min Error} & \\\\textbf{Max Error} & \\\\textbf{Avg Error} \\\\\\\\\n",
    "\\\\hline\n",
    "\"\"\"\n",
    "\n",
    "    # Sort backends by maximum error for better presentation\n",
    "    sorted_backends = sorted(optimal_groups.items(), key=lambda x: x[1]['max_error'])\n",
    "    \n",
    "    for backend, group_info in sorted_backends:\n",
    "        qubits_str = ', '.join(map(str, group_info['qubits']))\n",
    "        errors_str = ', '.join(f\"{e}\\\\%\" for e in group_info['errors'])\n",
    "        \n",
    "        latex_table += f\"{backend} & {group_info['size']} & {qubits_str} & {errors_str} & {group_info['min_error']}\\\\% & {group_info['max_error']}\\\\% & {group_info['avg_error']}\\\\% \\\\\\\\ \\\\hline \\n\"\n",
    "\n",
    "    latex_table += \"\"\"\n",
    "\\\\end{tabular}\n",
    "\\\\end{table}\"\"\"\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "def save_optimal_groups_to_csv(optimal_groups, filepath='hardware/optimal_qubit_groups.csv'):\n",
    "    \"\"\"Save optimal qubit groups to CSV file.\"\"\"\n",
    "    \n",
    "    # Ensure hardware directory exists\n",
    "    os.makedirs('hardware', exist_ok=True)\n",
    "    \n",
    "    # Prepare data for CSV\n",
    "    csv_data = []\n",
    "    for backend, group_info in optimal_groups.items():\n",
    "        csv_data.append({\n",
    "            'Backend': backend,\n",
    "            'Group Size': group_info['size'],\n",
    "            'Qubits': ';'.join(map(str, group_info['qubits'])),  # semicolon separated for CSV\n",
    "            'Individual Errors': ';'.join(map(str, group_info['errors'])),\n",
    "            'Min Error': group_info['min_error'],\n",
    "            'Max Error': group_info['max_error'],\n",
    "            'Average Error': group_info['avg_error']\n",
    "        })\n",
    "    \n",
    "    # Save to CSV\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Backend', 'Group Size', 'Qubits', 'Individual Errors', 'Min Error', 'Max Error', 'Average Error']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        \n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write data rows\n",
    "        for row in csv_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def run_optimal_qubit_analysis(data, max_group_size=5, min_group_size=2):\n",
    "    \"\"\"Find and display optimal qubit groups.\"\"\"\n",
    "       \n",
    "    # Find optimal groups\n",
    "    optimal_groups = find_optimal_qubit_groups(data, max_group_size, min_group_size)\n",
    "    \n",
    "    # Convert to LaTeX and print\n",
    "    latex_output = optimal_groups_to_latex(optimal_groups)\n",
    "    print(latex_output)\n",
    "    \n",
    "    # Save to CSV\n",
    "    save_optimal_groups_to_csv(optimal_groups)\n",
    "    print(f\"\\nOptimal groups saved to: hardware/optimal_qubit_groups.csv\")\n",
    "    print(f\"Found optimal groups for {len(optimal_groups)} backends\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nSummary of optimal groups:\")\n",
    "    for backend, group_info in sorted(optimal_groups.items(), key=lambda x: x[1]['max_error']):\n",
    "        qubits_str = ', '.join(map(str, group_info['qubits']))\n",
    "        print(f\"{backend}: {group_info['size']} qubits [{qubits_str}] - Min: {group_info['min_error']}%, Max: {group_info['max_error']}%, Avg: {group_info['avg_error']}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd7fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hardware_summary_latex(data):\n",
    "    \"\"\"Print a summary table showing backend overview in LaTeX format.\"\"\"\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data to display\")\n",
    "        return\n",
    "    \n",
    "    # Group data by backend to create summary\n",
    "    backend_summary = {}\n",
    "    for row in data:\n",
    "        backend = row['Backend']\n",
    "        if backend not in backend_summary:\n",
    "            backend_summary[backend] = {\n",
    "                'num_qubits': row['Num_Qubits'],\n",
    "                'avg_readout_error': [],\n",
    "                'basis_gates': row['Basis_Gates_Count'],\n",
    "                'noise_instructions': row['Noise_Instructions_Count']\n",
    "            }\n",
    "        backend_summary[backend]['avg_readout_error'].append(row['Readout_Error_Percent'])\n",
    "    \n",
    "    # Calculate averages\n",
    "    for backend in backend_summary:\n",
    "        errors = backend_summary[backend]['avg_readout_error']\n",
    "        backend_summary[backend]['avg_readout_error'] = round(sum(errors) / len(errors), 2)\n",
    "    \n",
    "    # LaTeX summary table\n",
    "    latex_output = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Quantum Backend Summary}\n",
    "\\label{tab:backend_summary}\n",
    "\\begin{tabular}{|l|c|c|c|c|}\n",
    "\\hline\n",
    "\\textbf{Backend} & \\textbf{Qubits} & \\textbf{Avg. Readout Error (\\%)} & \\textbf{Basis Gates} & \\textbf{Noise Instructions} \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "    \n",
    "    # Add summary rows\n",
    "    for backend, info in backend_summary.items():\n",
    "        latex_output += f\"{backend} & {info['num_qubits']} & {info['avg_readout_error']} & {info['basis_gates']} & {info['noise_instructions']} \\\\\\\\ \\\\hline \\n\"\n",
    "    \n",
    "    # LaTeX table footer\n",
    "    latex_output += r\"\"\"\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    print(latex_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cx_error_from_backend(backend, qubit1, qubit2):\n",
    "    \"\"\"\n",
    "    Get CX gate error rate between two specific qubits using main methods.\n",
    "    \n",
    "    Args:\n",
    "        backend: Qiskit backend\n",
    "        qubit1, qubit2: Qubit indices\n",
    "        \n",
    "    Returns:\n",
    "        dict: CX gate information or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        properties = backend.properties()\n",
    "        \n",
    "        # Method 1: Direct gate_error method (most reliable)\n",
    "        try:\n",
    "            error_rate = properties.gate_error('cx', [qubit1, qubit2])\n",
    "            gate_time = properties.gate_length('cx', [qubit1, qubit2])\n",
    "            \n",
    "            # Skip if error rate is zero (indicates unavailable CNOT)\n",
    "            if error_rate == 0:\n",
    "                return None\n",
    "            \n",
    "            return {\n",
    "                'error_rate': error_rate,\n",
    "                'error_percent': round(error_rate * 100, 2),  # Changed to 2 digits\n",
    "                'gate_time': gate_time,\n",
    "                'method': 'properties_direct'\n",
    "            }\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 2: Search through all gates in properties\n",
    "        for gate in properties.gates:\n",
    "            if gate.gate == 'cx' and set(gate.qubits) == {qubit1, qubit2}:\n",
    "                error_rate = gate.parameters[0].value  # Gate error is usually first parameter\n",
    "                \n",
    "                # Skip if error rate is zero (indicates unavailable CNOT)\n",
    "                if error_rate == 0:\n",
    "                    return None\n",
    "                \n",
    "                gate_time = None\n",
    "                \n",
    "                # Try to find gate time\n",
    "                for param in gate.parameters:\n",
    "                    if param.name == 'gate_time':\n",
    "                        gate_time = param.value\n",
    "                        break\n",
    "                \n",
    "                return {\n",
    "                    'error_rate': error_rate,\n",
    "                    'error_percent': round(error_rate * 100, 2),  # Changed to 2 digits\n",
    "                    'gate_time': gate_time,\n",
    "                    'method': 'properties_search'\n",
    "                }\n",
    "        \n",
    "        # Skip Method 3 - we ignore pairs without connectivity\n",
    "            \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def analyze_group_cx_gates(backend, qubit_group):\n",
    "    \"\"\"\n",
    "    Analyze all possible CX gate pairs within a qubit group.\n",
    "    Only includes pairs with actual connectivity and non-zero error rates.\n",
    "    \n",
    "    Args:\n",
    "        backend: Qiskit backend object\n",
    "        qubit_group: List of qubit indices\n",
    "        \n",
    "    Returns:\n",
    "        dict: CX gate analysis for the group\n",
    "    \"\"\"\n",
    "    cx_analysis = {\n",
    "        'total_pairs': 0,\n",
    "        'available_cx_pairs': 0,\n",
    "        'cx_gates': {},\n",
    "        'statistics': {}\n",
    "    }\n",
    "    \n",
    "    # Get all possible pairs within the group\n",
    "    all_pairs = list(combinations(qubit_group, 2))\n",
    "    cx_analysis['total_pairs'] = len(all_pairs)\n",
    "    \n",
    "    cx_errors = []\n",
    "    cx_times = []\n",
    "    \n",
    "    for q1, q2 in all_pairs:\n",
    "        # Try both directions for CX gate\n",
    "        cx_info = get_cx_error_from_backend(backend, q1, q2)\n",
    "        if not cx_info:\n",
    "            cx_info = get_cx_error_from_backend(backend, q2, q1)\n",
    "        \n",
    "        # Only count pairs with valid connectivity and non-zero error rates\n",
    "        if cx_info and cx_info['error_rate'] is not None and cx_info['error_rate'] > 0:\n",
    "            cx_analysis['available_cx_pairs'] += 1\n",
    "            cx_analysis['cx_gates'][(q1, q2)] = cx_info\n",
    "            cx_errors.append(cx_info['error_rate'])\n",
    "            if cx_info['gate_time'] is not None:\n",
    "                cx_times.append(cx_info['gate_time'])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if cx_errors:\n",
    "        cx_analysis['statistics'] = {\n",
    "            'avg_cx_error': round(sum(cx_errors) / len(cx_errors), 6),\n",
    "            'min_cx_error': round(min(cx_errors), 6),\n",
    "            'max_cx_error': round(max(cx_errors), 6),\n",
    "            'avg_cx_error_percent': round(sum(cx_errors) / len(cx_errors) * 100, 2),  # Changed to 2 digits\n",
    "            'connectivity_ratio': round(cx_analysis['available_cx_pairs'] / cx_analysis['total_pairs'], 3)\n",
    "        }\n",
    "        \n",
    "        if cx_times:\n",
    "            cx_analysis['statistics']['avg_gate_time'] = round(sum(cx_times) / len(cx_times), 9)\n",
    "            cx_analysis['statistics']['min_gate_time'] = round(min(cx_times), 9)\n",
    "            cx_analysis['statistics']['max_gate_time'] = round(max(cx_times), 9)\n",
    "    \n",
    "    return cx_analysis\n",
    "\n",
    "\n",
    "def calculate_total_error(readout_errors, cx_error_rate):\n",
    "    \"\"\"\n",
    "    Calculate total error as sum of two qubit readout errors plus CX error.\n",
    "    \n",
    "    Args:\n",
    "        readout_errors: List of readout error percentages for qubits in the pair\n",
    "        cx_error_rate: CX gate error rate (as percentage)\n",
    "        \n",
    "    Returns:\n",
    "        float: Total error percentage\n",
    "    \"\"\"\n",
    "    return sum(readout_errors) + cx_error_rate\n",
    "\n",
    "\n",
    "def find_optimal_qubit_groups_with_cx(data, max_group_size=5, min_group_size=2):\n",
    "    \"\"\"\n",
    "    Find optimal qubit groups with lowest total errors (readout + CX) and analyze CX gates within groups.\n",
    "    \n",
    "    Args:\n",
    "        data: Hardware data from extract_hardware_data()\n",
    "        max_group_size: Maximum size of qubit groups\n",
    "        min_group_size: Minimum size of qubit groups\n",
    "        \n",
    "    Returns:\n",
    "        dict: Optimal groups with CX gate analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group data by backend\n",
    "    backend_data = {}\n",
    "    backends_map = {}  # Store backend objects\n",
    "    \n",
    "    # Get backend objects\n",
    "    provider = FakeProviderForBackendV2()\n",
    "    for backend_obj in provider.backends():\n",
    "        backend_name = backend_obj.name.replace(\"fake_\", \"\").title()\n",
    "        backends_map[backend_name] = backend_obj\n",
    "    \n",
    "    for row in data:\n",
    "        backend = row['Backend']\n",
    "        if backend not in backend_data:\n",
    "            backend_data[backend] = []\n",
    "        backend_data[backend].append({\n",
    "            'qubit': row['Qubit'],\n",
    "            'error': row['Readout_Error_Percent']\n",
    "        })\n",
    "    \n",
    "    optimal_groups = {}\n",
    "    \n",
    "    for backend, qubits in backend_data.items():\n",
    "        if backend not in backends_map:\n",
    "            print(f\"Warning: Backend {backend} not found in provider\")\n",
    "            continue\n",
    "            \n",
    "        backend_obj = backends_map[backend]\n",
    "        \n",
    "        # Sort qubits by readout error (lowest first)\n",
    "        qubits_sorted = sorted(qubits, key=lambda x: x['error'])\n",
    "        \n",
    "        best_groups = []\n",
    "        \n",
    "        # Try different group sizes (larger groups preferred)\n",
    "        for group_size in range(max_group_size, min_group_size - 1, -1):\n",
    "            if len(qubits_sorted) < group_size:\n",
    "                continue\n",
    "                \n",
    "            # Find all possible combinations of this size\n",
    "            for combo in combinations(qubits_sorted[:min(10, len(qubits_sorted))], group_size):\n",
    "                # Calculate readout error statistics for this group\n",
    "                errors = [q['error'] for q in combo]\n",
    "                avg_error = sum(errors) / len(errors)\n",
    "                min_error = min(errors)\n",
    "                max_error = max(errors)\n",
    "                total_readout_error = sum(errors)  # Add total readout error calculation\n",
    "                \n",
    "                qubit_indices = [q['qubit'] for q in combo]\n",
    "                \n",
    "                # Analyze CX gates within this group\n",
    "                cx_analysis = analyze_group_cx_gates(backend_obj, qubit_indices)\n",
    "                \n",
    "                # Calculate total error (readout + CX) for ranking\n",
    "                total_error = 0\n",
    "                if cx_analysis['statistics'] and cx_analysis['cx_gates']:\n",
    "                    # For pairs, calculate total error as sum of readout errors + CX error\n",
    "                    if group_size == 2:\n",
    "                        cx_error_percent = cx_analysis['statistics']['avg_cx_error_percent']\n",
    "                        total_error = sum(errors) + cx_error_percent\n",
    "                    else:\n",
    "                        # For larger groups, use average approach\n",
    "                        total_error = avg_error * group_size + cx_analysis['statistics']['avg_cx_error_percent']\n",
    "                else:\n",
    "                    # If no CX connectivity, use a high penalty\n",
    "                    total_error = float('inf')\n",
    "                \n",
    "                group_info = {\n",
    "                    'qubits': qubit_indices,\n",
    "                    'readout_errors': [round(e, 2) for e in errors],  # Changed to 2 digits\n",
    "                    'avg_readout_error': round(avg_error, 2),  # Changed to 2 digits\n",
    "                    'min_readout_error': round(min_error, 2),  # Changed to 2 digits\n",
    "                    'max_readout_error': round(max_error, 2),  # Changed to 2 digits\n",
    "                    'total_readout_error': round(total_readout_error, 2),  # Add total readout error\n",
    "                    'size': group_size,\n",
    "                    'cx_analysis': cx_analysis,\n",
    "                    'total_error': round(total_error, 2) if total_error != float('inf') else None\n",
    "                }\n",
    "                \n",
    "                best_groups.append(group_info)\n",
    "        \n",
    "        # Sort by minimum total error (ascending), then connectivity ratio (descending), then group size (descending)\n",
    "        best_groups = [g for g in best_groups if g['total_error'] is not None]  # Remove groups without connectivity\n",
    "        best_groups.sort(key=lambda x: (\n",
    "            x['total_error'],\n",
    "            -x['cx_analysis']['statistics'].get('connectivity_ratio', 0),\n",
    "            -x['size']\n",
    "        ))\n",
    "        \n",
    "        # Take the best group\n",
    "        if best_groups:\n",
    "            optimal_groups[backend] = best_groups[0]\n",
    "    \n",
    "    return optimal_groups\n",
    "\n",
    "\n",
    "def get_backend_info(backend_obj):\n",
    "    \"\"\"\n",
    "    Get backend information including number of qubits and availability date if possible.\n",
    "    \n",
    "    Args:\n",
    "        backend_obj: Qiskit backend object\n",
    "        \n",
    "    Returns:\n",
    "        dict: Backend information\n",
    "    \"\"\"\n",
    "    info = {\n",
    "        'name': backend_obj.name,\n",
    "        'num_qubits': backend_obj.configuration().n_qubits,\n",
    "        'availability_date': None,\n",
    "        'description': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Try to get description from backend\n",
    "        config = backend_obj.configuration()\n",
    "        if hasattr(config, 'description'):\n",
    "            info['description'] = config.description\n",
    "        \n",
    "        # Try to get backend version or date information\n",
    "        if hasattr(config, 'backend_version'):\n",
    "            info['backend_version'] = config.backend_version\n",
    "            \n",
    "        # Some backends might have date information in their properties\n",
    "        if hasattr(backend_obj, 'properties') and backend_obj.properties():\n",
    "            props = backend_obj.properties()\n",
    "            if hasattr(props, 'last_update_date'):\n",
    "                info['last_update_date'] = props.last_update_date\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return info\n",
    "\n",
    "def print_backend_summary(backends_map):\n",
    "    \"\"\"Print summary of all backends with their specifications.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BACKEND SPECIFICATIONS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    backend_info_list = []\n",
    "    \n",
    "    for backend_name, backend_obj in backends_map.items():\n",
    "        info = get_backend_info(backend_obj)\n",
    "        backend_info_list.append(info)\n",
    "    \n",
    "    # Sort by number of qubits (ascending)\n",
    "    backend_info_list.sort(key=lambda x: x['num_qubits'])\n",
    "    \n",
    "    print(f\"{'Backend Name':<20} {'Qubits':<8} {'Description'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for info in backend_info_list:\n",
    "        backend_display = info['name'].replace('fake_', '').title()\n",
    "        description = info.get('description', 'N/A')\n",
    "        if description and len(description) > 40:\n",
    "            description = description[:37] + \"...\"\n",
    "        \n",
    "        print(f\"{backend_display:<20} {info['num_qubits']:<8} {description}\")\n",
    "    \n",
    "    print(f\"\\nTotal backends analyzed: {len(backend_info_list)}\")\n",
    "\n",
    "\n",
    "def save_to_csv(optimal_groups, backends_map, filename='hardware/optimal_qubit_groups.csv'):\n",
    "    \"\"\"Save optimal groups analysis to CSV file with backend info, sorted by Total_Readout_Error_Percent.\"\"\"\n",
    "    \n",
    "    # Prepare data for sorting\n",
    "    rows_data = []\n",
    "    \n",
    "    for backend, group_info in optimal_groups.items():\n",
    "        cx_stats = group_info['cx_analysis']['statistics']\n",
    "        cx_gates = group_info['cx_analysis']['cx_gates']\n",
    "        \n",
    "        # Get backend info\n",
    "        backend_obj = backends_map.get(backend)\n",
    "        backend_info = get_backend_info(backend_obj) if backend_obj else {}\n",
    "        \n",
    "        # Format CX gate pairs for CSV\n",
    "        cx_pairs_str = '; '.join([f\"{pair[0]}-{pair[1]}({info['error_percent']}%)\" \n",
    "                                for pair, info in cx_gates.items()])\n",
    "        \n",
    "        row = {\n",
    "            'Backend': backend,\n",
    "            'Backend_Qubits': backend_info.get('num_qubits', 'N/A'),\n",
    "            'Group_Size': group_info['size'],\n",
    "            'Qubits': ','.join(map(str, group_info['qubits'])),\n",
    "            'Total_Readout_Error_Percent': group_info['total_readout_error'],\n",
    "            'Avg_Readout_Error_Percent': group_info['avg_readout_error'],\n",
    "            'Min_Readout_Error_Percent': group_info['min_readout_error'],\n",
    "            'Max_Readout_Error_Percent': group_info['max_readout_error'],\n",
    "            'Total_Pairs': group_info['cx_analysis']['total_pairs'],\n",
    "            'Available_CX_Pairs': group_info['cx_analysis']['available_cx_pairs'],\n",
    "            'Connectivity_Ratio': cx_stats.get('connectivity_ratio', 0),\n",
    "            'Avg_CX_Error_Percent': cx_stats.get('avg_cx_error_percent', 0),\n",
    "            'Min_CX_Error_Percent': round(cx_stats.get('min_cx_error', 0) * 100, 2) if cx_stats.get('min_cx_error') else 0,\n",
    "            'Max_CX_Error_Percent': round(cx_stats.get('max_cx_error', 0) * 100, 2) if cx_stats.get('max_cx_error') else 0,\n",
    "            'Total_Error_Percent': group_info['total_error'],\n",
    "            'Available_CX_Gate_Pairs': cx_pairs_str,\n",
    "            'Backend_Description': backend_info.get('description', 'N/A')\n",
    "        }\n",
    "        \n",
    "        rows_data.append(row)\n",
    "    \n",
    "    # Sort by Total_Readout_Error_Percent (ascending order - lowest error first)\n",
    "    rows_data.sort(key=lambda x: float(x['Total_Readout_Error_Percent']) if x['Total_Readout_Error_Percent'] != 'N/A' else float('inf'))\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = [\n",
    "            'Backend', 'Backend_Qubits', 'Group_Size', 'Qubits', 'Total_Readout_Error_Percent',\n",
    "            'Avg_Readout_Error_Percent', 'Min_Readout_Error_Percent', 'Max_Readout_Error_Percent',\n",
    "            'Total_Pairs', 'Available_CX_Pairs', 'Connectivity_Ratio',\n",
    "            'Avg_CX_Error_Percent', 'Min_CX_Error_Percent', 'Max_CX_Error_Percent',\n",
    "            'Total_Error_Percent', 'Available_CX_Gate_Pairs', 'Backend_Description'\n",
    "        ]\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write sorted data\n",
    "        for row in rows_data:\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"\\n✅ Results saved to {filename}\")\n",
    "\n",
    "def print_cx_comparison_latex(optimal_groups, backends_map):\n",
    "    \"\"\"Print LaTeX table comparing CX gate performance across backends.\"\"\"\n",
    "    \n",
    "    latex_output = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Optimal Qubit Groups: CX Gate Performance Comparison (Ordered by Total Error)}\n",
    "\\label{tab:cx_comparison}\n",
    "\\begin{tabular}{|l|c|c|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\textbf{Backend} & \\textbf{Total Qubits} & \\textbf{Group Size} & \\textbf{Connectivity} & \\textbf{Total Readout (\\%)} & \\textbf{Avg CX Error (\\%)} & \\textbf{Total Error (\\%)} & \\textbf{Qubits} \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "    \n",
    "    # Sort by total error for LaTeX table\n",
    "    sorted_groups = sorted(optimal_groups.items(), key=lambda x: x[1]['total_error'] or float('inf'))\n",
    "    \n",
    "    for backend, group_info in sorted_groups:\n",
    "        # Get backend info\n",
    "        backend_obj = backends_map.get(backend)\n",
    "        backend_info = get_backend_info(backend_obj) if backend_obj else {}\n",
    "        \n",
    "        cx_stats = group_info['cx_analysis']['statistics']\n",
    "        connectivity = cx_stats.get('connectivity_ratio', 0) * 100\n",
    "        avg_cx_error = cx_stats.get('avg_cx_error_percent', 0)\n",
    "        total_readout = group_info['total_readout_error']  # Use total readout instead of avg\n",
    "        total_error = group_info['total_error']\n",
    "        qubits_str = ','.join(map(str, group_info['qubits']))\n",
    "        total_qubits = backend_info.get('num_qubits', 'N/A')\n",
    "        \n",
    "        latex_output += f\"{backend} & {total_qubits} & {group_info['size']} & {connectivity:.1f}\\\\% & {total_readout:.2f} & {avg_cx_error:.2f} & {total_error:.2f} & {qubits_str} \\\\\\\\ \\\\hline \\n\"\n",
    "    \n",
    "    latex_output += r\"\"\"\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LATEX TABLE - CX COMPARISON (Ordered by Total Error)\")\n",
    "    print(\"=\"*50)\n",
    "    print(latex_output)\n",
    "\n",
    "\n",
    "def run_optimal_qubit_analysis_cx(data):\n",
    "    \"\"\"Run the complete analysis with all improvements.\"\"\"\n",
    "    \n",
    "    # Get backend objects for info extraction\n",
    "    provider = FakeProviderForBackendV2()\n",
    "    backends_map = {}\n",
    "    for backend_obj in provider.backends():\n",
    "        backend_name = backend_obj.name.replace(\"fake_\", \"\").title()\n",
    "        backends_map[backend_name] = backend_obj\n",
    "\n",
    "    # Print backend specifications summary\n",
    "    print_backend_summary(backends_map)\n",
    "    \n",
    "    # Find optimal groups with CX analysis\n",
    "    optimal_groups = find_optimal_qubit_groups_with_cx(data, max_group_size=4, min_group_size=2)\n",
    "    \n",
    "    # Print LaTeX comparison table\n",
    "    print_cx_comparison_latex(optimal_groups, backends_map)\n",
    "    \n",
    "    # Save to CSV\n",
    "    save_to_csv(optimal_groups, backends_map)\n",
    "    \n",
    "    return optimal_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eebf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_hardware_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hardware_analysis(data)\n",
    "run_optimal_qubit_analysis(data, max_group_size=4, min_group_size=2)\n",
    "print_hardware_summary_latex(data)\n",
    "run_optimal_qubit_analysis_cx(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
